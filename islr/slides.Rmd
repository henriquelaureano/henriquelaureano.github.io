---
title: "Classification \\newline
        \\normalsize chapter 4 of
        \\textit{An Introduction to Statistical Learning} (ISL)"
short-title: "Classification: ISL book chapter"
author: "Henrique Laureano \\newline \\url{http://leg.ufpr.br/~henrique}"
short-author: "leg.ufpr.br/~henrique"
#email: "laureano\\@ufpr.br"
#date: "August 12, 2019"
#short-date:
#department: "Laboratory of Statistics and Geoinformation (LEG)"
#institute: "UFPR/DEST/LEG"
#short-institute: "LEG"
section-titles: true
safe-columns: true # enables special latex macros for columns
output:
  legtheme::beamer_leg
---

### What we read (long description)

\begincols
 \column{.4\linewidth}
  \begin{figure}
   \centering
   \includegraphics[width=.95\textwidth]{book.jpg}
  \end{figure}
 \column{.6\linewidth}
  \begin{figure}
   \centering
   \includegraphics[width=\textwidth]{chap4.png}
  \end{figure}
\endcols

###

\textcolor{beamer@UIUCorange}{Now in a shorter way}

###

\textcolor{beamer@UIUCblue}{What we read (short description)}

\begin{adjustwidth}{2.5em}{0pt}
At chapter 4 are discussed three of the most widely-used classifiers.
\begin{itemize}
\item Logistic Regression
\item Linear Discriminant Analysis (LDA)
\item K-Nearest Neighbors (KNN)
\end{itemize}
\end{adjustwidth}

\textcolor{beamer@UIUCblue}{What we didn't read}

\begin{adjustwidth}{2.5em}{0pt}
More computer-intensive methods are discussed in later chapters, such as
\begin{itemize}
\item Generalized Additive Models (GAM)
\item Trees
\item Random Forests
\item Boosting
\item Support Vector Machines (SVM)
\end{itemize}
\end{adjustwidth}

# Why Not Linear Regression?

###

We could consider encoding the response, \(Y\), as a quantitative
variable, e.g.,

\begin{block}{Predict the medical condition of a patient on the basis of
her symptoms.}
\[
Y = \begin{cases}
  1 & \text{if}~\textcolor{beamer@UIUCblue}{\texttt{stroke}};\\
  2 & \text{if}~\textcolor{beamer@UIUCblue}{\texttt{drug overdose}};\\
  3 & \text{if}~\textcolor{beamer@UIUCblue}{\texttt{epileptic seizure}}.\\
\end{cases}
\]
\end{block}

\pause
\textcolor{beamer@UIUCorange}{Unfortunately, this coding implies an
ordering on the outcomes.}

Each possible coding would produce a fundamentally different linear
model that would ultimately lead to different sets of predictions.

### That leads us to other questions,

+ What if the response variable values did take on a natural ordering,
  \textcolor{beamer@UIUCblue}{\texttt{such as mild}},
  \textcolor{beamer@UIUCblue}{\texttt{moderate}}, and
  \textcolor{beamer@UIUCblue}{\texttt{severe}}?

+ For a \textcolor{beamer@UIUCblue}{\texttt{binary}} (two level)
  qualitative response, the situation is better.

   - However, if we use linear regression, some of our estimates might
     be outside the [0, 1] interval.

   - However, the dummy variable approach cannot be easily extended to
     accommodate qualitative responses with more than two levels.

\pause
\textcolor{beamer@UIUCorange}{For these reasons, it is preferable to use
a classification method that is truly suited for qualitative response
values, such as the ones presented next.}

\begin{block}{Curiously,}
it turns out that the classifications that we get if we use linear
regression to predict a binary response will be the same as for the
linear discriminant analysis (LDA) procedure we discuss later.
\end{block}

# A typical dataset

### A classic 'book example dataset relationship'

\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{iBagens/dataset_chap4.png}
\end{figure}

... a very pronounced relationship between `balance` and `default`.

# Logistic Regression

### To start, a comparison with Linear Regression

\begin{figure}
\centering
\includegraphics[width=\textwidth]{iBagens/linr-vs-logr.png}
\end{figure}

###

\textcolor{beamer@UIUCorange}{\textit{Logistic regression in two slides}}

## The model framework

### Some math, but with just one predictor

\begin{block}{The model and its relations (\textit{showing my
\LaTeX~skills})}
\[
p(X) = \underset{\begin{gathered}
                  \text{\textit{logistic}}\\[-\jot]
                  \text{\textit{function}}\\[-\jot]
                  (\textit{S-shaped})
                 \end{gathered}}{
        \underbrace{\frac{e^{\beta_{0}+\beta_{1}X}}{
         1+e^{\beta_{0}+\beta_{1}X}}}} \Rightarrow
       \underset{\textit{odds}~\in~(0, \infty)}{
        \underbrace{\frac{p(X)}{1-p(X)}}} = e^{\beta_{0}+\beta_{1}X}
       \Rightarrow \underset{\begin{gathered}
                              \text{\textit{log-odds}}\\[-\jot]
                              \text{or}\\[-\jot]
                              \text{\textit{logit}}
                             \end{gathered}}{
                    \underbrace{\log \frac{p(X)}{1-p(X)}}} =
                   \beta_{0}+\beta_{1}X
\]
\end{block}

\pause
For example,

\[
p(X) = 0.2~\Rightarrow~\frac{0.2}{1-0.2} = \frac{1}{4}
\quad \textit{ and } \quad
p(X) = 0.9~\Rightarrow~\frac{0.9}{1-0.9} = 9.
\]

## Estimating the Regression Coefficients

### Maximum likelihood

The estimates \(\hat{\beta}_{0}\) and  \(\hat{\beta}_{1}\) are chosen to
\textcolor{beamer@UIUCblue}{maximize} a math equation called a

\begin{block}{\textit{likelihood function}}
\[
l(\beta_{0}, \beta_{1}) =
\prod_{i:y_{i}=1} p(x_{i}) \prod_{i':y_{i'}=0} (1-p(x_{i'})).
\]
\end{block}

\pause
The coefficients \(\hat{\beta}_{0}\) and \(\hat{\beta}_{1}\) are
unknown, and must be estimated. The general method of
\textcolor{beamer@UIUCblue}{maximum likelihood} is preferred, since it
has better statistical properties.

Maximum likelihood is a very general approach that is used to fit many
of the non-linear models examined throughout the book. In the linear
regression setting, the least squares approach is in fact a special case
of maximum likelihood.

# Linear Discriminant Analysis (LDA)

## To start... why do we need something different?

###

\textcolor{beamer@UIUCorange}{Different ideas, sometimes the same results}

###

\textcolor{beamer@UIUCblue}{Different ideas,}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{iBagens/logr-vs-lda.jpeg}
\end{figure}

\begin{adjustwidth}{2.5em}{0pt}
With LDA we model the distribution of the predictors \(X\) separately in
each of the response classes (i.e. given \(Y\)), and then use Bayesâ€™
theorem to flip these around into estimates for \(\mathbb{P}[Y = k | X =
x]\).
\end{adjustwidth}

\pause
\textcolor{beamer@UIUCblue}{Sometimes the same results}

\begin{adjustwidth}{2.5em}{0pt}
When these distributions are assumed to be normal, it turns out that
the model is very similar in form to logistic regression.
\end{adjustwidth}

###

\textcolor{beamer@UIUCorange}{But, ok... why not continue with logistic
regression?}

### But, ok... why not continue with logistic regression?

Simple, LDA is popular when we have more than two response classes.

\noindent{\color{beamer@UIUCorange}\rule{\linewidth}{0.25mm}}

Now, a reason more serious: \textcolor{beamer@UIUCblue}{stability}

+ When the classes are well-separated, the parameter estimates for the
  logistic regression model are surprisingly unstable. LDA does not
  suffer from this problem.

+ If \(n\) is small and the distribution of the predictors \(X\) is
  approximately normal in each of the classes, the linear discriminant
  model is again more stable than the logistic regression model.

## LDA in a nutshell

### Model framework

\begin{figure}
\centering
\includegraphics[width=\textwidth]{iBagens/lda-def.jpeg}
\end{figure}

+ \(\pi_{\textcolor{red}{k}}\) is the overall or
  \textcolor{beamer@UIUCblue}{prior} prob. that a chosen obs. comes from
  \(\textcolor{red}{k}\).

+ In general, estimating \(\pi_{\textcolor{red}{k}}\) is easy if we have
  a sample of \(\textcolor{beamer@UIUCblue}{Y}\)s: we simply compute the
  fraction of observations that belong to the \(\textcolor{red}{k}\)th
  class. However, estimating
  \(f_{\textcolor{red}{k}}(\textcolor{red}{x})\) tends to be more
  challenging, unless we assume some simple forms for these densities.

\textcolor{beamer@UIUCorange}{Remember from Chap. 2} that the Bayes
classifier has the lowest possible error rate out of all classifiers.

## Living in a simple and \textit{normal} world

### Dealing with just one predictor

\textcolor{beamer@UIUCorange}{Assumptions}:
\(f_{\textcolor{red}{k}}(\textcolor{red}{x})\) is
\textcolor{beamer@UIUCblue}{normal} with equal variance for the
\(\textcolor{red}{k}\)th classes.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{iBagens/lda1-steps.jpeg}
\end{figure}

Putting a \textcolor{beamer@UIUCblue}{hat} (simple average and a
weighted average of the sample variances for each class) in everything,
the LDA \textcolor{beamer@UIUCblue}{approx.} this Bayes classifier.

###

Ok, nice! But... \textcolor{beamer@UIUCblue}{why} the name
\textcolor{beamer@UIUCorange}{linear discriminant analysis}?

\pause
\begin{adjustwidth}{2.5em}{0pt}
The word \textcolor{beamer@UIUCorange}{linear} stems from the fact that
the \textcolor{beamer@UIUCorange}{discriminant functions}
\(\hat{\delta}_{\textcolor{red}{k}}(x)\) are linear functions of \(x\).
\end{adjustwidth}

## Now, with more than one predictor

### Getting bigger

\[
\text{More than one predictor}~\Rightarrow~
\begin{gathered}
  \text{Multivariate normal distribution,}\\[-\jot]
  \text{with a class-specific mean vector}\\[-\jot]
  \text{and a common covariance matrix}
\end{gathered}
\]

# K-Nearest Neighbors (KNN)

### and...

\begin{figure} \centering \includegraphics[width=.5\textwidth]{end.jpg}
\end{figure}
\hfill \small \href{mailto:laureano@ufpr.br}{laureano@ufpr.br}
