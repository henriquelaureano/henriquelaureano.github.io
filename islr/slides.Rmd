---
title: "Classification \\newline
        \\normalsize chapter 4 of
        \\textit{An Introduction to Statistical Learning} (ISL)"
short-title: "Classification: ISL book chapter"
author: "Henrique Laureano \\newline \\url{http://leg.ufpr.br/~henrique}"
short-author: "leg.ufpr.br/~henrique"
#email: "laureano\\@ufpr.br"
#date: "August 12, 2019"
#short-date:
department: "
Laboratory of Statistics and Geoinformation (LEG)"
institute: "UFPR/DEST/LEG"
#short-institute: "LEG"
section-titles: true
safe-columns: true # enables special latex macros for columns
output:
  legtheme::beamer_leg
---

### What we read (long description)

\begincols
 \column{.4\linewidth}
  \begin{figure}
   \centering
   \includegraphics[width=.95\textwidth]{book.jpg}
  \end{figure}
 \column{.6\linewidth}
  \begin{figure}
   \centering
   \includegraphics[width=\textwidth]{chap4.png}
  \end{figure}
\endcols

### What we read (short description)

At chapter 4 are discussed three of the most widely-used classifiers.

+ Logistic Regression

+ Linear Discriminant Analysis (LDA)

+ K-Nearest Neighbors (KNN)

\begin{block}{What we didn't read}
More computer-intensive methods are discussed in later chapters, such as
\begin{itemize}
\item Generalized Additive Models (GAM)
\item Trees
\item Random Forests
\item Boosting
\item Support Vector Machines (SVM)
\end{itemize}
\end{block}

# Why Not Linear Regression?

###

We could consider encoding the response, \(Y\), as a quantitative
variable, e.g.,

\begin{block}{Predict the medical condition of a patient on the basis of
her symptoms.}
\[
Y = \begin{cases}
  1 & \text{if}~\textcolor{beamer@UIUCblue}{\texttt{stroke}};\\
  2 & \text{if}~\textcolor{beamer@UIUCblue}{\texttt{drug overdose}};\\
  3 & \text{if}~\textcolor{beamer@UIUCblue}{\texttt{epileptic seizure}}.\\
\end{cases}
\]
\end{block}

\pause
\textcolor{beamer@UIUCorange}{Unfortunately, this coding implies an
ordering on the outcomes.}

Each possible coding would produce a fundamentally different linear
model that would ultimately lead to different sets of predictions.

### That leads us to other questions,

+ What if the response variable values did take on a natural ordering,
\textcolor{beamer@UIUCblue}{\texttt{such as mild}},
\textcolor{beamer@UIUCblue}{\texttt{moderate}}, and
\textcolor{beamer@UIUCblue}{\texttt{severe}}?

+ For a \textcolor{beamer@UIUCblue}{\texttt{binary}} (two level)
qualitative response, the situation is better.

   - However, if we use linear regression, some of our estimates might
     be outside the [0, 1] interval.

   - However, the dummy variable approach cannot be easily extended to
     accommodate qualitative responses with more than two levels.

\pause
\textcolor{beamer@UIUCorange}{For these reasons, it is preferable to use
a classification method that is truly suited for qualitative response
values, such as the ones presented next.}

\begin{block}{Curiously,}
it turns out that the classifications that we get if we use linear
regression to predict a binary response will be the same as for the
linear discriminant analysis (LDA) procedure we discuss later.
\end{block}

# A typical dataset

### A classic 'book example dataset relationship'

\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{iBagens/dataset_chap4.png}
\end{figure}

... a very pronounced relationship between `balance` and `default`.

# Logistic Regression

### To start, a comparison with Linear Regression

\begin{figure}
\centering
\includegraphics[width=\textwidth]{iBagens/linr-vs-logr.png}
\end{figure}

## The model framework

### Some math, but with just one predictor

\begin{block}{The model and its relations}
\[
p(X) = \underset{\begin{gathered}
                  \text{\textit{logistic}}\\[-\jot]
                  \text{\textit{function}}\\[-\jot]
                  (\textit{S-shaped})
                 \end{gathered}}{
        \underbrace{\frac{e^{\beta_{0}+\beta_{1}X}}{
         1+e^{\beta_{0}+\beta_{1}X}}}} \Rightarrow
       \underset{\textit{odds}~\in~(0, \infty)}{
        \underbrace{\frac{p(X)}{1-p(X)}}} = e^{\beta_{0}+\beta_{1}X}
       \Rightarrow \underset{\begin{gathered}
                              \text{\textit{log-odds}}\\[-\jot]
                              \text{or}\\[-\jot]
                              \text{\textit{logit}}
                             \end{gathered}}{
                    \underbrace{\log \frac{p(X)}{1-p(X)}}} =
                   \beta_{0}+\beta_{1}X
\]
\end{block}

\pause
For example,

\[
p(X) = 0.2~\Rightarrow~\frac{0.2}{1-0.2} = \frac{1}{4}
\quad \textit{ and } \quad
p(X) = 0.9~\Rightarrow~\frac{0.9}{1-0.9} = 9.
\]

## Estimating the Regression Coefficients

### Maximum likelihood

The estimates \(\hat{\beta}_{0}\) and  \(\hat{\beta}_{1}\) are chosen to
\textcolor{beamer@UIUCblue}{maximize} a math equation called a

\begin{block}{\textit{likelihood function}}
\[
l(\beta_{0}, \beta_{1}) =
\prod_{i:y_{i}=1} p(x_{i}) \prod_{i':y_{i'}=0} (1-p(x_{i'})).
\]
\end{block}

\pause
The coefficients \(\hat{\beta}_{0}\) and \(\hat{\beta}_{1}\) are
unknown, and must be estimated. The general method of
\textcolor{beamer@UIUCblue}{maximum likelihood} is preferred, since it
has better statistical properties.

Maximum likelihood is a very general approach that is used to fit many
of the non-linear models examined throughout the book. In the linear
regression setting, the least squares approach is in fact a special case
of maximum likelihood.

# Linear Discriminant Analysis (LDA)

# K-Nearest Neighbors (KNN)

### and...

\begin{figure}
\centering \includegraphics[width=.5\textwidth]{end.jpg}
\end{figure}
\hfill \small \href{mailto:laureano@ufpr.br}{laureano@ufpr.br}
