---
title: "QUASI-LIKELIHOOD FUNCTIONS"
subtitle: By Peter McCullagh, 1983
author: Henrique Laureano ([.github.io](https://henriquelaureano.github.io))
date: July 23, 2021
institute: LEG @ UFPR
classoption: [aspectratio=169]
output:
  beamer_presentation:
    includes:
      in_header: beamerheader.txt
---

###

\begin{columns}
 \column{3cm}
  \includegraphics[height=3cm]{logo/mccullagh_old.jpg}
 \column{12cm}
  \includegraphics[height=4.5cm]{
   logo/quasi_likelihood_functions-cover.png}
\end{columns}

\bigskip
\begin{columns}
 \column{3cm}
  \includegraphics[height=3cm]{logo/mccullagh_now.jpg}
 \column{12cm}
  \begin{enumerate}
   \item Distinguished Professor\\
         in the Department of Statistics @ University of Chicago;
   \vspace{0.15cm}
   \item Completed his PhD at Imperial College London,\\
         supervised by David Cox and Anthony Atkinson;
   \vspace{0.15cm}
   \item Also at Imperial College London,\\
         was the PhD supervisor of Gauss Cordeiro.
  \end{enumerate}
\end{columns}

# Introduction

### Introduction

\vspace{-0.1cm}
1. Likelihood fucntion with \textbf{exponential family form}\newline
   \hspace*{0.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
   MLE through \textbf{weighted least squares}

   - \textbf{variance (assumed) constant}:
     we minimize a sum of squared residuals;
   
   - \textbf{variance not constant}:\newline
     estimating equations can be thought as a generalization of the
     scoring method.

2. Likelihood function \textcolor{AlertOrange}{without} exponential
   family form\newline
   \hspace*{0.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
   In some cases: weighted least squares\newline
   \hspace*{1cm}\rotatebox[origin=c]{180}{$\Lsh$}
   Jorgensen, B. (1983). Maximum likelihood estimation and large sample
                         \newline
                         \hspace*{5.05cm}
                         inference for generalized linear and non-linear
                         \newline
                         \hspace*{5.05cm}
                         regression models. \textit{Biometrika} 70

\vspace{-0.1cm}
\begin{minipage}{14cm}
\begin{block}{Paper purposes}
 \begin{enumerate}
  \item Maximize the likelihood function through weighted least squares
        \newline
        \hspace*{.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
        In which class of problems;
  \item Weighted least squares under 2nd moment assumptions
        (\textbf{quasi-likelihood}).
 \end{enumerate}
\end{block}
\end{minipage}

# A class of likelihood functions

### A class of likelihood functions

\[
 \text{Log likelihood written in the form}: \qquad
 \sigma^{-2}\{\mathbf{y}^{\top}\bm{\theta} -
              \mathbf{b}(\bm{\theta}) - \mathbf{c}(\mathbf{y}, \sigma)\}
\]

\begin{center}
 \begin{minipage}{12.25cm}
  \begin{block}{The first two cumulants}
   By differentiating it and assuming that the support does not depend
   on \(\bm{\theta}\)\newline
   \hspace*{.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
   \(E(\mathbf{Y}) = \bm{\mu} = {\mathbf{b}}'(\bm{\theta})\) and
   \(\text{Cov}(\mathbf{Y}) = \sigma^{2}{\mathbf{b}}''(\bm{\theta})
                            = \sigma^{2}\mathbf{V}(\bm{\mu})\).
  \end{block}
 \end{minipage}
\end{center}

\bigskip
\begin{description}
 \item[In fact,] the \(r\)th order cumulants of \(\mathbf{Y}\) are given
                by \(\kappa_{r} = \sigma^{2r-2}
                                  \mathbf{b}^{(r)}(\bm{\theta})\).
\end{description}

\bigskip
\begin{enumerate}
 \item The first two cumulants describe the random component of the
       model;
 \item However, in applications it is usually the systematic or
       nonrandom variation that is of primary importance\newline
       \hspace*{0.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
       \(E(\mathbf{Y}) = \bm{\mu} = \bm{\mu}(\bm{\beta})\) or
       \(E\{(\mathbf{h}(\mathbf{Y}))\} = \bm{\psi}(\bm{\beta})\)
       (implicitly involving \(\sigma^{2}\)).
\end{enumerate}

### A class of likelihood functions

\begin{description}
 \item[If \(\sigma^{2}\) is known,] log-likelihood is an exponential
                                    family
  \vspace{0.15cm}\newline
  \hspace*{0.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
  variance and all higher order cumulants of \(\mathbf{Y}\) are\newline
  \hspace*{0.82cm}functions of the mean vector alone
  \vspace{0.15cm}\newline
  \hspace*{1cm}\rotatebox[origin=c]{180}{$\Lsh$}
  exponential, Poisson, multinomial, noncentral hypergeometric\newline
  \hspace*{1.31cm}and partial likelihoods (survival analysis)
  \vspace{0.15cm}\newline
  \hspace*{1.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
  MLE of \(\bm{\beta}\) through weighted least squares\newline
  \hspace*{2cm}\rotatebox[origin=c]{180}{$\Lsh$}
  \(\bm{\mu}\) and \(\sigma^{2}\) are orthogonal\newline
  \hspace*{2.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
  \(\bm{\beta}\) and \(\sigma^{2}\) also orthogonal;
 \bigskip
 \item[If \(\sigma^{2}\) is unknown,] log-likelihood is
          \textcolor{AlertOrange}{not generally} an exponential
          family\newline
          \hspace*{0.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
          However, MLE of \(\bm{\beta}\) still through weighted least
          squares\newline
          \hspace*{1cm}\rotatebox[origin=c]{180}{$\Lsh$}
          \textcolor{UniBlue}{If}
          \(E(\mathbf{Y})\) does not involve \(\sigma^{2}\).
\end{description}

### A class of likelihood functions

\vspace{-0.25cm}
\begin{minipage}{13cm}
 \begin{block}{Least square equations}
  \[
   \def\hat{\mathaccent "705E\relax}
   \mathbf{D}^{\top}\mathbf{V}^{-}
   \{\mathbf{y} - \bm{\mu}(\hat{\bm{\beta}})\} = \mathbf{0}, \qquad
   \text{for the parameters in} \quad
   E(\mathbf{Y}) = \bm{\mu}(\bm{\beta})
  \]
  \begin{multicols}{2}
   \begin{itemize}
    \item \(\mathbf{D} = d\bm{\mu}/d\bm{\beta}, \quad N \times p\);
    \item \(\mathbf{V}^{-}\) is a generalized inverse of \(\mathbf{V}\).
   \end{itemize}
  \end{multicols}
  \vspace{-0.15cm}
  Why its name?
  \vspace{0.15cm}
  \begin{enumerate}
   \item \textbf{Geometrical interpretation}:
         projections of the residual vector
         \(\def\hat{\mathaccent "705E\relax}
           \mathbf{y} - \bm{\mu}(\hat{\bm{\beta}}_{0})\) on to the
         tangent space of the solution locus \(\bm{\mu}(\bm{\beta})\);
   \vspace{0.15cm}
   \item These equations \textbf{do not} depend on \(\sigma^{2}\).
  \end{enumerate}
 \end{block}
\end{minipage}

\vspace{-0.1cm}
\begin{center}
 \begin{minipage}{12cm}
  \begin{block}{Newton-Raphson method}
   We replace the second derivative matrix by its expected value,
   \(\mathbf{D}^{\top}\mathbf{V}^{-}\mathbf{D}\)
   \[
    \def\hat{\mathaccent "705E\relax}
    \hat{\bm{\beta}}_{1} - \hat{\bm{\beta}}_{0} =
    (\mathbf{D}^{\top}\mathbf{V}^{-}\mathbf{D})^{-1}
    \mathbf{D}^{\top}\mathbf{V}^{-}(\mathbf{y} - \hat{\bm{\mu}}_{0}).
   \]
  \end{block}
 \end{minipage}
\end{center}

# Quasi-likelihood functions

### Quasi-likelihood functions

\begin{minipage}{14.5cm}
 \begin{block}{Reversing the natural order of assumptions}
  \begin{enumerate}
   \item Instead of taking the log-likelihood to be of the exponential
         family form and \textcolor{AlertOrange}{then} deriving its
         moments;
          
   \vspace{0.15cm}
   \item We begin with the moments and \textcolor{AlertOrange}{then}
         attempt to reconstruct the log-likelihood.
  \end{enumerate}
  \vspace{0.3cm}
  \[
   \text{The reconstituted function is called a
   \textbf{quasi-likelihood}}.
  \]
 \end{block}
\end{minipage}

\vspace{0.15cm}
\begin{columns}
\column{9cm}
The log-quasi-likelihood, function of \(\bm{\mu}\),\newline
is given by the system of partial differential equations
\column{6cm}
\begin{block}{}
 \[
  \frac{\partial \ell(\bm{\mu}; \mathbf{y})}{\partial \bm{\mu}} =
  \mathbf{V}^{-}(\bm{\mu})(\mathbf{y} - \bm{\mu}).
 \]
\end{block}
\end{columns}

Which extends \textbf{Wedderburn's (1974)} definition.

\begin{enumerate}
 \item We get \(\def\hat{\mathaccent "705E\relax} \hat{\bm{\beta}}\)
       from \(\def\hat{\mathaccent "705E\relax}
              \mathbf{D}^{\top}\mathbf{V}^{-}
              \{\mathbf{y} - \bm{\mu}(\hat{\bm{\beta}})\} = \mathbf{0}\)
       (\textbf{generalized least squares equations});
 \item There is no guarantee that
       \(\def\hat{\mathaccent "705E\relax} \hat{\bm{\beta}}\)
       is the MLE.
\end{enumerate}

# Properties of quasi-likelihood functions

### Properties of quasi-likelihood functions

\vspace{-0.15cm}
\begin{minipage}{13.5cm}
 \textit{Very similar to those of ordinary likelihoods
  \textcolor{AlertOrange}{except} that the nuisance parameter,
  \(\sigma^{2}\), when unknown, is treated separately from
  \(\bm{\beta}\) and is not estimated by weighted least squares.
}
\end{minipage}

\vspace{0.15cm}
The principal results fall into three classes:

\begin{enumerate}
 \item Those concerning the score function
       \(\mathbf{U}_{\beta} = \partial\ell / \partial\bm{\beta}\);
  \begin{block}{}
   \vspace{-0.5cm}
   \[
    \mathbf{U}_{\beta} =
    \mathbf{D}^{\top}\mathbf{V}^{-}(\mathbf{Y} - \bm{\mu}(\bm{\beta}))
    \quad \text{has zero mean and covariance matrix} \quad
    \sigma^{2}\mathbf{i}_{\beta} =
    \sigma^{2}\mathbf{D}^{\top}\mathbf{V}^{-}\mathbf{D}
   \]
  \end{block}
  \vspace{-0.15cm}
  where \(\mathbf{i}_{\beta}\) is the expected second derivative matrix
  of \(\ell(\bm{\mu}(\bm{\beta});\mathbf{Y})\).
  
 \vspace{0.15cm}
 \item Those concerning the estimator
       \(\def\hat{\mathaccent "705E\relax} \hat{\bm{\beta}}\);
  \begin{block}{}
   \vspace{-0.15cm}
   \[
    \def\hat{\mathaccent "705E\relax}
    \text{There exists a}~\hat{\bm{\beta}}~\text{satisfying}~
    \hat{\bm{\beta}} - \bm{\beta} =
    \mathbf{I}_{\beta^{\ast}}^{-1}\mathbf{U}_{\beta} \quad
    \text{(minimum asymptotic variance)}
   \]
  \end{block}
  \vspace{-0.15cm}
  where \(\mathbf{I}_{\beta}\) is the observed matrix of second
  derivatives\newline
  and \(\bm{\beta}^{\ast}\) is a point lying on the line segment joining
  \(\def\hat{\mathaccent "705E\relax}
    \hat{\bm{\beta}}~\text{and}~\bm{\beta}\).
\end{enumerate}

### Properties of quasi-likelihood functions

3. Those concerning the distribution of the quasi-likelihood-ratio
   statistic.
\begin{block}{}
 \vspace{-0.15cm}
 \[
  \def\hat{\mathaccent "705E\relax}
  2\ell(\hat{\bm{\beta}}; \mathbf{Y}) - 2\ell(\bm{\beta}; \mathbf{Y}) =
  \mathbf{U}_{\beta}^{\top}\mathbf{i}_{\beta}^{-1}\mathbf{U}_{\beta} +
  O_{p}(N^{-1/2}) \quad
  \text{is asymptotically} \quad \sigma^{2}\chi_{p}^{2}
 \]
\end{block}

The \textbf{asymptotic optimality} follows the same line\newline
as the optimal property of Gauss-Markov estimators.

The conclusions (asymptotic unbiasedness),

+ while they apply \textcolor{AlertOrange}{more widely} than the
  Gauss-Markov theorem,\newline
  are inevitably a \textcolor{AlertOrange}{little weaker} being
  asymptotic rather than exact.

# Estimation of \(\sigma^{2}\)

### Estimation of \(\sigma^{2}\)

# Examples of quasi-likelihood functions

### Examples of quasi-likelihood functions

# A higher order theory

### A higher order theory

### Take-home message

\vspace{-0.55cm}
\begin{minipage}{13cm}
 \begin{block}{}
  \textbf{The complete model works}. It's not magnificent, but it works.
  \vspace{0.2cm}
  \begin{enumerate}
   \item It works better in the high CIF scenarios;
   \vspace{0.15cm}
   \item As expected, as the sample size increases the results get
         better;
   \vspace{0.15cm}
   \item We do not see any considerable performance difference between
         cluster/family sizes;
   \vspace{0.15cm}
   \item Satisfactory full likelihood analysis under the maximum
         likelihood estimation framework (the estimates bias-variance
         could be smaller).
   \end{enumerate}
 \end{block}
\end{minipage}
\vspace{0.1cm}
What else can we do?

1. Instead of a conditional approach (latent effects model),\newline
   we can try a marginal approach e.g., an McGLM [@mcglm];

2. We can also try a copula [@copulas], on maybe two fronts:\newline 1)
   for a full specification; 2) to accommodate the within-cluster
   dependence.

\includegraphics[height=0.5cm]{logo/book.png}
For more read @laurence master thesis.

### Thanks for watching and have a great day

\vspace{0.5cm}
Special thanks to

\bigskip
\begin{columns}
 \column{1.5cm}
  \centering\includegraphics{logo/ppgmne-logo.png}
 \column{13.75cm}
  {\large \textbf{PPGMNE}}\newline
  Programa de Pós-Graduação em\newline
  Métodos Numéricos em Engenharia
\end{columns}

\bigskip
\begin{columns}
 \column{2cm}
  \includegraphics[height=2cm]{logo/capes.jpg}
 \column{3cm}
  \includegraphics[height=2cm]{logo/ufpr.png}
 \column{4.5cm}
  Joint work with
  \vspace{0.2cm}\newline
  Wagner H. Bonat\newline
  \url{http://leg.ufpr.br/~wagner}
 \column{4.75cm}
  \textcolor{white}{Joint work with}
  \vspace{0.2cm}\newline
  Paulo Justiniano Ribeiro Jr.\newline
  \url{http://leg.ufpr.br/~paulojus}
\end{columns}

\vspace{1.5cm}
\begin{columns}
 \column{10cm}
 \column{5cm}
  \includegraphics[height=0.3cm,angle=90]{../../laurence.jpg}
  \hspace{0.05cm}
  \href{https://henriquelaureano.github.io}{henriquelaureano.github.io}
\end{columns}
