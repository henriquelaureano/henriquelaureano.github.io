---
title: "Naive Bayes & Regress\\(\\~{a}\\)o Log\\(\\'{i}\\)stica"
# short-title: ""
author: "Henrique Laureano \\newline \\url{http://leg.ufpr.br/~henrique}"
short-author: "leg.ufpr.br/~henrique"
# email: "laureano\\@ufpr.br"
date: "\\href{https://cidamo.github.io/CiDWeek/}{CiDWeek I}, 03-07/02/2020"
short-date: "leg.ufpr.br/~henrique"
department: ""
institute: ""
# short-institute: "LEG"
section-titles: true
safe-columns: true # enables special latex macros for columns
output:
  legtheme::beamer_leg
---

```{r, include=FALSE}
library(knitr)

knit_hooks$set(rmdsize = function(before, options, envir) {
    if (before) "\\footnotesize"
    else "\\normalsize"
})

opts_chunk$set(rmdsize = TRUE,
               warning = FALSE,
               cache = TRUE,
               cache_path = "multinom_cache/")
```

### Naive Bayes

Primeiro, precisamos falar sobre o que \(\'{e}\) um
\textcolor{beamer@UIUCorange}{classificador de Bayes}.

\begin{block}{Classificador de Bayes}
 Um classificador probabil\(\'{i}\)stico  baseado no
 \textcolor{beamer@UIUCorange}{teorema de Bayes}.
\end{block}

\noindent{\color{beamer@UIUCblue}Exemplo,~\rule{.85\linewidth}{0.25mm}}

+ Meningite causa torcicolo 50% das vezes,
  \textcolor{beamer@UIUCorange}{\(\mathbb{P}[T | M]\)}

+ Prob. *a priori* de um paciente estar com meningite \(\'{e}\)
  1/50.000, \textcolor{beamer@UIUCorange}{\(\mathbb{P}[M]\)}

+ Probabilidade *a priori* de um paciente estar com torcicolo \(\'{e}\)
  1/20, \textcolor{beamer@UIUCorange}{\(\mathbb{P}[T]\)}

\begin{minipage}{.9\linewidth}
 Se um paciente est\(\'{a}\) com torcicolo, qual a probabilidade dele
 estar com meningite?
 \[ \textcolor{beamer@UIUCorange}{\mathbb{P}[M | T]} =
    \frac{\mathbb{P}[T | M]~\mathbb{P}[M]}{\mathbb{P}[T]} =
    \frac{1/2 \times 1/50.000}{1/20} = 0.0002.
 \]
\end{minipage}

### Classificadores Bayesianos

Considere \textcolor{beamer@UIUCorange}{atributos}
\(A_{1}, A_{2}, \dots A_{n}\) e uma \textcolor{beamer@UIUCorange}{classe}
\(C\) com r\(\'{o}\)tulos \(c_{1}, c_{2}, \dots c_{k}\).

O que queremos?
\[ \text{Predi\c{c}\~{a}o}: \quad
   C = c_{1} \text{ ou } C = c_{2} \text{ ou } \dots, \]
i.e., queremos o valor de \(C\) que maximiza
\(\mathbb{P}[C | A_{1}, A_{2}, \dots, A_{n}]\).

\begin{minipage}{.85\linewidth}
 \begin{block}{Como fazemos? Teorema de Bayes.}
  Calculamos a probabilidade \textit{a posteriori}
  \(\mathbb{P}[C | A_{1}, A_{2}, \dots, A_{n}]\) para todos os valores de
  \(C\),
  \[ \mathbb{P}[C_{k} | A_{1}, A_{2}, \dots, A_{n}] =
     \frac{\mathbb{P}[A_{1}, A_{2}, \dots, A_{n} | C_{k}]~
           \mathbb{P}[C_{k}]}{
                              \mathbb{P}[A_{1}, A_{2}, \dots, A_{n}]
                             }.
  \]
 \end{block}
\end{minipage}

\noindent{\color{beamer@UIUCorange}\rule{.85\linewidth}{0.25mm}}

E como calculamos \(\mathbb{P}[A_{1}, A_{2}, \dots, A_{n} | C_{k}]\)?
\textcolor{beamer@UIUCorange}{Naive Bayes}.

### Classificador Naive Bayes

\begin{block}{Por que \textit{naive}?}
 Porque se assume \textcolor{beamer@UIUCorange}{independ\(\^{e}\)ncia}
 entre os atributos \(A_{i }\) \underline{dado} uma classe, i.e.,
 \[ \mathbb{P}[A_{1}, A_{2}, \dots, A_{n} | C_{k}] =
    \mathbb{P}[A_{1} | C_{k}]~
    \mathbb{P}[A_{2} | C_{k}]~
    \dots~
    \mathbb{P}[A_{n} | C_{k}].
 \]
\end{block}

Vantagem: \underline{Grande} redu\(\c{c}\~{a}\)o do custo computational.

Um novo ponto \(\'{e}\) classificado como \(C_{k}\) se
\(\mathbb{P}[C_{k}] \times \prod_{i=1}^{n} \mathbb{P}[A_{i} | C_{k}]\)
\(\'{e}\) m\(\'{a}\)ximo.

\noindent{\color{beamer@UIUCorange}i.e.,~\rule{.95\linewidth}{0.25mm}}

\[ C_{k} = \underset{k \in \{1, \dots, K\}}{\text{argmax}}
           \mathbb{P}[C_{k}] \times
           \prod_{i=1}^{n} \mathbb{P}[A_{i} | C_{k}]
\]

### Exemplo: Estimando probabilidades a partir dos dados

\begin{columns}
 \begin{column}{.5\textwidth}
  \includegraphics[width=\linewidth]{naive_bayes-example.png}
 \end{column}
 \begin{column}{.5\textwidth}
  \begin{itemize}
   \item \(\mathbb{P}[C] = N_{k} / N\)

    \begin{itemize}
     \item \(\mathbb{P}[C = \text{No}] = 7 / 10\)
     \item \(\mathbb{P}[C = \text{Yes}] = 3 / 10\)
    \end{itemize}
  \end{itemize}

  \noindent{\color{beamer@UIUCblue}
            Atributos discretos,~\rule{.45\linewidth}{0.25mm}}

  \begin{itemize}
   \item \(\mathbb{P}[A_{i} | C_{k}] = A_{ik} / N_{k}\)

    \begin{itemize}
     \item \(\mathbb{P}[\text{Status} = \text{Married} | \text{No}] =
             4 / 7\)
     \item \(\mathbb{P}[\text{Refund} = \text{Yes} | \text{Yes}] = 0\)
     \item \(\dots\)
    \end{itemize}

  \end{itemize}
 \end{column}
\end{columns}

### E com atributos cont\(\'{i}\)nuos?

\begin{columns}
 \begin{column}{.4\textwidth}
  \includegraphics[width=\linewidth]{naive_bayes-example.png}
 \end{column}
 \begin{column}{.6\textwidth}
  \begin{block}{Estima\(\c{c}\~{a}\)o da densidade de probabilidade}
   \begin{itemize}
    \item Se assume distribui\(\c{c}\~{a}\)o Normal

    \item Se estima a m\(\'{e}\)dia \(\mu\) e o desvio padr\(\~{a}\)o
          \(\sigma\)

    \item Se estima a probabilidade condicional
     \[ \mathbb{P}[A_{i} | C_{k}] =
        \frac{\exp \left\{-\frac{(A_{i} - \mu_{ik})^{2}}{2\sigma_{ik}^{2}}
                   \right\}}{\sqrt{2\pi\sigma_{ik}^{2}}}
     \]
   \end{itemize}
  \end{block}
 \end{column}
\end{columns}

\noindent{\color{beamer@UIUCorange}Exemplo,~\rule{.75\linewidth}{0.25mm}}
\vspace{-.25cm}
\begin{minipage}{.85\linewidth}
 \[ \begin{aligned}
     \mathbb{P}[\text{Income} = 120 | \text{No}] &=
     \frac{1}{\sqrt{2\pi 2975}}
     \exp \left\{-\frac{(120 - 110)^{2}}{2\times 2975}\right\}\\
     &= 0.0072.
    \end{aligned} \]
\end{minipage}

### Classificador Naive Bayes: Exemplo

\begin{block}{Dado o perfil:
              \(X\) = (Refund = No, Married, Income = 120k)}
 \[ \begin{aligned}
     \mathbb{P}[X | \text{Class} = \text{No}] =
     ~&\mathbb{P}[\text{Refund} = \text{No} | \text{Class} = \text{No}]
       ~\times\\
      &\mathbb{P}[\text{Married} | \text{Class} = \text{No}]
       ~\times\\
      &\mathbb{P}[\text{Income} = 120k | \text{Class} = \text{No}]\\
   = ~& 4/7 \times 4/7 \times 0.0072 = 0.0024.
    \end{aligned}
 \]
 \[ \begin{aligned}
     \mathbb{P}[X | \text{Class} = \text{Yes}] =
     ~&\mathbb{P}[\text{Refund} = \text{No} | \text{Class} = \text{Yes}]
       ~\times\\
      &\mathbb{P}[\text{Married} | \text{Class} = \text{Yes}]
       ~\times\\
      &\mathbb{P}[\text{Income} = 120k | \text{Class} = \text{Yes}]\\
   = ~& 1 \times 0 \times 10^{-9} = 0.
    \end{aligned}
 \]
\end{block}
\vspace{-.3cm}
\noindent{\color{beamer@UIUCorange}\rule{\linewidth}{0.25mm}}

J\(\'{a}\) que
\(\mathbb{P}[X | \text{No}]~\mathbb{P}[\text{No}] >
  \mathbb{P}[X | \text{Yes}]~\mathbb{P}[\text{Yes}]\),
\[ \Rightarrow \quad
   \mathbb{P}[X | \text{No}] > \mathbb{P}[X | \text{Yes}]
   \Rightarrow
   \textcolor{beamer@UIUCorange}{\text{Class} = \text{No}}.
\]

###

\begin{block}{Problema de probabilidade zero}
 Se uma das probabilidades condicionais \(\'{e}\) zero, ent\(\~{a}\)o
 toda a express\(\~{a}\)o
 \[ \mathbb{P}[A_{1}, A_{2}, \dots, A_{n} | C_{k}] =
    \prod_{i=1}^{n} \mathbb{P}[A_{i} | C_{k}], \text{ se torna zero.}
 \]
\end{block}

Como evitamos isso?

Abordagens:
\[ \text{Original}: \quad \mathbb{P}[A_{i} | C_{k}] =
   \frac{N_{ik}}{N_{k}}, \qquad
   \textcolor{beamer@UIUCorange}{\text{Laplace}}: \quad
   \mathbb{P}[A_{i} | C_{k}] = \frac{N_{ik} + 1}{N_{k} + k},
\]
\[ \text{Estimativa-M}: \quad \mathbb{P}[A_{i} | C_{k}] =
   \frac{N_{ik} + mp}{N_{k} + m},
\]
em que \(p\) \(\'{e}\) uma probabilidade *a priori* e \(m\) \(\'{e}\) um
par\(\^{a}\)metro.
\noindent{\color{beamer@UIUCblue}\rule{.8\linewidth}{0.25mm}}

Qual a abordagem \(\'{e}\) mais utilizada? \(\newline\)
Corre\(\c{c}\~{a}\)o de \textcolor{beamer@UIUCorange}{Laplace}
(ou estimador de Laplace).

### Classificador Naive Bayes: Coment\(\'{a}\)rios

\noindent{\color{beamer@UIUCblue}Vantagens,~\rule{.5\linewidth}{0.25mm}}

\begin{itemize}
 \item F\(\'{a}\)cil de implementar

 \item Apresenta bons resultados na maioria dos cen\(\'{a}\)rios

 \item Robusto com \textit{outliers} e atributos irrelevantes

 \item Ignora dados faltantes durante o c\(\'{a}\)lculo das
       probabilidades
\end{itemize}

\noindent{\color{beamer@UIUCblue}Desvantagens,~\rule{.5\linewidth}{0.25mm}}

\begin{itemize}
 \item Suposi\(\c{c}\~{a}\)o de
       \textcolor{beamer@UIUCorange}{independ\(\^{e}\)ncia}

 \item Perda de acur\(\'{a}\)cia
\end{itemize}

\begin{minipage}{.825\linewidth}
 \begin{block}{Como lidar com essa depend\(\^{e}\)ncia?}
  \textcolor{beamer@UIUCorange}{Redes Bayesianas}: Um modelo
  gr\(\'{a}\)fico baseado em vari\(\'{a}\)veis condicionalmente
  independentes.
 \end{block}
\end{minipage}

### Regress\(\~{a}\)o Log\(\'{i}\)stica

\begin{minipage}{.5\linewidth}
 \begin{block}{Contexto}
  \[ \text{Regress\~{a}o}: \quad Y = X \beta + \epsilon. \]
 \end{block}
\end{minipage}

Log\(\'{i}\)stica? Quando? Quando \(Y\) \(\'{e}\)
\textcolor{beamer@UIUCorange}{qualitativa}.

\noindent
{\color{beamer@UIUCblue}Ideia!
E se n\(\'{o}\)s codificarmos \(Y\)?~\rule{.5\linewidth}{0.25mm}}

Assim podemos continuar usando a regress\(\~{a}\)o linear usual, e.g.,
\(\newline\) Queremos saber o que ocorreu com um paciente com base em
seus sintomas

\[ Y = \begin{cases}
        1 & \text{se overdose de drogas}\\
        2 & \text{se ataque epil\'{e}tico}
      \end{cases}
\]

\begin{minipage}{.5\linewidth}
 Problema! \(\newline\)
 Este tipo de codifica\(\c{c}\~{a}\)o implica num ordenamento das
 respostas.
\end{minipage}

### Por que usar Regress\(\~{a}\)o Log\(\'{i}\)stica?

\begin{columns}
 \begin{column}{.25\linewidth}
  \centering
  \includegraphics[width=.8\linewidth]{roll_safe.png}
 \end{column}
 \begin{column}{.75\linewidth}
  Ok, mas e se \(Y\) tiver uma ordena\(\c{c}\~{a}\)o natural? \(\newline\)
  e.g., leve, moderado e severo.
 \end{column}
\end{columns}

Se \(Y\) for bin\(\'{a}\)ria a regress\(\~{a}\)o linear at\(\'{e}\) que
funciona.

+ Contudo, as predi\(\c{c}\~{o}\)es podem ficar fora do intervalo
  \([0, 1]\).

\noindent{\color{beamer@UIUCblue}\rule{\linewidth}{0.25mm}}

\begin{minipage}{.75\linewidth}
 Por raz\(\~{o}\)es como esta que \(\'{e}\) prefer\(\'{i}\)vel o uso
 de m\(\'{e}\)todos de classifica\(\c{c}\~{a}\)o pr\(\'{o}\)prios
 para vari\(\'{a}\)veis qualitativas.
 \[ \text{M\'{e}todos de classifica\c{c}\~{a}o pr\'{o}prios? }
    \text{\textcolor{beamer@UIUCorange}{Regress\~{a}o Log\'{i}stica}}.
 \]
\end{minipage}

###

Regress\(\~{a}\)o Linear \(\times\) Regress\(\~{a}\)o Log\(\'{i}\)stica

```{r, echo=FALSE, fig.height=3.5}
pacman::p_load(ISLR, ggplot2, gridExtra, latex2exp)

grid.arrange(
    ggplot(Default, aes(x = balance, y = as.numeric(default) - 1)) +
    geom_point(size = 1, col = "#0080ff") +
    ylim(- .1, 1.05) +
    labs(x = NULL, y = NULL) +
    geom_hline(yintercept = c(0, 1),
               linetype = "dashed", col = "#0080ff") +
    geom_smooth(se = FALSE, method = "lm", col = "orange") +
    theme_minimal(base_size = 17) +
    theme(axis.text.x = element_blank()),
    ggplot(Default, aes(x = balance, y = as.numeric(default) - 1)) +
    geom_point(size = 1, col = "#0080ff") +
    ylim(- .1, 1.05) +
    labs(x = NULL, y = NULL) +
    geom_hline(yintercept = c(0, 1),
               linetype = "dashed", col = "#0080ff") +
    geom_smooth(se = FALSE, method = "glm", col = "orange",
                method.args = list(family = "binomial")) +
    theme_minimal(base_size = 17) +
    theme(axis.text.x = element_blank()),
    nrow = 1)
```

\noindent{\color{beamer@UIUCorange}\rule{\linewidth}{0.25mm}}

\[ \text{De onde vem esta forma em } S?
   \textcolor{beamer@UIUCorange}{\text{ fun\c{c}\~{a}o log\'{i}stica}}:
   \frac{e^{X\beta}}{1+e^{X\beta}}.
\]

###

\begin{columns}
 \begin{column}{.3\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{rambo.jpg}
 \end{column}
 \begin{column}{.7\linewidth}
  Ok, mas onde e como se usa essa
  \textcolor{beamer@UIUCorange}{fun\c{c}\~{a}o log\'{i}stica}?
 \end{column}
\end{columns}

Num modelo de regress\(\~{a}\)o temos
\(\mathbb{E}[Y | X] = \mu = g^{-1}(X \beta)\).

+ No caso Normal, \(g\) \(\'{e}\) uma fun\(\c{c}\~{a}\)o identidade.
  \(\newline\)
  O que configura a regress\(\~{a}\)o linear que todos conhecemos,
  \(Y = X\beta + \epsilon\).

+ Se assumida uma distribui\(\c{c}\~{a}\)o diferente da Normal para
  \(Y | X\), \(g\) ser\(\'{a}\) diferente da fun\(\c{c}\~{a}\)o
  identidade e assim teremos o que configura os chamados GLMs.

\noindent{\color{beamer@UIUCorange}\rule{\linewidth}{0.25mm}}

\begin{minipage}{.7\linewidth}
 \begin{block}{Regress\(\~{a}\)o Log\(\'{i}\)stica}
  Se \(Y\) for dicot\(\^{o}\)mica e \(g\) for a fun\(\c{c}\~{a}\)o
  log\(\'{i}\)stica, \(\newline\)
  ent\(\~{a}\)o temos uma regress\(\~{a}\)o log\(\'{i}\)stica.
 \end{block}
\end{minipage}

### Interpreta\(\c{c}\~{a}\)o?

\begin{center}
\includegraphics[width=.8\linewidth]{logistic-relations.jpg}
\end{center}
\vspace{1.25cm}

### Interpreta\(\c{c}\~{a}\)o? Raz\(\~{a}\)o de chances

\noindent{\color{beamer@UIUCorange}Chances,~\rule{.8\linewidth}{0.25mm}}
\[
 \text{chance}(X) = e^{X\beta} = \frac{g(X\beta)}{1 - g(X\beta)}.
\]
e.g.,
 \[ g(X\beta) = 0.2 \Rightarrow \frac{0.2}{1-0.2} = \frac{1}{4}, \quad
    g(X\beta) = 0.9 \Rightarrow \frac{0.9}{1-0.9} = 9.
 \]
\noindent
{\color{beamer@UIUCorange}Raz\(\~{a}\)o de chances,~\rule{.675\linewidth}{0.25mm}}

Para uma vari\(\'{a}\)vel cont\(\'{i}\)nua:
\[
 \frac{\text{chance}(x+1)}{\text{chance}(x)} =
 \frac{e^{\beta_{0}+\beta_{1}(x+1)}}{e^{\beta_{0}+\beta_{1}x}} =
 e^{\beta_{1}}.
\]

### M\(\'{a}\)xima verossimilhan\(\c{c}\)a

\noindent
{\color{beamer@UIUCorange}O qu\(\^{e}\)?~\rule{.75\linewidth}{0.25mm}}

Fun\(\c{c}\~{a}\)o de verossimilhan\(\c{c}\)a \(\'{e}\) o nome dado a
fun\(\c{c}\~{a}\)o que precisamos \textcolor{beamer@UIUCorange}{maximizar}.

\begin{minipage}{.6\linewidth}
 \begin{block}{Verossimilhan\(\c{c}\)a}
  \[ L(\beta) = \prod_{i: Y_{i}=1} g(X_{i}\beta)
                \prod_{{i}': Y_{{i}'}=0} (1 - g(X_{i}\beta))
  \]
 \end{block}
\end{minipage}

\noindent
{\color{beamer@UIUCorange}Por qu\(\^{e}\)?~\rule{.75\linewidth}{0.25mm}}

Queremos estimativas para \(\beta\), e tais estimativas s\(\~{a}\)o
obtidas via a maximiza\(\c{c}\~{a}\)o de \(L(\beta)\).

\noindent
{\color{beamer@UIUCorange}Como?~\rule{.7\linewidth}{0.25mm}}
\vspace{.1cm}
\begin{minipage}{.8\linewidth}
 \(L(\beta)\) \(\'{e}\) uma fun\(\c{c}\~{a}\)o "qualquer" que queremos
 otimizar. \(\newline\)
 Dependendo da fun\(\c{c}\~{a}\)o uma solu\(\c{c}\~{a}\)o
 anal\(\'{i}\)tica pode n\(\~{a}\)o existir, e a\(\'{i}\) m\(\'{e}\)todos
 num\(\'{e}\)ricos se fazem necess\(\'{a}\)rio.
\end{minipage}

### Software: \texttt{R}

\noindent
{\color{beamer@UIUCorange}Naive Bayes,~\rule{.55\linewidth}{0.25mm}}

```{r, eval=FALSE}
library(e1071)

modelo <- naiveBayes(Y ~ x1 + x2 + x3, data = dados)
## ou
modelo <- naiveBayes(Y ~ ., data = dados)
## se Y, x1, x2 e x3 forem todas as colunas de "dados"
```

\noindent
{\color{beamer@UIUCorange}
 Regress\(\~{a}\)o Log\(\'{i}\)stica,~\rule{.6\linewidth}{0.25mm}}

```{r, eval=FALSE}
modelo <- glm(Y ~ x1 + x2 + x3,
              family = binomial(link = "logit"), data = dados)
## ou
modelo <- glm(Y ~ ., family = binomial(link = "logit"), data = dados)
## se Y, x1, x2 e x3 forem todas as colunas de "dados"
```
\vspace{1.25cm}

###

\centering
\includegraphics[width=.95\linewidth]{thank_you.jpg}
