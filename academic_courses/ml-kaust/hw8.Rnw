\documentclass[12pt, oldfontcommands]{article}
\usepackage[english]{babel}
%\usepackage[brazilian, brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage[top = 2cm, left = 2cm, right = 2cm, bottom = 2cm]{geometry}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{multicol}
\usepackage{vwcol}
\usepackage[normalem]{ulem}
\usepackage{breqn}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{xcolor}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{makecell}
\setlength\parindent{0pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\newcommand{\Perp}{\perp \! \! \! \perp}
\newcommand{\nPerp}{\cancel{\perp \! \! \! \perp}}

\title{  
 \normalfont \normalsize 
 \textsc{CS 229 - Machine Learning} \\
 Xiangliang Zhang \\
 Computer Science (CS)/Statistics (STAT) Program \\
 Computer, Electrical and Mathematical Sciences \& Engineering (CEMSE) Division \\
 King Abdullah University of Science and Technology (KAUST) \\[25pt]
 \horrule{.5pt} \\ [.4cm]
 \LARGE HOMEWORK \\
  VIII
 \horrule{2pt} \\[ .5cm]}
 
\author{Henrique Aparecido Laureano}
\date{\normalsize Spring Semester 2018}

\begin{document}

\maketitle

%\newpage

\vspace{\fill}

\tableofcontents

\horrule{1pt} \\

\newpage

<<setup, include=FALSE>>=
# <r code> ===================================================================== #
library(knitr)

tema <- knit_theme$get("acid")

knit_theme$set(tema)

opts_chunk$set(size='small'
               , cache=TRUE
               , cache.path='cache/'
               , comment=NA
               , warning=FALSE
               , message=FALSE
               , fig.align='center'
               , dpi=100
               , fig.path='iBagens/'
               , fig.pos='H'
               , background='#ffffff'
               , results='hold'
               , fig.show='hold')
# </r code> ==================================================================== #
@

\begin{description}
 \item[Data]: All questions will use the data in: \texttt{clu_data.txt}.
<<data, fig.height=4, fig.width=4.25, fig.cap="\\texttt{clu_data} scatter plot.">>=
# <r code> ===================================================================== #
path <- "~/Dropbox/KAUST/machine_learning/hw8/"                       # files path
                                              # df: dataframe. reading the dataset
df <- read.table(paste0(path, "clu_data.txt"))
x <- df$V1 ; y <- df$V2                            # creating data vectors x and y
par(mar = c(4, 4, 0, 0) + .1) ; plot(x, y)  # graph. definitions and data plotting
# </r code> ==================================================================== #
@
\end{description}

\horrule{1pt}

\section*{[20 points] Question 1: Programming k-means}
\addcontentsline{toc}{section}{Question 1: Programming k-means}

\horrule{1pt}

\begin{description}
 \item \textbf{Write your own code of \(k\)-\textit{means} algorithm.}
 
 Here I'm computing the distance by
 
 \[ \sqrt{(x_{i} - C_{k}^{x})^{2} + (y_{i} - C_{k}^{y})^{2}}, \]
 
 where \(C_{k}\) represents the centroid of the cluster \(k\) and \(i\)
 represents the data points. \\
 
 The sum of squared error (SSE) is also computed, it is given by the
 following expression
 
 \[
  {\rm SSE} =
  \sum_{k = 1}^{K} \sum_{x_{i} \in {\rm Cluster}_{k}} (x_{i} - C_{k})^{2},
 \]
 
 where \(x_{i}\) represents the data points, \({\rm Cluster}_{k}\) represents
 the cluster \(k\), and \(C_{k}\) the respective cluster centroid.
 
<<fig.height=4, fig.width=4.25, fig.cap="yo yo yo.">>=
# <r code> ===================================================================== #
kmeans2.0 <- function(x, y, nclus, random.seed = 1) {
  set.seed(random.seed)                # the seed make all the values reproducible 
                                                       # defining random centroids
  cen.x = runif(n = nclus, min = min(x), max = max(x))   
  cen.y = runif(n = nclus, min = min(y), max = max(y))
                                                              # clusters centroids
  clus = data.frame(cluster = 1:nclus, cen.x = cen.x, cen.y = cen.y)
  df = data.frame(xs = x, ys = y, cluster = NA)  # data & cluster assignment in df
                  # vector to keep the sum of squared error (sse) for each cluster
  sse.clus = numeric(length(nclus))
  iter = 1                                                    # iterations counter
  sse = numeric(iter)                    # vector to keep the sum of squared error
  done = FALSE                                                # stopping criterion
  while(done == FALSE) {                   # while loop (where the beauty happens)
                            # computing the centroid distances for each data point
    for(i in 1:length(x)) { #    and assignment to the cluster of minimum distance
      distance = sqrt( (x[i] - clus$cen.x)**2 + (y[i] - clus$cen.y)**2 )
      df$cluster[i] = which.min(distance)
    }
    cen.xold = clus$cen.x ; cen.yold = clus$cen.y
    for(i in 1:nclus) {                 # updating centroids and computing the sse
      xs.clus = subset(df$xs, df$cluster == i)
      ys.clus = subset(df$ys, df$cluster == i)
      clus[i, 2] = mean(xs.clus)                                           # cen.x
      clus[i, 3] = mean(ys.clus)                                           # cen.y
      sse.clus[i] = sum( (xs.clus - clus[i, 2])**2 + (ys.clus - clus[i, 3])**2 )
    }
    sse[iter] = sum(sse.clus)                 # computing the sse of the iteration
                     # checking stopping criterion:
                     # stop the loop *if* there is no change in cluster assignment
    if(identical(cen.xold, clus$cen.x) & identical(cen.yold, clus$cen.y)) {
      done = TRUE
    } else iter = iter + 1                  # else, increase the iteration counter
  }
  return(list(data = df, sse = sse))                # returning data frame and sse
}
# </r code> ==================================================================== #
@
\end{description}

\subsection*{1)} \addcontentsline{toc}{subsection}{1)}

\horrule{.5pt}

\begin{description}
 \item
  \textbf{Try different settings of parameter \(k\). For \textit{each value of k},
          compute the \textit{SSE (sum of squared error)} of clustering result.
          Plot the \textit{SSE curve} w.r.t. the various \(k\) values.}

<<fig.height=6.45, fig.cap="Sum of squared error (SSE) curve w.r.t. different \\(k\\)'s (number of different clusters).">>=
# <r code> ===================================================================== #
par(mar = c(4, 4, 3, 1) + .1, mfrow = c(3, 3))             # graphical definitions
for (i in 4:10)                 # trying different k's and plotting the SSE curves
  plot(kmeans2.0(x, y, nclus = i)$sse, type = "b"
       , xlab = "Iterations", ylab = "SSE", main = paste(i, "clusters"))
# </r code> ==================================================================== #
@
\end{description}

\hfill \(\square\)

\subsection*{2)} \addcontentsline{toc}{subsection}{2)}

\horrule{.5pt}

\textbf{Does the SSE curve suggest the best clustering results?} \\

No. As \(k\) increases the SSE decrease. Therefore, choosing the smallest SSE
doesn't mean that it correspond, exactly, to the best clustering results. The
algorithm starts from random centroids, changing the initial centroids the
results can change.

\hfill \(\square\)

\subsection*{3)} \addcontentsline{toc}{subsection}{3)}

\horrule{.5pt}

\begin{description}
 \item
  \textbf{Plot the best clustering result you think (using different colors to
          show the different clusters), and answer:}

<<q1_clusters, fig.height=2.55, fig.cap="Clustering results for some number of clusters (in the graphic corresponding to 10 clusters the color black appears two times, but representing different clusters).">>=
# <r code> ===================================================================== #
par(mar = c(2, 2, 3, 1) + .1, mfrow = c(1, 3))             # graphical definitions

for (i in c(4, 8, 10))                          # plotting some clustering results
  plot(x, y, col = kmeans2.0(x, y, nclus = i)$data$cluster, xlab = NA, ylab = NA
       , main = paste(i, "clusters"))
# </r code> ==================================================================== #
@
\end{description}

\subsubsection*{a)} \addcontentsline{toc}{subsubsection}{a)}

\horrule{.25pt}

\textbf{How does \(k\)-means (with your setting of \(k\)) perform on clustering
        the data?} \\

Looking to the data scatter plot (Figure \ref{fig:data}) we could expect 4
clusters, 3 for the groups of points in the left and one big one for the rest of
the points. However we don't get this result. The algorithm only create the 3
clusters for the left groups when we allow 6 clusters or more for the
data. \\

With 8 and 10 clusters (Figure \ref{fig:q1_clusters}) the divisions are good, but
in none of the cases the algorithm captures perfectly the non-convex shape of
the data, instead, it breaks the data in small groups. This small, and closer,
clusters can be putted together in the end and make a unique cluster.
  
\hfill \(\square\)

\subsubsection*{b)} \addcontentsline{toc}{subsubsection}{b)}

\horrule{.25pt}

\textbf{And why?} \\

The \(k\)-means algorithm ins't suitable to discover clusters with different
sizes, different density, and non-convex shapes. The data studied here present
all this characteristics.

\hfill \(\square\)

\section*{[40 points] Question 2: \\
          \vspace{0cm} \hfill Programming hierarchical clustering algorithm}
\addcontentsline{toc}{section}{Question 2:
                               Programming hierarchical clustering algorithm}

\horrule{1pt}

\begin{description}
 \item
  \textbf{Write your own code of \textit{agglomerative hierarchical clustering}.}

The main \texttt{R} function responsable for hierarchical clustering is the
\texttt{hclust}. To be able to build the dendrograms without some crazy coding I
put a class \texttt{hclust} in my main function \texttt{cluster}, in this way with
a \texttt{plot} I can generate the dendrograms. For this I also compute some
things take the plot function need for the \texttt{hclust} class.

<<>>=
# <r code> ===================================================================== #
distance <- function(x) {                 # building the euclidian distance matrix
  x = as.matrix(x)
  u = apply(x*x, 1, sum) %*% matrix(1.0, 1, nrow(x))
  sqrt( abs( u + t(u) - 2 * x %*% t(x) ) )
}
iorder <- function(m) {                   # ordering to avoid crossing connections
  N = nrow(m) + 1
  iorder = rep(0, N)
  iorder[1] = m[N - 1, 1]
  iorder[2] = m[N - 1, 2]
  loc = 2
  for (i in seq(N - 2, 1)) {
    for (j in seq(1, loc)) {
      if (iorder[j] == i) {
        iorder[j] = m[i, 1]
        if (j == loc) {
          loc = loc + 1
          iorder[loc] = m[i, 2]
        } else {
          loc = loc + 1
          for (k in seq(loc, j + 2)) iorder[k] = iorder[k - 1]
          iorder[j + 1] = m[i, 2]
        }
      }
    }
  }
  - iorder
}                                                                  # main function
cluster <- function(d, method = c("single", "complete", "average")) {
  if (!is.matrix(d)) d = as.matrix(d)
  method_fn = switch(match.arg(method)             # picking a clustering function
                     , single = min, complete = max, average = mean)
  N = nrow(d)
  diag(d) = Inf
  n = -(1:N)                                             # tracks group membership
  m = matrix(0, nrow = N - 1, ncol = 2)                      # hclust merge output
  h = rep(0, N - 1)                                         # hclust height output
  for (j in seq(1, N - 1)) {
    h[j] = min(d)            # finding smallest distance and corresponding indices
    i = which(d - h[j] == 0, arr.ind = TRUE)
    i = i[1, , drop = FALSE]
    p = n[i]
    p = p[order(p)]                                    # ordering each m[j, ] pair
    m[j, ] = p
                                  # agglomerating the pair and all previous groups
                                  #     they belong to into the current j-th group
    grp = c( i, which(n %in% n[i[1, n[i] > 0]]) )
    n[grp] = j
    r = apply(d[i, ], 2, method_fn)
                               # moving on to the next minimum distance, excluding
                               #      current one by modifying the distance matrix
    d[min(i), ] = d[ , min(i)] = r
    d[min(i), min(i)] = Inf
    d[max(i), ] = d[ , max(i)] = Inf
  }
                           # returning something similar to the output from hclust
  structure(list(merge = m, height = h, order = iorder(m)), class = "hclust")
}
# </r code> ==================================================================== #
@
\end{description}

\subsection*{1)} \addcontentsline{toc}{subsection}{1)}

\horrule{.5pt}

\begin{description}
 \item
  \textbf{Use \textit{single linkage} (min, shortest distance). Plot the best
          clustering result you think. How do you think the performance of single
          linkage?}

<<fig.height=4, fig.cap="Cluster dendrogram using \\textit{single linkage}.">>=
# <r code> ===================================================================== #
par(mar = c(0, 4, 0, 0) + .1)                               # graphical definition
                                                          # computing and plotting
plot(cluster(distance(df), method = "single"), main = NULL, labels = FALSE)
# </r code> ==================================================================== #
@
\end{description}

By the dendrogram we can see five clusters well-defined. 

\hfill \(\square\)

\subsection*{2)} \addcontentsline{toc}{subsection}{2)}

\horrule{.5pt}

\begin{description}
 \item
  \textbf{Use \textit{complete linkage} (max, furthest distance). Plot the best
          clustering result you think. How do you think the performance of
          complete linkage?}

<<fig.height=4, fig.cap="Cluster dendrogram using \\textit{complete linkage}.">>=
# <r code> ===================================================================== #
par(mar = c(0, 4, 0, 0) + .1)                               # graphical definition
                                                          # computing and plotting
plot(cluster(distance(df), method = "complete"), main = NULL, labels = FALSE)
# </r code> ==================================================================== #
@
\end{description}

Comparing with the \textit{simple linkage} we see a very different behaviour. By
the dendrogram we can see four clusters (with the \textit{simple linkage} we
see five). Here, if you want to be more specific each chunk can be broken in
more clusters, here this divisions/cutting points are more clear.

\hfill \(\square\)

\subsection*{3)} \addcontentsline{toc}{subsection}{3)}

\horrule{.5pt}

\textbf{} \\

\begin{description}
 \item
  \textbf{Use \textit{average linkage} (average distance). Plot the best
          clustering result you think. How do you think the performance of average
          linkage?}

<<fig.height=4, fig.cap="Cluster dendrogram using \\textit{average linkage}.">>=
# <r code> ===================================================================== #
par(mar = c(0, 4, 0, 0) + .1)                               # graphical definition
                                                          # computing and plotting
plot(cluster(distance(df), method = "average"), main = NULL, labels = FALSE)
# </r code> ==================================================================== #
@
\end{description}

The behaviour here is more similar with the behaviour of the
\textit{complete linkage}. By the dendrogram we can see five clusters. If you want
to be more specific this clusters can be broken in more clusters, the divisions
are more evident with the last cluster (in the right).

\hfill \(\square\)

\section*{[40 points] Question 3: Programming DbScan algorithm}
\addcontentsline{toc}{section}{Question 3: Programming DbScan algorithm}

\horrule{1pt}

\begin{description}
 \item
  \textbf{Write your own code of \textit{DbScan} algorithm. Try different settings
          of parameter \textit{Minpts} and \textit{Eps}. Plot the best clustering
          result you think (using different colors to show the different
          clusters), and answer:}

<<>>=
# <r code> ===================================================================== #
dbscan2.0 <- function(data, eps, MinPts) {
  distcomb <- function(x, data) {
    data = t(data)
    temp = apply(x, 1, function(x) sqrt(colSums((data - x)**2)))
    return(t(temp))
  }
  data = as.matrix(data)
  n = nrow(data)
  classn = cv = integer(n)
  is.seed = logical(n)
  cn <- integer(1)
  for (i in 1:n) {
    unclass = (1:n)[cv < 1]
    if (cv[i] == 0) {
      reachables = unclass[
        as.vector(distcomb(data[i, , drop = FALSE],
                           data[unclass, , drop = FALSE])) <= eps]
      if (length(reachables) + classn[i] < MinPts) cv[i] = (-1)
      else {
        cn = cn + 1
        cv[i] = cn
        is.seed[i] = TRUE
        reachables = setdiff(reachables, i)
        unclass = setdiff(unclass, i)
        classn[reachables] = classn[reachables] + 1
        while (length(reachables)) {
          cv[reachables] = cn
          ap = reachables
          reachables = integer()
          for (i2 in seq(along = ap)) {
            j = ap[i2]
            jreachables = unclass[
              as.vector(distcomb(data[j, , drop = FALSE],
                                 data[unclass, , drop = FALSE])) <= eps]
            if (length(jreachables) + classn[j] >= MinPts) {
              is.seed[j] = TRUE
              cv[jreachables[cv[jreachables] < 0]] = cn
              reachables = union(reachables, jreachables[cv[jreachables] == 0])
            }
            classn[jreachables] = classn[jreachables] + 1
            unclass = setdiff(unclass, j)
          }
        }
      }
    }
    if (!length(unclass)) break
  }
  rm(classn)
  if (any(cv == (-1))) cv[cv == (-1)] <- 0
  out = list(cluster = cv, eps = eps, MinPts = MinPts)
  if (cn > 0) out$is.seed = is.seed
  return(out)
}
# </r code> ==================================================================== #
@
\end{description}

\subsection*{1)} \addcontentsline{toc}{subsection}{1)}

\horrule{.5pt}

\begin{description}
 \item
  \textbf{How the clustering result is changing when you increase \textit{Minpts}?
         }

<<fig.height=6.475, fig.cap="Clustering results for different \\textit{Minpts}. To differentiate some repetead colors we use different characters (circles and triangles). Outliers are represented by x's.">>=
# <r code> ===================================================================== #
par(mar = c(2, 2, 3, 1) + .1, mfrow = c(3, 3))
for (i in 5:13) {
  clust = dbscan2.0(df, eps = 2, MinPts = i)$cluster
  plot(x, y, col = clust + 1, pch = ifelse(clust == 0, 4, ifelse(clust > 8, 2, 1))
       , xlab = NA, ylab = NA, main = paste("Minpts:", i))} 
# </r code> ==================================================================== #
@
\end{description}

Here we used \textit{Eps} equal 2.

With small values for \textit{Minpts} we see that few clusters are identified. As
\textit{Minpts} increase the number of detected clusters also increase, reaching a
point where the clusters begin to divide into several small clusters. The best
clustering result in observed with \textit{Minpts} equal 8. With less than this
the three clusters of the left are not identified, and with more than 8 the
clusters start to split in several small clusters.

\hfill \(\square\)

\subsection*{2)} \addcontentsline{toc}{subsection}{2)}

\horrule{.5pt}

\begin{description}
 \item
  \textbf{How the clustering result is changing when you increase \textit{Eps}?}

<<fig.height=4.8, fig.cap="Clustering results for different \\textit{Eps}. To differentiate some repetead colors we use different characters (circles and triangles). Outliers are represented by x's.">>=
# <r code> ===================================================================== #
par(mar = c(2, 2, 3, 1) + .1, mfrow = c(2, 3))
for (i in c(.5, .75, 1, 1.25, 1.5, 1.75)) {
  clust = dbscan2.0(df, eps = i, MinPts = 8)$cluster
  plot(x, y, col = clust + 1, pch = ifelse(clust == 0, 4, ifelse(clust > 8, 2, 1))
       , xlab = NA, ylab = NA, main = paste("Eps:", i))}
# </r code> ==================================================================== #
@
<<fig.height=7.3, fig.cap="Clustering results for different \\textit{Eps}. To differentiate some repetead colors we use different characters (circles and triangles). Outliers are represented by x's.">>=
# <r code> ===================================================================== #
par(mar = c(2, 2, 3, 1) + .1, mfrow = c(3, 3))
for (i in c(1.8, 1.9, 2, 2.1, 2.15, 2.2, 2.5, 2.75, 3)) {
  clust = dbscan2.0(df, eps = i, MinPts = 8)$cluster
  plot(x, y, col = clust + 1, pch = ifelse(clust == 0, 4, ifelse(clust > 8, 2, 1))
       , xlab = NA, ylab = NA, main = paste("Eps:", i))}
# </r code> ==================================================================== #
@
\end{description}

Here we used \textit{Minpts} equal 8.

With small \textit{Eps}, 0.5 e.g., few points are detected and became clusters. As
\textit{Eps} increase reasonable clusters are formed. The best result is obtained
with \textit{Eps} equal 2.1. With 2.15, a very small difference, the difference in
the clustering is huge. With bigger values we see that all the data became a
cluster.

So, we see that with a small value the clusters aren't identified and with a big
value all the data became a cluster. The point here is find the intermediate
value that result in the good, reasonable, adequate clustering.

\hfill \(\square\)

\subsection*{3)} \addcontentsline{toc}{subsection}{3)}

\horrule{.5pt}

\begin{description}
 \item
  \textbf{How many \textit{core points}, \textit{border points} and
          \textit{outliers} do you have in your best clustering result?}

<<final, fig.height=4.25, fig.width=4.325, fig.cap="Best clustering result. Outliers are represented by x's.">>=
# <r code> ===================================================================== #
par(mar = c(2, 2, 3, 1) + .1)
clust = dbscan2.0(df, eps = 2.1, MinPts = 8)$cluster
plot(x, y, col = clust + 1, pch = ifelse(clust == 0, 4, 1), xlab = NA, ylab = NA
     , main = paste("Minpts: 8 and Eps: 2.1"))
legend(-3, -9, legend = 1:8, col = c(2:8, 1), pch = 1, bty = "n", ncol = 4)
# </r code> ==================================================================== #
@
<<>>=
# <r code> ===================================================================== #
best.clus <- dbscan2.0(df, eps = 2.1, MinPts = 8)

cross.table <- function (x) {
  tab = table(c("seed", "border")[2 - x$is.seed], cluster = x$cluster)
  tab = rbind(tab, total = colSums(tab))
  tab = cbind(tab, total = rowSums(tab))
  print(tab)
}
cross.table(best.clus)
# </r code> ==================================================================== #
@
\end{description}

In the \texttt{R} output above zero represent the outliers, so, we have 9 outliers
and eight clusters, as we can see in Figure \ref{fig:final}. In total we have 43
border points, and the cluster with more border points is the cluster one, with
10. In the \texttt{R} output we have the number of border points, the number of
non-border points, and the total number of points for each cluster.

\hfill \(\blacksquare\)

\horrule{.5pt}

\vspace{\fill}

\horrule{1pt} \\

\end{document}