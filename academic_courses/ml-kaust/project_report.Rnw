\documentclass[12pt]{article}
\usepackage[top=1.75cm, bottom=1.75cm, left=2.25cm, right=2.25cm]{geometry}
\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{bbding} % checkmarks
\usepackage{fancyhdr} % change the position of page number
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\fancyhf{} % clear all header and footers
\renewcommand{\headrulewidth}{0pt} % remove the header rule
\rfoot{\thepage} % puts the page number on the right side
\pagestyle{fancy}

\title{Project Report:\\
       \textbf{Bank Marketing Dataset:\\ An overview of classification algorithms}
       \\ CS229: Machine Learning}
\date{Spring Semester\\ 2018}
\author{Henrique Ap. Laureano\\ ID 158811 \vspace{.2cm}\\
        \includegraphics[width=.3\textwidth]{logo.pdf}\\
        \texttt{/KAUST/CEMSE/STAT}}

\begin{document}

\maketitle
\thispagestyle{empty}

\vfill
\noindent \horrule{.5pt} \vspace{-.95cm} \tableofcontents \noindent \horrule{.5pt}

<<setup, include=FALSE>>=
# <r code> ===================================================================== #
library(knitr)

tema <- knit_theme$get("acid")

knit_theme$set(tema)

opts_chunk$set(size='small'
               , cache=TRUE
               , cache.path='cache/'
               , comment=NA
               , warning=FALSE
               , message=FALSE
               , fig.align='center'
               , dpi=100
               , fig.path='iBagens/'
               , fig.pos='H'
               , background='#ffffff'
               , results='hold'
               , fig.show='hold'
               , echo=FALSE)
# </r code> ==================================================================== #
@

\section*{Data and goals}\addcontentsline{toc}{section}{Data and goals}

In this project we study different approachs to predict the sucess of bank
telemarketing. As instrument we have a dataset related with direct marketing
campaigns based on phone calls of a Portuguese banking institution. Often, more
than one contact to the same client was required, in order to access if the
product (bank term deposit) would be (\texttt{yes}) or not (\texttt{no})
subscribed.

The data under study here is called Bank Marketing Dataset (BMD) and he was
found in the Machine Learning Repository (UCI). The data is public available in
the url \url{https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#}. The size of
the dataset is considerably large, especially if we consider its origin. Data from
clients of financial institutions are usually difficult to find, and when found,
are rarely available in this quantity. In the BMD data we have 41188 observations,
with eighteen features.

The eighteen features are briefly described in Table \ref{tab:data}, were in the
left column we have the original feature name in the dataset, and in the right
column its description, mentioning also if the feature is numeric, categorial, and
with how many levels (if categorical, of course). The first one called of
\texttt{y} is the response, the desired target. The other features are presented
in the same order that they appear in the dataset.

To know better the data some descriptive analysis is performed, see Figure
\ref{fig:pairs} and Figure \ref{fig:barras}.

<<>>=
# <r code> ===================================================================== #
path <- "~/Dropbox/KAUST/machine_learning/project/report/"

df <- read.csv(paste0(path, "bank-additional-full.csv"), header = TRUE, sep = ";")
df <- df[ , -c(5, 11, 13)]

id <- sample(1:nrow(df), round(nrow(df) / 10, 0))

train <- df[-id, ] ; test <- df[id, ]
# </r code> ==================================================================== #
@

\begin{table}[H]
 \centering
 \caption{Features description of the Bank Marketing Dataset (BMD).}
 \label{tab:data}
 \vspace{.2cm}
 \begin{tabular}{r|l}
  \toprule
  \textbf{Feature} & \textbf{Description} \\
  \midrule
  \texttt{y} & desired target. has the client subscribed a term deposit?
               (\texttt{no}, \texttt{yes})\\
  \midrule
  \texttt{age} & numeric\\
  \midrule
  \texttt{job} & type of job, twelve categories\\
  \midrule
  \texttt{marital} & marital status, four categories\\
  \midrule
  \texttt{eduacation} & eight categories\\
  \midrule
  \texttt{housing} & has housing loan?
                     (\texttt{no}, \texttt{yes}, \texttt{unknown})\\
  \midrule
  \texttt{loan} & has personal loan?
                  (\texttt{no}, \texttt{yes}, \texttt{unknown})\\
  \midrule
  \texttt{contact} & contact communication type
                     (\texttt{cellular}, \texttt{telephone})\\
  \midrule
  \texttt{month} & last contact month of year (twelve levels, months)\\
  \midrule
  \texttt{day.of.week} & last contact day of the week (five levels, days)\\
  \midrule
  \texttt{campaign} & number of contacts performed during this campaign and for
                      this client\\
  \midrule
  \texttt{previous} & number of contacts performed before this campaign and for
                      this client\\
  \midrule
  \texttt{poutcome} & previous marketing campaign
                      (\texttt{failure}, \texttt{nonexistent}, \texttt{success})\\
  \midrule
  \texttt{emp.var.rate} & numeric.
                          employment variation rate - quarterly indicator\\
  \midrule
  \texttt{cons.price.idx} & numeric. consumer price index - monthly indicator\\
  \midrule
  \texttt{cons.conf.idx} & numeric.
                           consumer confidence index - monthly indicator\\
  \midrule
  \texttt{euribor3m} & numeric. euribor 3 month rate - daily indicator\\
  \midrule
  \texttt{nr.employed} & numeric. number of employees - quarterly indicator\\
  \bottomrule
 \end{tabular}
\end{table}

<<packages>>=
# <r code> ===================================================================== #
library(MASS)
library(e1071)
library(klaR)
library(FNN)
library(randomForest)
library(rpart)
library(partykit)
library(pROC)
library(latticeExtra)
# </r code> ==================================================================== #
@

<<pairs, fig.width=6.075, fig.height=6.075, fig.cap="Scatterplot lower triangular matrix and correlation upper triangular matrix for all the quantitative features presented in the Bank Marketing Dataset (BMD).">>=
# <r code> ===================================================================== #
quant <- df[ , c(1, 10, 11, 13:17)]
pairs(quant
      , upper.panel = function(x, y, ...) {
        usr <- par("usr") ; on.exit(par(usr)) ; par(usr = c(0, 1, 0, 1))
        r <- cor(x, y)
        txt <- format(c(r, 0.123456789), digits = 2)[1]
        text(.5, .5, txt, cex = 1.25)
      }
      , diag.panel = function(a, b, ...) {
        usr <- par("usr") ; on.exit(par(usr)) ; par(usr = c(0, 1, 0, 1))
        rect(0, 0, 1, 1, col = "#37B9DA")
      }
      , pch = 19, gap = .25, xaxt = "n", yaxt = "n"
      , col = c("#0080FF", "#F3953E")[df$y]
      , label.pos = .5, oma = c(1, 1, 1, 1))
# </r code> ==================================================================== #
@

In Figure \ref{fig:pairs} we see the scatterplots and correlations, two-by-two,
for all the eight numerical features in the BMD. In more than half of them we see
a random behaviour, that is also described by a correlation close to zero or
between the interval -0.3 and 0.3. A (very) strong (and positive) correlation is
seen in three cases. \texttt{emp.var.rate} vs. \texttt{euribor3m} (cor. 0.97),
\texttt{euribor3m} vs. \texttt{nr.employed} (cor. 0.95), and
\texttt{emp.var.rate} vs. \texttt{nr.employed} (cor. 0.91), i.e., involving only
three features - employment variation rate, Euro Interbank Offered Rate (Euribor)
and number of employees. During the analysis this point can be better studied.

Already in Figure \ref{fig:barras} we have the frequencies for each level of the
categorical features in the BMD. First, we see that the desired target is 
unbalanced, with more than 85\% of the observations corresponding to clients that
didn't subscribed to a term deposit. An equilibrium between levels is only present
in the \texttt{day.of.week} last contact feature. By this Figure we can also see
that the last contact of most of the clients was in may (\texttt{month} feature),
that most of the clients have a nonexistent previous marketing campaign
(\texttt{poutcome} feature), that they are married (\texttt{marital} feature) and
that most have a \texttt{job} in the administrative sector.

<<barras, fig.width=10, fig.height=13.7, fig.cap="Bar plots for all the qualitative features presented in the Bank Marketing Dataset (BMD).">>=
# <r code> ===================================================================== #
barras <- function(variable, nome, limite) {
  da = table(variable)
  barchart(sort(da)
           , col = "orange"
           , border = "transparent"
           , xlab = NULL
           , main = nome
           , scales = list(x = list(draw = FALSE))
           , xlim = limite
           , panel = function(...){
             panel.barchart(...)
             args <- list(...)
             panel.text(args$x, args$y, args$x, pos = 4, cex = .8)})
}
print(barras(df$job, "job", c(0, 15e3)),
      position = c(0, 3/4, 1/3, 1), more = TRUE)
print(barras(df$marital, "marital", c(0, 33e3)),
      position = c(1/3, 3/4, 2/3, 1), more = TRUE)
print(barras(df$education, "education", c(0, 19e3)),
      position = c(2/3, 3/4, 1, 1), more = TRUE)
print(barras(df$housing, "housing", c(0, 28e3)),
      position = c(0, 2/4, 1/3, 3/4), more = TRUE)
print(barras(df$loan, "loan", c(0, 44e3)),
      position = c(1/3, 2/4, 2/3, 3/4), more = TRUE)
print(barras(df$contact, "contact", c(0, 35e3)),
      position = c(2/3, 2/4, 1, 3/4), more = TRUE)
print(barras(df$month, "month", c(0, 18e3)),
      position = c(0, 1/4, 1/3, 2/4), more = TRUE)
print(barras(df$day_of_week, "day_of_week", c(0, 11e3)),
      position = c(1/3, 1/4, 2/3, 2/4), more = TRUE)
print(barras(df$poutcome, "poutcome", c(0, 47e3)),
      position = c(2/3, 1/4, 1, 2/4), more = TRUE)
print(barras(df$y, "y", c(0, 46e3)),
      position = c(1/3, 0, 2/3, 1/4))
# </r code> ==================================================================== #
@

Using this features, described in Table \ref{tab:data}, the goal here is test
several algorithms to see how good they are to predict the desired target, i.e.,
predict, given the seventeen features, if the bank term deposit would be or
not subscribed. The algorithms used for this task are described in the next
section, together with some extra informations about the analysis procedure.

\section*{Methods}\addcontentsline{toc}{section}{Methods}

To predict, given the seventeen features, if the bank term deposit would be or
not subscribed, fifteen algorithms are used. They are: four generalized linear
models with a Bernoulli response and with different link functions (logit,
probit, cauchit and complementary log-log); a standard linear regression model;
naive Bayes classifier; three discriminant analysis algorithms (linear, quadratic
and regularized); four support vector machines with different kernels (linear,
polynomial, radial and sigmoid); a random forest; and a decision tree.

As mentioned before, the BMD consists of 41188 observations. A random sample of
10\% of this size, 4119 observations, was withdrawn to be used as a test dataset.
The rest, 37069 observations, was used as a train dataset.

All the analysis are performed using the \texttt{R} \hyperlink{r}{[1]} language
and environment for statistical computing. To take advantage of the most efficient
available algorithm versions, we use some \texttt{R} libraries where the
algorithms are implemented. A brief description of the algorithms is given now,
always mentioning the corresponding \texttt{R} library where the algorithm is
implemented.

\subsection*{Generalized Linear regression Model (GLM) with Bernoulli response}

The GLM is a flexible generalization of ordinary linear regression that allows
responses with error distribution models other than a normal distribution. The GLM
generalizes linear regression by allowing the linear model to be related to the
response via a link function. In a GLM each outcome \(Y\) of the response is
assumed to be generated from a particular distribution in the exponential family,
a large range of probability distributions. The mean, \(\mu\), of the distribution
depends on the features, \(X\), through:

\[ {\rm E}(Y) = \boldsymbol{\mu} = g^{-1}(X \boldsymbol{\beta}), \]

where \({\rm E}(Y)\) is the expected value of \(Y\); \(X \boldsymbol{\beta}\) is
the linear predictor, a linear combination of unknown parameters
\(\boldsymbol{\beta}\); \(g\) is the link function. The unknown parameters,
\(\boldsymbol{\beta}\), are typically estimated with maximum likelihood.

When the response data, \(Y\), are binary (taking on only values 0 and 1), the
distribution function is generally chosen to be the Bernoulli distribution and the
interpretation of \(\mu_{i}\) is then the probability, \(p\), of \(Y_{i}\) taking
on the value one. The logit is the canonical link function and when used the
resulting model is called of logistic regression. However, other link function can
be used. The four most popular link functions, and used here, are:

\begin{itemize}
 \item Logit function: \(g(p) = \ln \left({p \over 1 - p} \right)\);
 \item Probit or inverse Normal function: \(g(p) = \Phi^{-1}(p)\);
 \item Cauchit function: \(\tan\left(\pi p - \frac{\pi}{2}\right)\);
 \item Complementary log-log function: \(g(p) = \log(-\log(1 - p))\).
\end{itemize}

To test the significance of the features we use the Akaike information criterion
(AIC). Given a collection of models, the AIC estimates the quality of each model,
relative to each of the other models. Thus, AIC provides a means for model 
selection. The AIC value of a given model is the following:

\[ {\rm AIC} = 2 par - 2 \log \widehat{L}. \]

With \(\widehat{L}\) being the maximum value of the likelihood function for the
model and \(par\) being the number of estimated parameters in the model.

More details about GLM can be see, for example, in
\url{https://en.wikipedia.org/wiki/Generalized_linear_model}.

\subsection*{Linear regression Model (LM)}

LM is a linear approach to modelling the relationship between a response and
features, where the response have a normal error distribution. Commonly, the
conditional mean of the response given the values of the features is assumed to be
an affine function of those values. This relationship is modeled through a
disturbance term or error \(\epsilon\) - an unobserved feature that adds "noise"
to the linear relationship between the response and features. Thus the model
takes, in matrix notation, the form

\[ Y = X \boldsymbol{\beta} + \boldsymbol{\varepsilon}. \]

The unknown parameters, \(\boldsymbol{\beta}\), are typically estimated via least
squares. Ordinary Least Squares (OLS) method minimizes the sum of squared
residuals, and leads to a closed-form  expression for the estimated value of the
unknown parameter \(\boldsymbol{\beta}\)

\[ \widehat{\boldsymbol{\beta}} = (X^{\top} X)^{-1} X^{\top} Y. \]

More details about LM can be see, for example, in
\url{https://en.wikipedia.org/wiki/Linear_regression}.

\subsection*{Naive Bayes classifier}

Naive Bayes classifiers are a family of simple "probabilistic classifiers" based
on applying Bayes' theorem with strong (naive) independence assumptions between
the features. Naive Bayes is a reference to the use of Bayes' theorem in the
classifier's decision rule, but is not (necessarily) a Bayesian method.

Considering each attribute and class label as random variables, given a record
with attributes \(A_{1}, A_{2}, \dots, A_{n}\), and a goal - predict class \(C\),
we want to find the value of C that maximizes
\({\rm P}(A_{1}, A_{2}, \dots, A_{n} \mid C) {\rm P}(C)\). The naive Bayes
classifier assume independence among attributes \(A_{i}\) when class is given,
therefore

\[
 {\rm P}(A_{1}, A_{2}, \dots, A_{n} \mid C) =
 {\rm P}(A_{1} \mid C_{j}) {\rm P}(A_{2} \mid C_{j}) \dots
 {\rm P}(A_{n} \mid C_{j}).
\]

This approach greatly reduces the computation cost - only counts the class
distribution, and can estimate \({\rm P}(A_{i} \mid C_{j})\) for all \(A_{i}\) and
\(C_{j}\). A new point is classified to \(C_{j}\) if
\(C_{j} \prod {\rm P}(A_{i} \mid C_{j})\) is maximal.

In \texttt{R} the main implementation of the naive Bayes classifier is found in
the \texttt{e1071} library \hyperlink{e1071}{[2]}.

\subsection*{Discriminant Analysis}

Linear Discriminant Analysis (LDA) or discriminant function analysis is a
generalization of Fisher's linear discriminant, a method used to find a linear
combination of features that characterizes or separates two or more classes of
objects or events.

LDA approaches the problem by assuming that the conditional probability density
functions (considering two classes) are both normally distributed with mean and
covariance parameters. Under this assumption, the Bayes optimal solution is to
predict points as being from the second class if the log of the likelihood ratios
is bigger than some threshold. Without any further assumptions, the resulting
classifier is referred to as QDA (Quadratic Discriminant Analysis). LDA instead
makes the additional simplifying homoscedasticity assumption (i.e. that the class
covariances are identical). More details about can be see, for example, in
\url{https://en.wikipedia.org/wiki/Linear_discriminant_analysis}.

Considering two more parameters that flexibilize the possible difference between
the covariance matrices between classes and the dependence between the same
covariances, we have a Regularized Discriminant Analysis (RDA).

In \texttt{R} the main implementation for LDA and QDA is found in the
\texttt{MASS} library \hyperlink{mass}{[3]}, and the main implementation for RDA
is found in the \texttt{klaR} library \hyperlink{klar}{[4]}.

\subsection*{Support Vector Machine (SVM)}

With SVM a data point is viewed as a \(p\)-dimensional vector (a list of \(p\)
numbers), and we want to know whether we can separate such points with a
(\(p\)-1)-dimensional hyperplane. There are many hyperplanes that might classify
the data. One reasonable choice as the best hyperplane is the one that represents
the largest separation, or margin, between the two classes. So we choose the
hyperplane so that the distance from it to the nearest data point on each side is
maximized. If such a hyperplane exists, it is known as the maximum-margin
hyperplane. More formally, a SVM constructs a hyperplane or set of hyperplanes in
a high- or infinite-dimensional space, which can be used for classification,
regression, or other tasks like outliers detection.

Often happens that the sets to discriminate are not linearly separable in that
space. We can construct nonlinear classifiers applying the kernel trick,
\(K(x_{i}, x_{j})\), to maximum-margin hyperplanes. Here we use the four most
common kernels in SVM

\begin{multicols}{2}
 \begin{itemize}
  \item Linear: \(K(x_{i}, x_{j}) = \left \langle x_{i}, x_{j} \right \rangle\)
  \item Polynomial \(K(x_{i}, x_{j}) =
                     (c_{0} + \gamma \left \langle x_{i}, x_{j} \right \rangle
                     )^{d}\)
  \columnbreak
  \item Radial: \(K(x_{i}, x_{j}) =
                  {\rm exp} (- \gamma \left \| x_{i}, x_{j} \right \|^{2})\)
  \item Sigmoid: \(K(x_{i}, x_{j}) =
                   {\rm tanh} (c_{0} +
                               \gamma \left \langle x_{i}, x_{j} \right \rangle
                              )\)
 \end{itemize}
\end{multicols}

More details about can be see, for example, in
\url{https://en.wikipedia.org/wiki/Support_vector_machine}. In \texttt{R} the main
implementation of SVM is found in the \texttt{e1071} library
\hyperlink{e1071}{[2]}.

\subsection*{Random forest}

Random forests are an ensemble learning method that operate by constructing a
multitude of decision trees at training time and outputting the class that is the
mode of the classes (classification) or mean prediction (regression) of the
individual trees. Random decision forests correct for decision trees' habit of
overfitting to their training set.

Random forests differ in only one way from tree bagging: they use a modified tree
learning algorithm that selects, at each candidate split in the learning process,
a random subset of the features. Tree bagging repeatedly selects a random sample
with replacement of the training set and fits trees to these samples. This
bootstrapping procedure leads to better model performance because it decreases the
variance of the model, without increasing the bias. More details about can be see,
for example, in \url{https://en.wikipedia.org/wiki/Random_forest}.

In \texttt{R} the main implementation of random forest is found in the
\texttt{randomForest} library \hyperlink{rf}{[5]}.

\subsection*{Decision tree}

A decision tree is a decision support tool that uses a tree-like graph or model of
decisions and their possible consequences, including chance event outcomes,
resource costs, and utility. It is one way to display an algorithm that only
contains conditional control statements. Decision trees are commonly used in
operations research, specifically in decision analysis, to help identify a
strategy most likely to reach a goal, but are also a popular tool in machine
learning.

A decision tree is a flowchart-like structure in which each internal node
represents a "test" on an attribute, each branch represents the outcome of the
test, and each leaf node represents a class label. The paths from root to leaf
represent classification rules. Algorithms for constructing decision trees usually
work top-down, by choosing a variable at each step that best splits the set of
items. Different algorithms use different metrics for measuring "best". These
generally measure the homogeneity of the target within the subsets and are applied
to each candidate subset, the resulting values are combined (e.g., averaged) to
provide a measure of the quality of the split.

In \texttt{R} the main implementation of decision tree is found in the
\texttt{rpart} library \hyperlink{rpart}{[6]}.

\section*{Results}\addcontentsline{toc}{section}{Results}

With the GLM's and LM we are able to do feature selection. Here we do this via
AIC. Which features are keeped and which features are dropped can be seen in
Table \ref{tab:features_selecion}.

The main measure that can be used to compare all the fifthteen algorithms is the
Receiver Operating Characteristic curve, i.e. ROC curve. A graphical plot that
illustrates the diagnostic ability of a binary classifier system as its
discrimination threshold is varied. The ROC curve is created by plotting the 
specificity, true negative rate, against the sensitivity, true positive rate, at
various threshold settings. More details about can be see, for example, in
\url{https://en.wikipedia.org/wiki/Receiver_operating_characteristic}. In
\texttt{R} the main implementation of the ROC curve is found in the \texttt{pROC}
library \hyperlink{roc}{[7]}.

When dealing with ROC curves the main measure returned is the Area Under the Curve
(AUC), that is qual to the probability that a classifier will rank a randomly
chosen positive instance higher than a randomly chosen negative one. The AUC for
each model is presented in Figure \ref{fig:aucs}. The highest is obtained with
the probit link function in the GLM.

Others measures as the specificity, sensitivity and the risk, are presented in
Table \ref{tab:results}. The risk here is defined as the proportion of
observations in the test dataset that are wrongly classified by the trained model.

The GLM's and the LM approachs return a probability for each observation, where
more close to zero means that the observation is more likely to be provinient from
the \texttt{no} class - a client that did not subscribed a term deposit. However,
with the ROC curve we obtain a optimized threshold for this decision. Thus, to
compute the risk in this models we use this obtained threshold, instead the
default value half - that in a first moment is the logical choice, since the
returned probability in between zero and one. This optimal threshold is defined
as the cutting point that returns the best specificity and sensitivity - in
general we don't want a good specificity value but with a bad sensitivity, or
vice-versa. We want the best possible value - combination - for both, at the same
time. So the threshold of this scenario is the used value to compute the risk in
this models. The others algorithms return directly the class label, not a
probability.

Again, this values - specificity, sensitivity and the classification risk/error -
can be checked in Table \ref{tab:results}.

\vspace{2.5cm}
\begin{table}[H]
 \centering
 \caption{Remaining features in each model after features selection by AIC.}
 \label{tab:features_selecion}
 \vspace{.2cm}
 \begin{tabular}{r|c|c|c|c|c}
  \toprule
  \multirow{2}{*}{\textbf{Feature}} & \multicolumn{5}{c}{\textbf{Model}} \\
  \cmidrule{2-6}
  & \textbf{Logistic} & \textbf{Probit} & \textbf{Cauchit} &
    \textbf{Comp. log-log} & \textbf{Least squares}\\
  \midrule
  \texttt{age} & & & & &\\
  \midrule
  \texttt{job} & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \Checkmark\\
  \midrule
  \texttt{marital} & & & & &\\
  \midrule
  \texttt{eduacation} & & & & &\\
  \midrule
  \texttt{housing} & & & & &\\
  \midrule
  \texttt{loan} & & & & &\\
  \midrule
  \texttt{contact} & \Checkmark & \Checkmark & \Checkmark & \Checkmark &
                     \Checkmark\\
  \midrule
  \texttt{month} & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \Checkmark
  \\ \midrule
  \texttt{day.of.week} & \Checkmark & \Checkmark & \Checkmark & \Checkmark &
                          \Checkmark\\
  \midrule
  \texttt{campaign} & \Checkmark & \Checkmark & \Checkmark & \Checkmark &
                       \Checkmark\\
  \midrule
  \texttt{previous} & & & & &\\
  \midrule
  \texttt{poutcome} & \Checkmark & \Checkmark & \Checkmark & \Checkmark &
                      \Checkmark\\
  \midrule
  \texttt{emp.var.rate} & \Checkmark & \Checkmark & \Checkmark & \Checkmark &
                           \Checkmark\\
  \midrule
  \texttt{cons.price.idx} & \Checkmark & \Checkmark & \Checkmark & \Checkmark &
                             \Checkmark\\
  \midrule
  \texttt{cons.conf.idx} & \Checkmark & \Checkmark & \Checkmark & \Checkmark &
                            \Checkmark\\
  \midrule
  \texttt{euribor3m} & \Checkmark & \Checkmark & \Checkmark & & \Checkmark\\
  \midrule
  \texttt{nr.employed} & \Checkmark & \Checkmark & \Checkmark & \Checkmark &\\
  \bottomrule
 \end{tabular}
\end{table}

<<cache.path="cache_logistic/", results='hide', dependson="packages">>=
# <r code> ===================================================================== #
# ---------------------------------------------------------- logistic regression #
logist <- glm(y ~ ., family = binomial, train) ; logist <- stepAIC(logist)
# </r code> ==================================================================== #
@

<<cache.path="cache_probit/", results='hide', dependson="packages">>=
# <r code> ===================================================================== #
# ------------------------------------------------------------ probit regression #
probit <- glm(y ~ ., family = binomial(link = "probit"), train)
probit <- stepAIC(probit)
# </r code> ==================================================================== #
@

<<cache.path="cache_cauchit/", results='hide', dependson="packages">>=
# <r code> ===================================================================== #
# ----------------------------------------------------------- cauchit regression #
cauchit <- glm(y ~ ., family = binomial(link = "cauchit"), train)
cauchit <- stepAIC(cauchit)
# </r code> ==================================================================== #
@

<<cache.path="cache_cloglog/", results='hide', dependson="packages">>=
# <r code> ===================================================================== #
# --------------------------------------------- complementary log-log regression #
cloglog <- glm(y ~ ., family = binomial(link = "cloglog"), train)
cloglog <- stepAIC(cloglog)
# </r code> ==================================================================== #
@

<<cache.path="cache_lm/", results='hide', dependson="packages">>=
# <r code> ===================================================================== #
# ------------------------------------------------------------ linear regression #
linear <- lm(as.numeric(y) ~ ., train) ; linear <- stepAIC(linear)
# </r code> ==================================================================== #
@

<<>>=
# <r code> ===================================================================== #
# ------------------------------------------------------------------ naive bayes #
naive <- naiveBayes(y ~ ., train)
# </r code> ==================================================================== #
@

<<>>=
# <r code> ===================================================================== #
# ------------------------------------------------- linear discriminant analysis #
lda.model <- lda(y ~ ., train)
# </r code> ==================================================================== #
@

<<>>=
# <r code> ===================================================================== #
# ---------------------------------------------- quadratic discriminant analysis #
qda.model <- qda(logist$formula, train)
# </r code> ==================================================================== #
@

<<cache.path="cache_rda/", dependson="packages">>=
# <r code> ===================================================================== #
# -------------------------------------------- regularized discriminant analysis #
rda.model <- rda(y ~ ., train)
# </r code> ==================================================================== #
@

<<cache.path="cache_svm-linear/", dependson="packages">>=
# <r code> ===================================================================== #
# ------------------------------------------------ linear support vector machine #
svm.linear <- svm(y ~ ., kernel = "linear", train)
# </r code> ==================================================================== #
@

<<cache.path="cache_svm-poly/", dependson="packages">>=
# <r code> ===================================================================== #
# -------------------------------------------- polynomial support vector machine #
svm.poly <- svm(y ~ ., kernel = "polynomial", train)
# </r code> ==================================================================== #
@

<<cache.path="cache_svm-radial/", dependson="packages">>=
# <r code> ===================================================================== #
# ------------------------------------------------ radial support vector machine #
svm.rad <- svm(y ~ ., kernel = "radial", train)
# </r code> ==================================================================== #
@

<<cache.path="cache_svm-sigmoid/", dependson="packages">>=
# <r code> ===================================================================== #
# ----------------------------------------------- sigmoid support vector machine #
svm.sigm <- svm(y ~ ., kernel = "sigmoid", train)
# </r code> ==================================================================== #
@

<<cache.path="cache_bagging/", dependson="packages">>=
# <r code> ===================================================================== #
# ---------------------------------------------------------------- random forest #
bag <- randomForest(x = train[, -18], y = train[ , 18], importance = FALSE)
# </r code> ==================================================================== #
@

<<>>=
# <r code> ===================================================================== #
# ---------------------------------------------------------- classification tree #
arvore <- rpart(y ~ ., train) ; arvore <- as.party(arvore)
# </r code> ==================================================================== #
@

<<aucs, fig.height=9.5, fig.width=6.5, fig.cap="ROC curve for each model (in the test) with respective AUC and thresholds.">>=
# <r code> ===================================================================== #
par(mfrow = c(5, 3))
# ---------------------------------------------------------- logistic regression #
plot.roc(roc(test$y, predict(logist, test, type = "response"))
         , main = "Logistic"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ------------------------------------------------------------ probit regression #
plot.roc(roc(test$y, predict(probit, test, type = "response"))
         , main = "Probit"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ----------------------------------------------------------- cauchit regression #
plot.roc(roc(test$y, predict(cauchit, test, type = "response"))
         , main = "Cauchit"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# --------------------------------------------- complementary log-log regression #
plot.roc(roc(test$y, predict(cloglog, test, type = "response"))
         , main = "Comp. log-log"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ------------------------------------------------------------ linear regression #
plot.roc(roc(test$y, predict(linear, test, type = "response"))
         , main = "Least squares"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ------------------------------------------------------------------ naive bayes #
plot.roc(roc(test$y, as.numeric(predict(naive, test)))
         , main = "Naive Bayes"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ------------------------------------------------- linear discriminant analysis #
plot.roc(roc(test$y, as.numeric(predict(lda.model, test)$class))
         , main = "Linear discriminant"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ---------------------------------------------- quadratic discriminant analysis #
plot.roc(roc(test$y, as.numeric(predict(qda.model, test)$class))
         , main = "Quadratic discriminant"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# -------------------------------------------- regularized discriminant analysis #
plot.roc(roc(test$y, as.numeric(predict(rda.model, test)$class))
         , main = "Regularized discriminant"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ------------------------------------------------ linear support vector machine #
plot.roc(roc(test$y, as.numeric(predict(svm.linear, test)))
         , main = "Linear SVM"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# -------------------------------------------- polynomial support vector machine #
plot.roc(roc(test$y, as.numeric(predict(svm.poly, test)))
         , main = "Polynomial SVM"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ------------------------------------------------ radial support vector machine #
plot.roc(roc(test$y, as.numeric(predict(svm.rad, test)))
         , main = "Radial SVM"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ----------------------------------------------- sigmoid support vector machine #
plot.roc(roc(test$y, as.numeric(predict(svm.sigm, test)))
         , main = "Sigmoid SVM"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ------------------------------------------------------------ --- random forest #
plot.roc(roc(test$y, as.numeric(predict(bag, test)))
         , main = "Random forest"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# ---------------------------------------------------------- classification tree #
plot.roc(roc(test$y, as.numeric(predict(arvore, test)))
         , main = "Decision tree"
         #, print.thres = TRUE
         , print.auc = TRUE, print.auc.cex = 1.25, print.auc.adj = c(.5, 3)
         , auc.polygon = TRUE, max.auc.polygon = TRUE)
# </r code> ==================================================================== #
@

\begin{table}[H]
 \centering
 \caption{Specificity, sensitivity and risk for each fitted model in the test
          Bank Marketing Dataset (BMD), in bold we have the best performances. The
          models in bold are the models with the best AUC.} \label{tab:results}
 \vspace{.2cm}
 \begin{tabular}{r|c|c|c}
  \toprule
  \textbf{Model} & \textbf{Specificity} & \textbf{Sensitivity} & \textbf{Risk} \\
  \midrule
  Logistic regression (GLM with logit link) & 0.891 & 0.609 &
  \Sexpr{round(
           mean(test$y != ifelse(predict(logist, test, type = "response") < .094
                                 , "no", "yes")), 3)}\\
  \midrule
  \textbf{Probit regression (GLM with probit link)} & 0.888 & 0.609 &
  \Sexpr{round(
           mean(test$y != ifelse(predict(probit, test, type = "response") < .102
                                 , "no", "yes")), 3)}\\
  \midrule
  Cauchit regression (GLM with cauchit link) & 0.893 & 0.594 &
  \Sexpr{round(
           mean(test$y != ifelse(predict(cauchit, test, type = "response") < .086
                                 , "no", "yes")), 3)}\\
  \midrule
  Comp. log-log regression (GLM with comp. log-log link) & 0.878 &\textbf{0.614} &
  \Sexpr{round(
           mean(test$y != ifelse(predict(cloglog, test, type = "response") < .078
                                 , "no", "yes")), 3)}\\
  \midrule
  Least squares regression (linear regression) & 0.886 & 0.609 &
  \Sexpr{round(
           mean(test$y != ifelse(predict(linear, test, type = "response") < 1.148
                                 , "no", "yes")), 3)}\\
  \midrule
  Naive Bayes & 0.829 & 0.621 & 0.192\\
  % round(mean(test$y != predict(naive, test)), 3)
  \midrule
  Linear discriminant analysis & 0.950 & 0.377 & 0.107\\
  % round(mean(test$y != predict(lda.model, test)$class), 3)
  \midrule
  Quadratic discriminant analysis & 0.897 & 0.558 & 0.137\\
  % round(mean(test$y != predict(qda.model, test)$class), 3)
  \midrule
  Radial discriminant analysis & 0.966 & 0.278 & 0.103\\
  % round(mean(test$y != predict(rda.model, test)$class), 3)
  \midrule
  Linear support vector machine & 0.985 & 0.188 & 0.095\\
  % round(mean(test$y != predict(svm.linear, test)), 3)
  \midrule
  Polynomial support vector machine & \textbf{0.992} & 0.171 & \textbf{0.09} \\
  % round(mean(test$y != predict(svm.poly, test)), 3)
  \midrule
  Radial support vector machine & 0.987 & 0.188 & 0.094\\
  % round(mean(test$y != predict(svm.rad, test)), 3)
  \midrule
  Sigmoid support vector machine & 0.939 & 0.227 & 0.133\\
  % round(mean(test$y != predict(svm.sigm, test)), 3)
  \midrule
  Random forest (bagging) & 0.973 & 0.309 & 0.093\\
  % round(mean(test$y != predict(bag, test)), 3)
  \midrule
  Decision tree & \textbf{0.992} & 0.176 & \textbf{0.09}\\
  % round(mean(test$y != predict(arvore, test, type = "response")), 3)
  \bottomrule
 \end{tabular}
\end{table}

\section*{Conclusion}\addcontentsline{toc}{section}{Conclusion}

Keep a feature means that the feature was significant, statistically significant,
in describing the difference between the classes of the desired target - if the
bank term deposit would be or not subscribed. In Table \ref{tab:features_selecion}
we can see a very high concordance between the models, in a general form. Each
model finished with eleven, from seventeen, features. This are the
dropped, nonsignificant in describing the difference between classes, features in
all models: \texttt{age}, \texttt{marital} status, \texttt{education},
\texttt{housing} loan, personal \texttt{loan}, and \texttt{previous} number of
contacts performed before this campaign and for this client.

Looking by the AUC, Figure \ref{fig:aucs}, the best model is the GLM with probit
link function. However, very similar values are obtained with the others link
functions and with the LM. With the other algorithms the AUC's are considerable
smaller, but always above 0.55 (a not bad value, but also not so good). Looking
to the other computed measures in Table \ref{tab:results}, we see that for all the
algorithms we obtain a very good specificity, true negative rate, and a bad or not
so good sensitivity, true positive rate. For all the algorithms we have a good
risk value, less than 0.30, but a very good risk value is obtained only with the
non-GLM/LM techniques.

The best specificities - pratically perfect - are obtained with the SVM with
polynomial kernel and with the decision tree. However, the sensitivities are very
low (nevertheless the risks are the lowest). The best sensitivity is obtained with
the GLM with complementaty log-log link function. The corresponding specificity of
0.878 is still pretty good, but the risk of 0.265 is not.

To summarize, in a general form we obtain a good specificity with all algorithms
- with some a vey good specificity. However, we only obtain a sensitivity above
0.5 with the GLM's, LM, naive Bayes and quadratic discriminant analysis. Using as
a final criterium the risk in the models with specificity and sensitivity above
0.6, we have the naive Bayes, LM and GLM with probit link function. This three
present very similar measures, making very hard to choose one between them tree.

\section*{References}\addcontentsline{toc}{section}{References}

\hypertarget{r}{[1] R Core Team (2017)}.
                R: A language and environment for statistical computing.\\
        \indent R Foundation for Statistical Computing, Vienna, Austria.
                \url{https://www.R-project.org/}. \vspace{.001cm} \\
\noindent
\hypertarget{e1071}{[2] Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A. and
                    Leisch, F. (2017)}.\\
            \indent e1071: Misc Functions of the Department of Statistics,
                    Probability Theory Group, TU Wien.\\
            \indent R package version 1.6-8.
                    \url{https://CRAN.R-project.org/package=e1071}.
            \vspace{.45cm} \\
\noindent
\hypertarget{mass}{[3] Venables, W. N. \& Ripley, B. D. (2002)}.
                   Modern Applied Statistics with S. Fourth Edition.\\
           \indent Springer, New York. ISBN 0-387-95457-0.
                   \url{http://www.stats.ox.ac.uk/pub/MASS4}. \vspace{.45cm} \\
\noindent
\hypertarget{klar}{[4] Weihs, C., Ligges, U., Luebke, K. and Raabe, N. (2005)}.\\
           \indent klaR Analyzing German Business Cycles. In Baier, D., Decker, R.
                   and Schmidt-Thieme, L.\\
           \indent (eds.). Data Analysis and Decision Support, 335-343,
                   Springer-Verlag, Berlin. \vspace{.45cm} \\
\noindent
\hypertarget{rf}{[5] Liaw, A. \& Wiener M. (2002)}.
                 Classification and Regression by randomForest.\\
         \indent R News 2(3), 18--22.
                 \url{http://CRAN.R-project.org/doc/Rnews/}. \vspace{.45cm} \\
\noindent
\hypertarget{rpart}{[6] Therneau, T., Atkinson, B. and Ripley, B. (2017)}. rpart:
                     Recursive Partitioning and Re-\\
             \indent gression Trees. R package version 4.1-11.
                     \url{https://CRAN.R-project.org/package=rpart}.
             \vspace{.025cm} \\
\noindent
\hypertarget{roc}{[7] Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F.,
                      Sanchez, JC. and M\"{u}ller, M.}\\
              \indent (2011). pROC: an open-source package for R and S+ to analyze
                      and compare ROC curves.\\
              \indent BMC Bioinformatics, 12, p. 77. DOI: 10.1186/1471-2105-12-77.
           \\ \indent \url{http://www.biomedcentral.com/1471-2105/12/77/}.

\end{document}