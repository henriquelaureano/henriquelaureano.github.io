\documentclass[12pt, oldfontcommands]{article}
\usepackage[english]{babel}
%\usepackage[brazilian, brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage[top = 2cm, left = 2cm, right = 2cm, bottom = 2cm]{geometry}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{multicol}
\usepackage{vwcol}
\usepackage[normalem]{ulem}
\usepackage{breqn}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{threeparttable}
\setlength\parindent{0pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

\title{
 \normalfont \normalsize 
 \textsc{STAT 260 - Nonparametric Statistics} \\
 Ying Sun \\
 Statistics (STAT) Program \\
 Computer, Electrical and Mathematical Sciences \& Engineering (CEMSE) Division \\
 King Abdullah University of Science and Technology (KAUST) \\[25pt]
 \horrule{.5pt} \\[.4cm]
 \LARGE HOMEWORK \\
  I
 \horrule{2pt} \\[.5cm]}
 
\author{Henrique Aparecido Laureano}
\date{\normalsize Spring Semester 2018}

\begin{document}

\maketitle

% \newpage

\vspace{\fill}

\tableofcontents

\horrule{1pt} \\

\newpage

<<setup, include=FALSE>>=
# <code r> ===================================================================== #
library(knitr)

tema <- knit_theme$get("acid")

knit_theme$set(tema)

opts_chunk$set(size='small'
               , cache=TRUE
               , cache.path='cache/'
               , comment=NA
               , warning=FALSE
               , message=FALSE
               , fig.align='center'
               , dpi=100
               , fig.path='iBagens/'
               , fig.pos='H'
               , background='#ffffff'
               , results='hold'
               , fig.show='hold')
# </code r> ==================================================================== #
@

\section*{Problem 1} \addcontentsline{toc}{section}{Problem 1}

\horrule{1pt} \\

\textbf{Question Q1.1-1.3 on page 15 of Topic 2.}

\subsection*{Q1.1}

\horrule{.5pt}

\[ \mathbb{V} \hat{\beta}_{0} =
              \sigma^{2} \frac{\sum_{i=1}^{n} x_{i}^{2}}{n S_{xx}} \quad ?
\]

\underline{Solution:}

\begin{align*}
 \mathbb{V} \hat{\beta}_{0} =
 \mathbb{V} (\bar{y} - \hat{\beta}_{1} \bar{x}) =
 \mathbb{V} \bar{y} + \bar{x}^{2} \mathbb{V} \hat{\beta}_{1} -
 2 \bar{x} {\rm Cov} (\bar{y}, \hat{\beta}_{1}).
\end{align*}

The variance terms are

\[ \mathbb{V} \bar{y} = \frac{1}{n^{2}} \sum_{i=1}^{n} \mathbb{V} y_{i}
                      = \frac{\sigma^{2}}{n}, \qquad
   \mathbb{V} \hat{\beta}_{1} =
   \frac{\sigma^{2}}{\sum_{i=1}^{n} (x_{i} - \bar{x})^{2}} =
   \frac{\sigma^{2}}{S_{xx}}.
\]

The covariance term is

\[ {\rm Cov} (\bar{y}, \hat{\beta}_{1}) = \mathbb{E} \bar{y} \hat{\beta}_{1} -
   \mathbb{E} \bar{y} \mathbb{E} \hat{\beta}_{1} =
   \beta_{1}(\beta_{0} + \beta_{1} \bar{x}) -
   (\beta_{0} + \beta_{1} \bar{x}) \beta_{1} = 0.
\]

So

\begin{align*}
 \mathbb{V} \hat{\beta}_{0} =
 \mathbb{V} (\bar{y} - \hat{\beta}_{1} \bar{x}) & =
 \mathbb{V} \bar{y} + \bar{x}^{2} \mathbb{V} \hat{\beta}_{1} -
 2 \bar{x} {\rm Cov} (\bar{y}, \hat{\beta}_{1}) \\ & =
 \frac{\sigma^{2}}{n} + \frac{\bar{x}^{2} \sigma^{2}}{S_{xx}} - 0 =
 \sigma^{2} \frac{S_{xx} + n \bar{x}^{2}}{n S_{xx}} =
 \sigma^{2} \frac{\sum_{i=1}^{n} (x_{i} - \bar{x})^{2} + n \bar{x}^{2}}{n S_{xx}}
 \\ & = \sigma^{2}
        \frac{\sum_{i=1}^{n} x_{i}^{2} - n \bar{x}^{2} + n \bar{x}^{2}}{n S_{xx}}
 = \sigma^{2} \frac{\sum_{i=1}^{n} x_{i}^{2}}{n S_{xx}}.
\end{align*}

Therefore

\[ \boxed{\mathbb{V} \hat{\beta}_{0} =
          \sigma^{2} \frac{\sum_{i=1}^{n} x_{i}^{2}}{n S_{xx}}.
         }
\]

\hfill \(\square\)

\subsection*{Q1.2}

\horrule{.5pt}

\[ {\rm Cov}(\hat{\beta}_{0}, \hat{\beta}_{1}) =
   -\sigma^{2} \frac{\bar{x}}{S_{xx}} \quad ?
\]

\underline{Solution:}

\begin{align*}
 {\rm Cov}(\hat{\beta}_{0}, \hat{\beta}_{1}) =
 \mathbb{E} (\hat{\beta}_{0} - \mathbb{E} \hat{\beta}_{0})
            (\hat{\beta}_{1} - \mathbb{E} \hat{\beta}_{1}) & =
 \mathbb{E} (\hat{\beta}_{0} - \beta_{0})
            (\hat{\beta}_{1} - \beta_{1}) \\ & =
 \mathbb{E} (\bar{y} - \hat{\beta}_{1} \bar{x} - \beta_{0})
            (\hat{\beta}_{1} - \beta_{1}) \\ & =
 \mathbb{E} (\beta_{0} + \beta_{1} \bar{x} - \hat{\beta}_{1} \bar{x} - \beta_{0})
            (\hat{\beta}_{1} - \beta_{1}) \\ & =
 \mathbb{E} (-(\hat{\beta}_{1} - \beta_{1}) \bar{x})
            (\hat{\beta}_{1} - \beta_{1}) \\ & =
 -\bar{x} \mathbb{E} (\hat{\beta}_{1} - \beta_{1})^{2} \\ & =
 -\bar{x}
 \mathbb{E} (\hat{\beta}_{1}^{2} - 2 \beta_{1} \hat{\beta}_{1} + \beta_{1}^{2}) \\
 & = -\bar{x}
     [\mathbb{E} \hat{\beta}_{1}^{2} - 2 \beta_{1} \mathbb{E} \hat{\beta}_{1}
      + \beta_{1}^{2}
     ] \\ & =
 -\bar{x}
 [\mathbb{V} \hat{\beta}_{1} + \mathbb{E}^{2} \hat{\beta}_{1} - \beta_{1}^{2}] \\
 & = -\bar{x}
     \bigg[\frac{\sigma^{2}}{S_{xx}} + \beta_{1}^{2} - \beta_{1}^{2}\bigg].
\end{align*}

Therefore,

\[ \boxed{{\rm Cov}(\hat{\beta}_{0}, \hat{\beta}_{1}) =
          -\frac{\sigma^{2} \bar{x}}{S_{xx}}.
         }
\]

\hfill \(\square\)

\subsection*{Q1.3}

\horrule{.5pt} \\

\textbf{What does mean} \(\mathbb{V} \hat{\beta}_{1} \neq 0\) ? \\

\underline{Solution:} \\

\fbox{\begin{minipage}{.98\textwidth}
       We don't know the real slope, \(\beta_{1}\), so \(\hat{\beta}_{1}\) is an
       estimative with mean \(\beta_{1}\) and with a respective variance. If the
       variance of \(\hat{\beta}_{1}\) is zero this means that, is this case,
       \(\hat{\beta}_{1}\) is exactly equal to \(\beta_{1}\).
      \end{minipage}
     } \\

\hfill \(\square\)

\textbf{What is the source of randomness in} \(\hat{\beta}_{1}\) ? \\

\underline{Solution:}

\[ \hat{\beta}_{1} =
   \frac{\sum_{i=1}^{n} y_{i}(x_{i}-\bar{x})}{\sum_{i=1}^{n} (x_{i}-\bar{x})^{2}}.
\]

\fbox{\begin{minipage}{.98\textwidth}
       The source of randomness is the data itself. In the numerator and
       denominator of the estimator we have the difference of each observation
       of the independent variable with its mean. This we can't control, depends
       from the data. This is the source of randomness.
      \end{minipage}
     } \\

\hfill \(\square\)

\textbf{How can you reduce} \(\mathbb{V} \hat{\beta}_{1}\) ? \\

\underline{Solution:}

\[ \mathbb{V} \hat{\beta}_{1}
   = \frac{\sigma^{2}}{S_{xx}}
    = \frac{\sigma^{2}}{\sum_{i=1}^{n} (x_{i}-\bar{x})^{2}}.
\]

\fbox{\begin{minipage}{.98\textwidth}
       Collecting more data. With a bigger sample size the component \(S_{xx}\) is
       bigger and consequently the variance is smaller.
      \end{minipage}
     } \\

\hfill \(\square\)

\section*{Problem 2} \addcontentsline{toc}{section}{Problem 2}

\horrule{1pt} \\

\textbf{Question Q2.1-2.2 on pages 16-17 of Topic 2.}

\subsection*{Q2.1}

\horrule{.5pt} \\

\textbf{Change the values of \(n\), sigma, see what happens.}

\vspace{2.25cm}
<<fig.width=10, fig.height=13.5>>=
# <code r> ===================================================================== #
n <- 30                                                    # picking a sample size
x <- cbind(x0 = rep(1, n), x1 = runif(n))           # making up a predictor matrix
sigma <- 1                                      # assuming a ’true’ error variance
beta <- c(b0 = 1, b1 = 2)                   # assuming a ’true’ coefficient vector
y <- x %*% beta + rnorm(n, 0, sigma)                # simulating a response vector
b <- solve(t(x) %*% x) %*% t(x) %*% y                              # ols estimator

# simulate and animate parallel universes/possible worlds/...
par(mfrow = c(10, 5), mar = c(4, 4, 2, 2) + .1)
for (i in 1:50) {
  y <- rnorm(n, x %*% beta, sigma)                  # simulating a response vector
  b <- solve(t(x) %*% x) %*% t(x) %*% y    
  plot(x[ , 2], y, pch = 16, col = "gray20", ylim = c(-2, 6), las = 1)
  abline(b, lwd = 3, col = "red")
}
# </code r> ==================================================================== #
@

\underline{Solution:} \\

In the fifth graphs presented before we see that all the samples for \(y\) are
very similar, given the small \texttt{sigma}, 1, that was used. In consequence,
the fitted lines was also vey similar in all the scenarios (parallel
universes/possible worlds). \\

To see what changes we elaborate four scenarios and for each one we run five
different samples for \(y\). Each scenario correspond to one line in the Figure
presented in the next page.

\begin{itemize}
 \item Scenario 1 (line 1):
       sample size 10 and standard deviation, \texttt{sigma}, 0.1;
 \item Scenario 2 (line 2): sample size 10 and standard deviation 10;
 \item Scenario 3 (line 3): sample size 100 and standard deviation 0.1;
 \item Scenario 4 (line 4): sample size 100 and standard deviation 10.
\end{itemize}

In the scenarios 1 and 3 we have an equal behaviour, all the fitted lines are
basically the same, given the small variance (\(0.1^{2}\)). This show that with
a small variance the sample size doesn't seem so important. \\

In the scenarios 2 and 4 we have a bigger variance, and in consequence we see
different fitted lines. However, with a small sample size the difference in the
lines is much bigger. In conclusion, the biggest differences are seen with a
small sample size and with a bigger variance.

\vspace{1.25cm}
<<fig.width=10, fig.height=8.5>>=
# <code r> ===================================================================== #
par(mfrow = c(4, 5), mar = c(4, 4, 2, 2) + .1)
q2.1 <- function(n, sigma, lim) {
  x <- cbind(x0 = rep(1, n), x1 = runif(n))         # making up a predictor matrix
  beta <- c(b0 = 1, b1 = 2)                 # assuming a ’true’ coefficient vector
  y <- x %*% beta + rnorm(n, 0, sigma)              # simulating a response vector
  b <- solve(t(x) %*% x) %*% t(x) %*% y                            # ols estimator
  # simulate and animate parallel universes/possible worlds/...
  for (i in 1:5) {
    y <- rnorm(n, x %*% beta, sigma)                # simulating a response vector
    b <- solve(t(x) %*% x) %*% t(x) %*% y    
    plot(x[ , 2], y, pch = 16, col = "gray20", ylim = lim, las = 1)
    abline(b, lwd = 3, col = "red")
  }
}
q2.1(n = 10,  sigma = .1, lim = c(1, 3))
q2.1(n = 10,  sigma = 10, lim = c(-15, 25))
q2.1(n = 100, sigma = .1, lim = c(1, 3))
q2.1(n = 100, sigma = 10, lim = c(-20, 30))
# </code r> ==================================================================== #
@

\hfill \(\square\)

\subsection*{Q2.2}

\horrule{.5pt} \\

\textbf{Change the values of \(n\), sigma, see what happens.}

<<fig.height=3.5>>=
# <code r> ===================================================================== #
n <- 30                                                    # picking a sample size
x <- cbind(x0 = rep(1, n), x1 = runif(n))           # making up a predictor matrix
sigma <- 1                                      # assuming a ’true’ error variance
beta <- c(b0 = 1, b1 = 2)                   # assuming a ’true’ coefficient vector
# next, simulate many response vectors (possible worlds, parallel universes,...)
#       and store the results:
nsim <- 1e4
bs <- matrix(NA, nrow = nsim, ncol = length(beta))       # storage for coefficient
                                                         #               estimates
colnames(bs) <- paste0("b", 0:(length(beta)-1))                        # cosmetics
for (isim in 1:nsim) {
  y <- x %*% beta + rnorm(n, 0, sigma)              # simulating a response vector
  bs[isim, ] <- solve(t(x) %*% x) %*% t(x) %*% y
  # if (isim %% 100 == 0) cat(isim, "...")
}
# plot slopes versus intercepts to illustrate the ’sampling’ or
# ’dataset-to-dataset’ variability of b:
par(mar = c(4, 4, 1, 2) + .1) ; plot(bs, pch = 16, cex = .5, las = 1)
# => negative correlation between b0 and b1 !!!
# </code r> ==================================================================== #
@

\underline{Solution:} \\

To see what changes we elaborate four scenarios, the results are presented in the
Figure in the next page.

\begin{itemize}
 \item Scenario 1 (line 1):
       sample size 10 and standard deviation, \texttt{sigma}, 0.1;
 \item Scenario 2 (line 2): sample size 10 and standard deviation 10;
 \item Scenario 3 (line 3): sample size 100 and standard deviation 0.1;
 \item Scenario 4 (line 4): sample size 100 and standard deviation 10.
\end{itemize}

<<fig.width=10>>=
# <code r> ===================================================================== #
par(mfrow = c(2, 2), mar = c(4, 4, 3, 2) + .1)
q2.2 <- function(n, sigma, title) {
  x <- cbind(x0 = rep(1, n), x1 = runif(n))         # making up a predictor matrix
  beta <- c(b0 = 1, b1 = 2)                 # assuming a ’true’ coefficient vector
  # next, simulate many response vectors (possible worlds, parallel universes,...)
  #       and store the results:
  nsim <- 1e4
  bs <- matrix(NA, nrow = nsim, ncol = length(beta))     # storage for coefficient
                                                         #               estimates
  colnames(bs) <- paste0("b", 0:(length(beta)-1))                      # cosmetics
  for (isim in 1:nsim) {
    y <- x %*% beta + rnorm(n, 0, sigma)            # simulating a response vector
    bs[isim, ] <- solve(t(x) %*% x) %*% t(x) %*% y
  }
  # plot slopes versus intercepts to illustrate the ’sampling’ or
  # ’dataset-to-dataset’ variability of b
  plot(bs, pch = 16, cex = .5, las = 1, main = title)
}
q2.2(n = 10,  sigma = .1, title = "n = 10, sigma = 0.1")
q2.2(n = 10,  sigma = 10, title = "n = 10, sigma = 10")
q2.2(n = 100, sigma = .1, title = "n = 100, sigma = 0.1")
q2.2(n = 100, sigma = 10, title = "n = 100, sigma = 10")
# </code r> ==================================================================== #
@

With the small sample size together with the bigger variance, scenario 2, we see
the biggest variance between the coefficients. The smallest variance between the
coefficients is seen with a bigger sample size and small variance, scenario 3. \\

Between the scenarios 1 and 4 the scenario 1 (with the small variance) is better
because present a small variance between the coefficients.

\hfill \(\square\)

\section*{Problem 3} \addcontentsline{toc}{section}{Problem 3}

\horrule{1pt} \\

\textbf{Question Q3 on page 19 of Topic 2.} \\

\textbf{Prove the following ANOVA Decomposition:}

\[ \underbrace{{\rm TSS}}_{\text{Total variations in } Y} =
   \underbrace{{\rm SS}_{\rm reg}}_{\text{Variations in } Y
                                    \text{ explained by model}} +
   \underbrace{{\rm RSS}}_{\text{Variations in } Y \text{ unexplained by model}}
\]

\underline{Solution:} \\

Notation:

\begin{align*}
 {\rm TSS} & = \sum_{i=1}^{n} (y_{i} - \bar{y})^{2}, \qquad
               \text{Total sum of squares} \\
 {\rm RSS} & = \sum_{i=1}^{n} (y_{i} - \hat{y})^{2}, \qquad
               \text{Residual sum of squares} \\
 {\rm SS}_{reg} & = \sum_{i=1}^{n} (\hat{y}_{i} - \bar{y})^{2}, \qquad
                    \text{Sum of squares in regression}
\end{align*}

with \(\hat{y} = \hat{\beta}_{0} + \hat{\beta}_{1} x_{i}\). \\

Proof:

\begin{align*}
 y_{i} - \bar{y} & = \hat{y}_{i} - \bar{y} + y_{i} - \hat{y}_{i} \\
 y_{i} - \bar{y} & = (\hat{y}_{i} - \bar{y}) + (y_{i} - \hat{y}_{i}) \\
 \sum_{i=1}^{n} (y_{i} - \bar{y})^{2} & =
 \sum_{i=1}^{n} \Big[(\hat{y}_{i} - \bar{y}) + (y_{i} - \hat{y}_{i})\Big]^{2} \\
 \sum_{i=1}^{n} (y_{i} - \bar{y})^{2} & =
 \sum_{i=1}^{n} \Big[(\hat{y}_{i} - \bar{y})^{2} + (y_{i} - \hat{y}_{i})^{2} +
                     2 (\hat{y}_{i} - \bar{y}) (y_{i} - \hat{y}_{i})
                \Big] \\
 \sum_{i=1}^{n} (y_{i} - \bar{y})^{2} & =
 \sum_{i=1}^{n} (\hat{y}_{i} - \bar{y})^{2} +
 \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2} +
 2 \sum_{i=1}^{n} (\hat{y}_{i} - \bar{y}) (y_{i} - \hat{y}_{i}),
\end{align*}

but

\begin{align*}
 \sum_{i=1}^{n} (\hat{y}_{i} - \bar{y}) (y_{i} - \hat{y}_{i}) & =
 \sum_{i=1}^{n} \Big[\hat{y}_{i} y_{i} - \hat{y}_{i}^{2} - \bar{y} y_{i} +
                     \bar{y} \hat{y}_{i}
                \Big] \\ & =
 \sum_{i=1}^{n} \hat{y}_{i} y_{i} - \sum_{i=1}^{n} \hat{y}_{i}^{2} -
 \bar{y} \sum_{i=1}^{n} y_{i} + \bar{y} \sum_{i=1}^{n} \hat{y}_{i} \\ & =
 \sum_{i=1}^{n} \hat{y}_{i} (y_{i} - \hat{y}_{i}) -
 \bar{y} \sum_{i=1}^{n} (y_{i} - \hat{y}_{i}) \\ & = 0,
\end{align*}

because

\begin{align*}
 i^{\rm th} \text{ residual } \qquad \Rightarrow \qquad
 e_{i} & = y_{i} - \hat{y}_{i} \\
 \sum_{i=1}^{n} e_{i} & = \sum_{i=1}^{n} (y_{i} - \hat{y}_{i}) \\
 & = \sum_{i=1}^{n} (y_{i} - \hat{\beta}_{0} - \hat{\beta}_{1} x_{i}) \\
 & = \sum_{i=1}^{n} y_{i} - n \hat{\beta}_{0}
                          - \hat{\beta}_{1} \sum_{i=1}^{n} x_{i} \\
 & = \sum_{i=1}^{n} y_{i} -
     n \bigg(\bar{y} - \hat{\beta}_{1} \sum_{i=1}^{n} \frac{x_{i}}{n}\bigg) -
     \hat{\beta}_{1} \sum_{i=1}^{n} x_{i} \\
 & = \sum_{i=1}^{n} y_{i} - \sum_{i=1}^{n} y_{i}
     + \hat{\beta}_{1} \sum_{i=1}^{n} x_{i} - \hat{\beta}_{1} \sum_{i=1}^{n} x_{i}
 \\ & = 0.
\end{align*}

Therefore,

\begin{align*}
 \sum_{i=1}^{n} (y_{i} - \bar{y})^{2} & =
 \sum_{i=1}^{n} (\hat{y}_{i} - \bar{y})^{2} +
 \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2} +
 2 \sum_{i=1}^{n} (\hat{y}_{i} - \bar{y}) (y_{i} - \hat{y}_{i}) \\
 & = \sum_{i=1}^{n} (\hat{y}_{i} - \bar{y})^{2} +
 \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2} + 0 \\
 \underbrace{\sum_{i=1}^{n} (y_{i} - \bar{y})^{2}}_{\rm TSS} & =
 \underbrace{\sum_{i=1}^{n} (\hat{y}_{i} - \bar{y})^{2}}_{{\rm SS}_{\rm reg}} +
 \underbrace{\sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2}}_{\rm RSS}.
\end{align*}

\hfill \(\square\)

\section*{Problem 4} \addcontentsline{toc}{section}{Problem 4}

\horrule{1pt} \\

\textbf{Question Q4 on page 20 of Topic 2.} \\

\textbf{Prove that for simple linear regression, \(R^{2}\) equals to \(r^{2}\),
        the sample correlation between \(X\) and \(Y\)}

\[ R^{2} = r^{2} =
   \frac{\Big(\sum_{i=1}^{n} (x_{i}-\bar{x}) (y_{i}-\bar{y})\Big)^{2}}{
         \sum_{i=1}^{n} (x_{i}-\bar{x})^{2} \sum_{i=1}^{n} (y_{i}-\bar{y})^{2}}.
\]

\underline{Solution:} \\

First,

\[ \text{we know that }  \quad
   R^{2} = \frac{{\rm SS}_{\rm reg}}{TSS}
         = \frac{\sum_{i=1}^{n} (\hat{y}_{i}-\bar{y})^{2}}{
                 \sum_{i=1}^{n} (y_{i}-\bar{y})^{2}}, \quad
  \hat{\beta}_{0} = \bar{y} - \hat{\beta}_{1} \bar{x} \quad {\rm and} \quad
  \hat{\beta}_{1} = \frac{\sum_{i=1}^{n} (x_{i}-\bar{x})(y_{i}-\bar{y})}{
                          \sum_{i=1}^{n} (x_{i}-\bar{x})^{2}}.
\]

So

\begin{align*}
 {\rm SS}_{\rm reg} = \sum_{i=1}^{n} (\hat{y}_{i}-\bar{y})^{2} & =
 \sum_{i=1}^{n} (\hat{\beta}_{0} + \hat{\beta}_{1} x_{i} - \bar{y})^{2} \\ & =
 \sum_{i=1}^{n}
 (\bar{y} - \hat{\beta}_{1} \bar{x} + \hat{\beta}_{1} x_{i} - \bar{y})^{2} \\ & =
 \hat{\beta}_{1}^{2} \sum_{i=1}^{n} (x_{i} - \bar{x})^{2} \\ & =
 \frac{\Big(\sum_{i=1}^{n} (x_{i}-\bar{x})(y_{i}-\bar{y})\Big)^{2}}{
       \Big(\sum_{i=1}^{n} (x_{i}-\bar{x})^{2}\Big)^{2}}
 \sum_{i=1}^{n} (x_{i} - \bar{x})^{2} \\ & =
 \frac{\Big(\sum_{i=1}^{n} (x_{i}-\bar{x})(y_{i}-\bar{y})\Big)^{2}}{
       \sum_{i=1}^{n} (x_{i}-\bar{x})^{2}}.
\end{align*}

Therefore,

\[ \boxed{
    R^{2} = \frac{{\rm SS}_{\rm reg}}{TSS}
          = \frac{\sum_{i=1}^{n} (\hat{y}_{i}-\bar{y})^{2}}{
                  \sum_{i=1}^{n} (y_{i}-\bar{y})^{2}} =
    \frac{\Big(\sum_{i=1}^{n} (x_{i}-\bar{x})(y_{i}-\bar{y})\Big)^{2}}{
          \sum_{i=1}^{n} (x_{i}-\bar{x})^{2} \sum_{i=1}^{n} (y_{i}-\bar{y})^{2}}
          = r^{2}.
         }
\]

\hfill \(\square\)

\section*{Problem 5} \addcontentsline{toc}{section}{Problem 5}

\horrule{1pt} \\

\textbf{Write your own version of \texttt{anova()} function on page 14 of Topic 3
        using \texttt{R}. Your function should not use \texttt{lm()} function.
        Must be able to compare two nested models. Replicate results on slides 14, 
        17, 18, 21 and 22 about the \texttt{gala} data set.} \\

\underline{Solution:}

\hfill \(\square\)

\section*{Problem 6} \addcontentsline{toc}{section}{Problem 6}

\horrule{1pt} \\

\textbf{Write an \texttt{R} function that can be used to automatically determine
        the order \textit{d} of the polynomial regression and replicate the
        results on pages 31 and 32 of Topic 3 about the \texttt{savings} data.
        (The output should be the chosen order \textit{d} and the plot of the
        fitted polynomial function to the data.)} \\

\underline{Solution:}

\hfill \(\blacksquare\)

\horrule{.5pt}

\vspace{\fill}

\horrule{1pt} \\

\end{document}