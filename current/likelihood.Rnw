\documentclass[12pt]{article}
\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry}
\usepackage{listings}
\usepackage{color} % red, green, blue, yellow, cyan, magenta, black, white
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[hidelinks]{hyperref}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{cite}
\usepackage{bbding} % checkmarks
\usepackage{fancyhdr} % change the position of page number
\usepackage[flushleft]{threeparttable} % footnote in tables
\usepackage{lipsum}
\usepackage{caption}
\captionsetup{format = hang}
\usepackage{color, colortbl} % coloring rows or columns in tables
\definecolor{Gray}{gray}{.9} % coloring rows or columns in tables
\definecolor{darkgreen}{RGB}{0,100,0}
\definecolor{darkred}{RGB}{100,0,0}
\usepackage{listings}
\lstset{ 
 language = R, % the language of the code
 numbers = left, % where to put the line-numbers
 numbersep = 6pt, % how far the line-numbers are from the code
 backgroundcolor = \color{white}, % choose the background color. You must add
                                  % \usepackage{color}
 showspaces = false, % show spaces adding particular underscores
 showstringspaces = false, % underline spaces within strings
 showtabs = false, % show tabs within strings adding particular underscores
 frame = single, % adds a frame around the code
 rulecolor = \color{black}, % if not set, the frame-color may be changed on
                            % line-breaks within not-black text
 tabsize = 2, % sets default tabsize to 2 spaces
 captionpos = b, % sets the caption-position to bottom
 breaklines = true, % sets automatic line breaking
 breakatwhitespace = false, % sets if automatic breaks should only happen at
                            % whitespace
 keywordstyle = \color{black}, % keyword style
 commentstyle = \color{darkgreen}, % comment style
 stringstyle = \color{darkred}, % string literal style
 basicstyle = \footnotesize\ttfamily,
}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\fancyhf{} % clear all header and footers
\renewcommand{\headrulewidth}{0pt} % remove the header rule
% changing the description label from bold to italic
\renewcommand{\descriptionlabel}[1]{\hspace{\labelsep}\textit{#1}}
\rfoot{\thepage} % puts the page number on the right side
\pagestyle{fancy}
\title{Exerc\'{i}cios sobre Verossimilhan\c{c}a - VI}
\date{Junho de 2019}
\author{Henrique Ap. Laureano\\
        \texttt{henriquelaureano@outlook.com} \(\wedge\)
        \texttt{http://mynameislaure.github.io/} \vspace{.25cm}\\
        \texttt{/UFPR/DEST/LEG/}\\
        \includegraphics[width=.225\textwidth]{logo_leg.jpg}}

\begin{document}

\maketitle
\thispagestyle{empty}

%\vfill
%\noindent \horrule{.5pt} \vspace{-.95cm} \tableofcontents \noindent \horrule{.5pt}
\noindent \horrule{.5pt}

<<setup, include=FALSE>>=
# <r code> -----
library(knitr)

tema <- knit_theme$get("clarity") # acid

knit_theme$set(tema)

opts_chunk$set(size='small'
               , cache=TRUE
               , cache.path='cache/'
               , comment=NA
               , warning=FALSE
               , message=FALSE
               , fig.align='center'
               , dpi=100
               , fig.path='iBagens/'
               , fig.pos='H'
               , results='hold'
               , fig.show='hold')
# </r code> -----
@

\begin{enumerate}
 \item Seja \(X\) uma v.a. com distribui\c{c}\~{a}o uniforme
       \(X \sim U(0, \theta)\).\\
       Uma amostra aleat\'{o}ria forneceu os seguintes valores:
       5,5; 3,2; 4;8, 5,3; 3,8 e 5,0. Obtenha a fun\c{c}\~{a}o de
       verossimilhan\c{c}a e um (ou mais) intervalo(s) adequado(s) especificando
       a forma de obten\c{c}\~{a}o.
<<>>=
# <r code> -----
data <- c(5.5, 3.2, 4.8, 5.3, 3.8, 5)
# </r code> -----
@

      Solution:
      
      The likelihood of a \({\rm Uniform}(0, \theta)\) is
      
      \begin{align*}
       L(\theta) &= \theta^{-n}, \quad \text{for } x_{i} < \theta
                                       \text{ for all } i\\
                 &= \theta^{-n}, \quad \text{for } \theta > x_{(n)},
      \end{align*}

      and equal to zero otherwise.\\
      Given the data above, we get \(x_{(n)} = \Sexpr{max(data)}\) and the
      likelihood is shown in Figure~\ref{fig:uniform}, together with some
      intervals.

<<uniform, fig.height=3.5, fig.cap=paste0("Likelihood function of \\(\\theta\\) in Uniform(0, \\(\\theta\\)) based on \\(x_{(6)}\\) = ", max(data), ". In the left, likelihood intervals at 15\\% (dashed line) and 4\\% (dotted line) cutoff. In the right, likelihood intervals at 5\\% (dashed line) and 1\\% (dotted line) cutoff. These cutoffs correspond, in both sides, to a 95\\% and 99\\% confidence interval, respectively.")>>=
# <r code> -----
lkl.unif <- function(theta, data) {
  n <- length(data) ; xn <- max(data)
  # this next step, in the way presented, isn't very efficient, since I'll
  # compute the likelihood for all theta's and then, if necessary, convert to
  # zero. however, since we have here a very simple likelihood (and this
  # notation is extremely clear, btw), I keep in this way
  lkl = theta**(-n)*{theta >= xn}
  return(lkl)
}
theta.seq <- seq(4, 12, length.out = 100)
lklseq.unif <- lkl.unif(theta.seq, data)

par(mfrow = c(1, 2), mar = c(4, 4, 2, 2) + .1) # cosmetics
# plotting the normalized likelihood function to have unit maximum 
plot(theta.seq, lklseq.unif/max(lklseq.unif), type = "l",
     xlab = expression(theta), ylab = "Likelihood")

# probability-based interval ------------------------------------------------
# cutoff's corresponding to a 95\% and 99\% confidence interval for the mean 
cuts <- c(.15, .04) * lkl.unif(max(data), data) / max(lklseq.unif)
abline(h = cuts, lty = 2:3)

# uniroot.all finds the zeros, so we need to subtract the cutoff point from
# the likelihood to be able to find the points where the likelihood is cut
ic.lkl.unif <- function(theta, cut, ...) {
  lkl.unif(theta, ...)/max(lklseq.unif) - cut
}
ic.95.prob <- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq),
                                     data = data, cut = cuts[1])
ic.99.prob <- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq),
                                     data = data, cut = cuts[2])
abline(v = ic.95.prob, lty = 2) ; abline(v = ic.99.prob, lty = 3)

plot(theta.seq, lklseq.unif/max(lklseq.unif), type = "l",
     xlab = expression(theta), ylab = "Likelihood")
# pure likelihood interval --------------------------------------------------
# cutoff's corresponding to a 95\% and 99\% confidence interval for the mean 
cuts <- c(.05, .01) * lkl.unif(max(data), data) / max(lklseq.unif)
abline(h = cuts, lty = 2:3)

ic.95.pure <- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq),
                                     data = data, cut = cuts[1])
ic.99.pure <- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq),
                                     data = data, cut = cuts[2])
abline(v = ic.95.pure, lty = 2) ; abline(v = ic.99.pure, lty = 3)
# </r code> -----
@

      In both graphs of Figure~\ref{fig:uniform} we have two confidence
      intervals for \(\theta\), one corresponding to a 95\% confidence interval
      (dashed line) and one corresponding to a 99\% confidence interval for the
      mean (dotted line). In the left we have a probability-based likelihood
      interval, in the right we have a pure likelihood interval.
      
      The left ones are bad intervals, since they're based in a large-sample
      theory that results in an exact interval for the Gaussian case, in a good
      approximation for a reasonably regular case, and as we can see by the
      figure... here we doesn't have a regular likelihood (the likelihood isn't
      well approximated by a quadratic function), thus, here this interval
      should not be so good.
      
      \begin{description}
       \item[Explaining better how we arrived in these intervals:]
       \hfill\\
       For a normalized likelihood, \(L(\theta)/L(\hat{\theta})\), we have the
       following Wilk's likelihood ratio statistic, defined as \(W\)

       \[
        W \equiv 2 \log \frac{L(\hat{\theta})}{L(\theta)} \sim \chi_{1}^{2}.
       \]
       Its \(\chi^{2}\) distribution is exact only in the normal mean model,
       and approximately true when the likelihood is reasonably regular.
       
       Based in this Wilk's statistic we are able to find the probability that
       the likelihood interval covers \(\theta\),

       \[
        {\rm Pr}\left\{\frac{L(\theta)}{L(\hat{\theta})} > c\right\} =
        {\rm Pr}\left\{2 \log \frac{L(\theta)}{L(\hat{\theta})} < -2 \log c
               \right\} = {\rm Pr}\{\chi_{1}^{2} < -2 \log c\}.
       \]

       So, if for some \(0 < \alpha < 1\) we choose a cutoff
      
       \[ c = e^{-\frac{1}{2} \chi_{1, (1-\alpha)}^{2}}, \]
      
       where \(\chi_{1, (1-\alpha)}^{2}\) is the \(100 (1-\alpha)\) percentile
       of \(\chi_{1}^{2}\).\\
       For \(\alpha = 0.05\) and \(0.01\) we have a cutoff \(c = 0.15\) and
       \(0.04\), that corresponds to a 95\% and 99\% confidence interval,
       respectively. \hfill \(\square\)
      \end{description}

      Having the cutoff values we need now to find the interval values,
      i.e., in which points the cutoff horizontal line cuts the likelihood.
      For this purpose we use the function \texttt{rootSolve::uniroot.all}.\\
      A likelihood interval at 15\% and 4\% cutoff for \(\theta\) are
      (\Sexpr{paste(round(ic.95.prob, 3), collapse = ", ")}) and
      (\Sexpr{paste(round(ic.99.prob, 3), collapse = ", ")}).
    
      Already on the right side of Figure~\ref{fig:uniform} we have a more
      coherent, let's say, confidence interval for the Uniform likelihood.
      While the likelihood isn't regular, it is still possible to provide an
      exact theoretical justification for a confidence interval interpretation.
      Now
      
      \[
       {\rm Pr}\left\{\frac{L(\theta)}{L(\hat{\theta})} > c\right\} =
       {\rm Pr}\left\{\frac{X_{(n)}}{\theta} > c^{1/n}\right\} =
       1 - {\rm Pr}\left\{\frac{X_{(n)}}{\theta} < c^{1/n}\right\} =
       1 - (c^{1/n})^{n} = 1 - c.
      \]
      So the likelihood interval with cutoff \(c\) is a \(100 (1-c)\)\%
      confidence interval.\\
      A likelihood interval at 15\% and 4\% cutoff for \(\theta\) are
      (\Sexpr{paste(round(ic.95.pure, 3), collapse = ", ")}) and
      (\Sexpr{paste(round(ic.99.pure, 3), collapse = ", ")}). \\
      
      Comparing the obtained intervals we see a broader range with the pure
      likelihood intervals. In other words, with the pure likelihood intervals
      we see a higher uncertainty than with the (not so recommended here)
      probability-based likelihood intervals.
      
      \vfill
      \textit{For doing this exercise I read and used it Pawitan's book:}
<<eval=FALSE>>=
@book{pawitan,
  author    = {Yudi Pawitan},
  title     = {In All Likelihood:
               Statistical Modelling and Inference Using Likelihood},
  year      = {1991},
  publisher = {Oxford University Press},
  address   = {Great Clarendon Street, Oxford OX2 6DP},
}
@
\newpage

 \item Seja \(X\) uma v.a. de uma distribui\c{c}\~{a}o de Poisson
       (\(X \sim P(\lambda))\)) para a qual foi obtida a seguinte amostra
       aleat\'{o}ria: (3, 1, 0, 2, 1, 1, 0, 0).
<<>>=
# <r code> -----
data <- c(3, 1, 0, 2, 1, 1, 0, 0)
# </r code> -----
@
  \begin{enumerate}[(a)]
   \item Obtenha a fun\c{c}\~{a}o de verossimilhan\c{c}a, sua
         aproxima\c{c}\~{a}o quadr\'{a}tica e intervalos para \(\lambda\).
         
         Solution:
         
         The likelihood and the log-likelihood of a Poisson(\(\lambda\)) is
         
         \[
          L(\lambda) = \prod_{i=1}^{n}
                       \frac{e^{-\lambda} \lambda^{x_{i}}}{x_{i}!}
                       \quad \text{and} \quad
          \log L(\lambda) =
          l(\lambda) =
          -n \lambda + \log \lambda \sum_{i=1}^{n} x_{i}
                     - \sum_{i=1}^{n} \log x_{i}!.
         \]
         
         \textit{We work here with the log-likelihood to make our life easier,
                 since the computations are much simpler in the log scale.}
         
         By computing the Score (derivative of \(l(\lambda)\) wrt to
         \(\lambda\)) and making it equal to zero, we find the MLE
         \(\hat{\lambda} = \bar{x}\).
         Computing the second derivative we find the observed information
         
         \[
          I_{O}(\lambda) = \frac{n \bar{x}}{\lambda^{2}}
          \quad \rightarrow \quad
          I_{O}(\hat{\lambda}) = \frac{n}{\bar{x}}.
         \]
         
         A graph of the Poisson log-likelihood is provided in black in
         Figure~\ref{fig:poisson}.
         
         Doing a quadratic approximation (Taylor expansion of second order) in
         \(l(\lambda)\) around \(\hat{\lambda}\) we have
         
         \begin{align*}
          l(\lambda) &\approx l(\hat{\lambda})
                     + (\lambda - \hat{\lambda}) {l}'(\hat{\lambda})
                     + \frac{1}{2} (\lambda - \hat{\lambda})^{2}
                                   {l}''(\hat{\lambda})\\
                     &\quad(\text{the Score is zero at the MLE})\\
                     &= l(\hat{\lambda})
                     + \frac{1}{2} (\lambda - \hat{\lambda})^{2}
                                   {l}''(\hat{\lambda})
                     = l(\hat{\lambda})
                     + \frac{1}{2} (\lambda - \hat{\lambda})^{2}
                                   I_{O}(\hat{\lambda}).
         \end{align*}
         
         A graph of the quadratic approximation of the Poisson log-likelihood
         is provided in red in Figure~\ref{fig:poisson}. There we can see how
         good is the approximation around the maximum likelihood estimator
         (MLE). Here the sample size is very small, the idea is that as the
         sample size increase the quality, in this case the range, of the
         approximation also increase. This should happen because as the sample
         size increase the Poisson likelihood should be more symmetric.
         
         In the topright graph of Figure~\ref{fig:poisson} we have two
         intervals for \(\lambda\).
         
<<poisson, fig.height=5.5, fig.cap="log-likelihood function and MLE of \\(\\lambda\\) in Poisson(\\(\\lambda\\)) based on \\texttt{data}. In the topleft, a quadratic approximation in red. In the topright, two intervals for \\(\\lambda\\) - one based in the likelihood (in black) and one based in the quadratic approximation (in red). In the bottom we provide a better look in the quadratic approximation.">>=
# <r code> -----
# likelihood -------------------------------------------------------------
lkl.poi <- function(lambda, data) {
  n <- length(data)
  # log-likelihood ignoring irrelevant constant terms
  lkl = -n * lambda + sum(data) * log(lambda)
  return(lkl)
}
lambda.seq <- seq(0, 4.5, length.out = 100)
lklseq.poi <- lkl.poi(lambda.seq, data)
par(mar = c(4, 4, 2, 2) + .1) # cosmetics
layout(matrix(c(1, 3, 2, 3), 2, 2), heights = c(1.5, 1)) # more cosmetics
plot(lambda.seq, lklseq.poi, type = "l",
     xlab = expression(lambda), ylab = "log-likelihood")
lambda.est <- mean(data) # MLE
abline(v = lambda.est, lty = 3, col = 4)
# quadratic approximation ------------------------------------------------
quadprox.poi <- function(lambda, lambda.est, data) {
  n <- length(data)
  obs.info <- n / lambda.est # observed information
  lkl.poi(lambda.est, data) - .5 * obs.info * (lambda - lambda.est)**2
}
curve(quadprox.poi(x, lambda.est, data), col = 2, add = TRUE)
legend(2.4, -6.25, c("log-like", "Quadratic\napprox.", "MLE"),
       col = c(1, 2, 4), lty = c(1, 1, 3), bty = "n")
# intervals for lambda ---------------------------------------------------
plot(lambda.seq, lklseq.poi, type = "l",
     xlab = expression(lambda), ylab = "log-likelihood")
curve(quadprox.poi(x, lambda.est, data), col = 2, add = TRUE)
abline(v = lambda.est, col = 4, lty = 3)
## probability-based interval --------------------------------------------
# cutoff's corresponding to a 95\% confidence interval for the mean
cut <- log(.15) + lkl.poi(lambda.est, data) ; abline(h = cut, lty = 2)
ic.lkl.poi <- function(lambda, cut, ...) {
  lkl.poi(lambda, ...) - cut
}
ic.95.prob <- rootSolve::uniroot.all(ic.lkl.poi, range(lambda.seq),
                                     data = data, cut = cut)
arrows(x0 = ic.95.prob, y0 = rep(cut, 2),
       x1 = ic.95.prob, y1 = rep(-25, 2), lty = 2, length = .1)
## wald confidence interval  ---------------------------------------------
cut <- log(.15) + quadprox.poi(lambda.est, lambda.est, data)
abline(h = cut, lty = 2, col = 2)
se.lambda.est <- sqrt(lambda.est / length(data))
wald.95 <- lambda.est + qnorm(c(.025, .975)) * se.lambda.est
arrows(x0 = wald.95, y0 = rep(cut, 2),
       x1 = wald.95, y1 = rep(-25, 2), lty = 2, length = .1, col = 2)
# a better look in the approximation around the MLE ----------------------
lambda.seq <- seq(.8, 1.2, length.out = 25)
plot(lambda.seq, lkl.poi(lambda.seq, data), type = "l",
     xlab = expression(lambda), ylab = "log-likelihood")
curve(quadprox.poi(x, lambda.est, data), col = 2, add = TRUE)
abline(v = lambda.est, lty = 3, col = 4)
# </r code> -----
@

         In the topright, in black, we have a interval based in the likelihood,
         \(\lambda \in (\Sexpr{paste(round(ic.95.prob, 3), collapse = ", ")})\),
         but with a cutoff criterion based in a \(\chi^{2}\) distribution.
         Thus, to have a nominal 95\% confidence interval we use a cutoff
         \(c = 15\%\). As we saw before, this interval is based in a
         large-sample theory. Since we're dealing here with a reasonable
         regular case, this interval shows as a good approximation.
         
         Still in the topright of Figure~\ref{fig:poisson}, in red we have a
         interval based in the quadratic approximation of the log-likelihood,
         \(\lambda \in (\Sexpr{paste(round(wald.95, 3), collapse = ", ")})\).
         From the quadratic approximation we get
         
         \[
          \log \frac{L(\lambda)}{L(\hat{\lambda})} \approx
          -\frac{1}{2} I_{O}(\hat{\lambda}) (\lambda - \hat{\lambda})^{2},
         \]
         
         that also follows a \(\chi^{2}\) distribution, since we have a r.v.
         \(\hat{\lambda}\) normalized (with its expected value subtracted and
         divided by its variance) and squared. From this we get the following
         exact (in the normal case) 95\% confidence interval
         
         \[
          \hat{\lambda} \pm 1.96~I_{O}(\hat{\lambda})^{-1/2}
          \qquad (\hat{\lambda} \pm 1.96~\text{se}(\hat{\lambda})).
         \]
         
         In the nonnormal cases this is an approximate 95\% CI.
         
         \textit{The actual variance is} \(I_{E}(\hat{\lambda})^{-1/2}\),
         \textit{but for the Poisson case the Fisher (expected) information is
                 equal to the observed one}, \(I_{O}(\hat{\lambda})^{-1/2}\).
                 
         A very nice thing that we can see from this intervals is that a Wald
         interval (a \(\pm\) interval) corresponds to a cut in the quadratic
         approximation exactly in the same point that the probability-based
         interval cuts the (log-)likelihood. Thus, as more regular the
         likelihood, better will be the fit of the approximation and more
         reliable will be the Wald interval.
                 
   \item Repita a quest\~{a}o anterior para a reparametriza\c{c}\~{a}o
         \(\theta = \log (\lambda)\).
         
         Solution:
         
         By the invariance property of the MLE we have
         
         \[
          \hat{\lambda} = \bar{x} \quad \Rightarrow \quad
          g(\hat{\lambda}) = \log \hat{\lambda} = \hat{\theta}
                           = \log \bar{x} = g(\bar{x}).
         \]
         
         i.e., the MLE of \(\hat{\theta}\) is \(\log \bar{x}\).
         
         By the Delta Method we compute the variance of \(\hat{\theta}\),
         
         \[
          V[\theta] = V[g(\lambda)]
                    = \left[\frac{\partial}{\partial \lambda} g(\lambda)
                      \right]^{2} V[\lambda]
                    = \left[\frac{1}{\lambda}\right]^{2} \frac{\lambda}{n}
                    = \frac{1}{\lambda~n}
                    \quad \rightarrow \quad
          V[\hat{\theta}] = \frac{1}{\bar{x}~n}.
         \]
         
         From this we can take the observed information for the
         reparametrization
         
         \[ V[\hat{\theta}] = I_{O}^{-1}(\hat{\theta}) = (\bar{x}~n)^{-1}. \]
         
         \textit{Now we do, in the same manner, everything that we did in the
                 previous letter.}
         
<<poisson-reparametrization, fig.height=5.5, fig.cap="log-likelihood function and MLE of \\(\\theta\\) in Poisson(\\(\\lambda = e^{\\theta}\\)) based on \\texttt{data}. In the topleft, a quadratic approximation in red. In the topright, two intervals for \\(\\theta\\) - one based in the likelihood (in black) and one based in the quadratic approximation (in red). In the bottom we provide a better look in the quadratic approximation.">>=
# <r code> -----
# likelihood -------------------------------------------------------------
## we can still use the lkl.poi function, but now we do
## lambda = exp(theta),
## with theta being a real rate (positive or negative)
theta.seq <- seq(-2, 2, length.out = 100)
lklseq.poi <- lkl.poi(exp(theta.seq), data)
par(mar = c(4, 4, 2, 2) + .1) # cosmetics
layout(matrix(c(1, 3, 2, 3), 2, 2), heights = c(1.5, 1)) # more cosmetics
plot(theta.seq, lklseq.poi, type = "l",
     xlab = expression(theta), ylab = "log-likelihood")
theta.est <- log(mean(data)) # MLE
abline(v = theta.est, lty = 3, col = 4)
# quadratic approximation for the reparametrization ----------------------
quadprox.poi.repa <- function(theta, theta.est, data) {
  obs.info <- sum(data) # observed information
  lkl.poi(exp(theta.est), data) - .5 * obs.info * (theta - theta.est)**2
}
curve(quadprox.poi.repa(x, theta.est, data), col = 2, add = TRUE)

legend(-2.15, -27.5, c("log-like", "Quadratic\napprox.", "MLE"),
       col = c(1, 2, 4), lty = c(1, 1, 3), bty = "n")
# intervals for lambda ---------------------------------------------------
plot(theta.seq, lklseq.poi, type = "l",
     xlab = expression(theta), ylab = "log-likelihood")
curve(quadprox.poi.repa(x, theta.est, data), col = 2, add = TRUE)
abline(v = theta.est, col = 4, lty = 3)
## probability-based interval --------------------------------------------
# cutoff's corresponding to a 95\% confidence interval for the mean
cut <- log(.15) + lkl.poi(exp(theta.est), data) ; abline(h = cut, lty = 2)
ic.lkl.poi <- function(theta, cut, ...) {
  lambda <- exp(theta)
  lkl.poi(lambda, ...) - cut
}
ic.95.prob <- rootSolve::uniroot.all(ic.lkl.poi, range(theta.seq),
                                     data = data, cut = cut)
arrows(x0 = ic.95.prob, y0 = rep(cut, 2),
       x1 = ic.95.prob, y1 = rep(-43, 2), lty = 2, length = .1)
## wald confidence interval  ---------------------------------------------
cut <- log(.15) + quadprox.poi.repa(theta.est, theta.est, data)
abline(h = cut, lty = 2, col = 2)

se.theta.est <- sqrt(1 / sum(data))
wald.95 <- theta.est + qnorm(c(.025, .975)) * se.theta.est

arrows(x0 = wald.95, y0 = rep(cut, 2),
       x1 = wald.95, y1 = rep(-43, 2), lty = 2, length = .1, col = 2)
# a better look in the approximation around the MLE ----------------------
theta.seq <- seq(-.75, .75, length.out = 25)
plot(theta.seq, lkl.poi(exp(theta.seq), data), type = "l",
     xlab = expression(theta), ylab = "log-likelihood")
curve(quadprox.poi.repa(x, theta.est, data), col = 2, add = TRUE)
abline(v = theta.est, lty = 3, col = 4)
# </r code> -----
@        
         
         We obtained here two intervals for \(\theta\). One based in a cut in
         the likelihood,
         \(\theta \in (\Sexpr{paste(round(ic.95.prob, 3), collapse = ", ")})\) -
         in black on the topright of Figure~\ref{fig:poisson-reparametrization},
         and one based in a cut in the quadratic approximation of the
         likelihood,
         \(\theta \in (\Sexpr{paste(round(wald.95, 3), collapse = ", ")})\) - in
         red on the topright of Figure~\ref{fig:poisson-reparametrization}.
         
         With the parametrization \(\theta = \log \lambda\) the two intervals
         are closer than the ones obtained for \(\lambda\). i.e., for \(\theta\)
         (with the use of the \(\log\)) we get a more regular likelihood.
         
   \item Obtenha ainda (por pelo menos dois m\'{e}todos diferentes) intervalos
         de confian\c{c}a para o par\^{a}metro \(\lambda\) a partir da
         fun\c{c}\~{a}o de verossimilhan\c{c}a (aproximada ou n\~{a}o) de
         \(\theta\).
         
         Solution:
         
         Since the MLE has the invariance property, a very simple idea is: take
         the obtained interval for \(\theta\) and apply a transformation,
         \(\lambda = e^{\theta}\). This simple idea is true for the interval
         obtained via likelihood function, as we can see in
         Figure~\ref{fig:poisson-c}. The obtained interval is the same that the
         one in letter \texttt{a)}
         
         However, this idea doesn't work for the interval based in the
         quadratic approximation.
         
<<poisson-c, fig.height=3.5, fig.width=5, fig.cap="log-likelihood function and quadratic approximation of \\(\\theta\\), and MLE of \\(\\lambda\\) in Poisson(\\(\\lambda = e^{\\theta}\\)) based on \\texttt{data}. In dashed, 95\\% confidence intervals for \\(\\lambda\\).">>=
# <r code> -----
# likelihood -------------------------------------------------------------
theta.seq <- seq(-2, 2, length.out = 100)
lklseq.poi <- lkl.poi(exp(theta.seq), data)
par(mar = c(4, 4, 2, 2) + .1) # cosmetics
plot(theta.seq, lklseq.poi, type = "l",
     xlab = expression(theta), ylab = "log-likelihood")
abline(v = lambda.est, lty = 3, col = 4)
# quadratic approximation for the reparametrization ----------------------
curve(quadprox.poi.repa(x, theta.est, data), col = 2, add = TRUE)
legend(-2.15, -25, c("log-like", "Quadratic\napprox.", "MLE"),
       col = c(1, 2, 4), lty = c(1, 1, 3), bty = "n")
# intervals for lambda ---------------------------------------------------
## probability-based interval --------------------------------------------
arrows(x0 = exp(ic.95.prob), y0 = c(-9, -36.5),
       x1 = exp(ic.95.prob), y1 = rep(-43, 2), lty = 2, length = .1)
## wald confidence interval  ---------------------------------------------
arrows(x0 = exp(wald.95), y0 = c(-9, -24),
       x1 = exp(wald.95), y1 = rep(-43, 2), lty = 2, length = .1, col = 2)
# </r code> -----
@

         In the letter \texttt{a)}, via the quadratic approximation we got
         \(\lambda \in (0.307, 1.693)\). Here, applying the relation
         \(\lambda = e^{\theta}\) we get
         \(\lambda \in (\Sexpr{paste(round(exp(wald.95), 3), collapse = ", ")})
         \). i.e., this shows that the invariance property applies to the
         likelihood, not to the quadratic approximation of the likelihood.
  \end{enumerate}
 
 \item A fim de se obter uma estimativa do p\'{u}blico de um jogo sem utilizar
       dados de venda de ingressos ou registros das roletas do est\'{a}dio,
       foram distribu\'{i}das camisas especiais para 300 torcedores sob
       condi\c{c}\~{a}o que estes a utilizassem durante um jogo. Durante o jogo
       foram selecionados ao acaso 250 torcedores verificando-se 12 destes
       possuiam a camisa.
  \begin{enumerate}[a)]
   \item Obtenha a fun\c{c}\~{a}o de verossimilhan\c{c}a para o n\'{u}mero total
         de torcedores.
         
         Solution:
         
         We have here a random variable, let's say, \(X\), representing the
         number of observed successes, \(k\). Here an observed success is select
         a fan using a special shirt. So, we have
         
         \[ X \sim \text{Hypergeometric}(N, K = 300, n = 250), \]
         
         with probability \(p = K/N\).
         
         \(N\) is the population size, that we want estimate. \(K\) is the
         unknown number of fans using a special shirt, and \(n\) is the number
         of, randomly, selected fans.
         
         \begin{align*}
          \text{Pr}[X = k] &=
          \frac{\binom{K}{k} \binom{N-K}{n-k}}{\binom{N}{n}}\\
          \text{Pr}[X = 12] &=
          \frac{\binom{300}{12} \binom{N-300}{250-12}}{\binom{N}{250}}\\
          &= \frac{300!~250!}{288!~238!~12!}
             \frac{(N-300)!~(N-250)!}{(N-538)!~N!}\\
          &= \text{constant} \times \frac{(N-300)!~(N-250)!}{(N-538)!~N!}.
         \end{align*}
         
         Since we already have a \(k\), the likelihood is equal to
         \(\text{Pr}[X = k]\).
         
         \begin{align*}
          L(N) &= \text{Pr}[X = 12]\\
               &= \text{constant} \frac{(N-300)!~(N-250)!}{(N-538)!~N!}\\
          l(N) = \log L(N) &= \log~\text{constant} +
                              \log~\frac{(N-300)!~(N-250)!}{(N-538)!~N!}\\
                           &\approx
                           \log~\frac{(N-300)!~(N-250)!}{(N-538)!~N!}.
         \end{align*}
         
         A plot of the likelihood is provided in Figure~\ref{fig:hypergeo-lkl}.

<<hypergeo-lkl, fig.height=6.75, fig.cap="log-likelihood of \\(N\\) in Hypergeometric(N, K = 300, n = 250) based on \\(k = 12\\). In dashed, the MLE. Different ranges of \\(N\\) to have a better understanding of the likelihood behaviour.">>=
# <r code> -----
# likelihood -------------------------------------------------------------
# always starting from N = 539
lkl.hypergeo <- function(N) {
  N.init <- 539
  size <- N - N.init
  n1 <- n2 <- n3 <- n4 <- numeric(size)
  N <- N.init
  for (i in 1:size) {
    n1[i] = log(N - 300)
    n2[i] = log(N - 250)
    n3[i] = log(N - 538)
    n4[i] = log(N)
    N = N + 1
  }
  lkl <- sum(n1) + sum(n2) - sum(n3) - sum(n4)
  return(lkl)
}

compute.lkl <- function(grid, ...) {
  size.grid = length(grid)
  lkl.grid = numeric(size.grid)
  i = 1
  for (j in grid) {
    lkl.grid[i] = lkl.hypergeo(N = j)
    i = i + 1
  }
  return(lkl.grid)
}

# plotting
par(mar = c(4, 4, 2, 2) + .1) # cosmetics
layout(matrix(c(0, 1, 1, 0, # more cosmetics
                2, 2, 3, 3), nrow = 2, byrow = TRUE))

n.grid <- seq(3000, 17000, by = 20)
plot(n.grid, compute.lkl(n.grid), "l", xlab = "N", ylab = "log-like")
abline(v = 6250, lty = 2)

n.grid <- seq(539, 1e4, by = 20)
plot(n.grid, compute.lkl(n.grid), "l", xlab = "N", ylab = "log-like")
abline(v = 6250, lty = 2)

n.grid <- seq(5000, 8000, by = 10)
plot(n.grid, compute.lkl(n.grid), "l", xlab = "N", ylab = "log-like")
abline(v = 6250, lty = 2)
# </r code> -----
@

         The MLE here is given by
         \(\hat{N} = \left\lfloor Kn/k \right\rfloor = 6250\).
         
         Figure~\ref{fig:hypergeo-lkl} present some very interesting behaviours.
         \\
         In the top, going from \(N = 3000\) to \(N = 17000\) we can see clearly
         the likelihood asymmetry. In the bottomleft, starting from the first
         valid possible value, 539, we see basically nothing. i.e., we don't see
         any possible curvature in the likelihood. In the bottomright we focus
         around the MLE to see better the curvature. Around that point we see
         a very small variation in the likelihood, \(0.3\), for a range of
         \(3000\) in \(N\). This shows how smooth is the curvature around the
         MLE in a large region.

   \item Obtenha a estimativa pontual e a intervalar, esta \'{u}ltima por pelo
         menos dois m\'{e}todos diferentes.
         
         Solution:
         
<<>>=
# <r code> -----
# point estimate ---------------------------------------------------------
optimize(lkl.hypergeo, lower = 539, upper = 2e4, maximum = TRUE)
# </r code> -----
@
         
         Using the \texttt{optimize} routine to do the optimization we obtain
         \(\hat{N} = 6251\), and a log-likelihood of 317.85. This value for
         \(\hat{N}\) is practically equal to what the ML theory says,
         \(\hat{N} = \left\lfloor Kn/k \right\rfloor = 6250\).
         
         Next, we compute a quadratic approximation for the hypergeometric
         (log-)likelihood followed by two intervals, one based in the
         likelihood, and a wald interval based in the quadratic approximation.
         All this is presented in Figure~\ref{fig:hypergeo-intervals}.
         
<<hypergeo-intervals, fig.height=5.5, fig.cap="log-likelihood function and MLE of \\(N\\) in Hypergeometric(N, K = 300, n = 250) based on \\(k = 12\\). In the topleft, a quadratic approximation in red. In the topright, two intervals for \\(N\\) - one based in the likelihood (in black) and one based in the quadratic approximation (in red). In the bottom we provide a better look in the quadratic approximation.">>=
# <r code> -----
# quadratic approximation of the likelihood ------------------------------
hess <- numDeriv:::hessian(lkl.hypergeo, x = 6250)

# quadratic approx.
quadprox.hypergeo <- function(N, N.est = 6250) {
  obs.info <- as.vector(-hess) # observed information
  lkl.hypergeo(N.est) - .5 * obs.info * (N - N.est)**2
}

par(mar = c(4, 4, 2, 2) + .1) # cosmetics
layout(matrix(c(1, 2,
                3, 3), nrow = 2, byrow = TRUE),
       heights = c(1.5, 1)) # more cosmetics

# likelihood plot
n.grid <- seq(3000, 17000, by = 20)
plot(n.grid, compute.lkl(n.grid), "l",
     xlab = "N", ylab = "log-likelihood")
abline(v = 6250, lty = 3, col = 4)

# quadratic approximation plot
curve(quadprox.hypergeo(x), col = 2, add = TRUE)

legend(10500, 318.25,
       c("log-like", "Quadratic\napprox.", "MLE"),
       col = c(1, 2, 4), lty = c(1, 1, 3), bty = "n")

# intervals for N --------------------------------------------------------
# first, we plot again the likelihood and the quadratic approx.
n.grid <- seq(2500, 12000, by = 20)

plot(n.grid, compute.lkl(n.grid), "l",
     xlab = "N", ylab = "log-likelihood")

abline(v = 6250, lty = 3, col = 4)

curve(quadprox.hypergeo(N = x), col = 2, add = TRUE)

## probability-based interval --------------------------------------------
# cutoff's corresponding to a 95\% confidence interval for N
cut <- log(.15) + lkl.hypergeo(6250)

abline(h = cut, lty = 2)

ic.lkl.hypergeo <- function(grid) {
  compute.lkl(grid) - cut
}
ic.95.prob <- rootSolve::uniroot.all(ic.lkl.hypergeo, range(n.grid))

arrows(x0 = ic.95.prob, y0 = rep(cut, 2),
       x1 = ic.95.prob, y1 = rep(309.5, 2), lty = 2, length = .1)

## wald confidence interval ---------------------------------------------
cut <- log(.15) + quadprox.hypergeo(N = 6250)

abline(h = cut, lty = 2, col = 2)

se.n.est <- sqrt(as.vector(-1 / hess))
wald.95 <- 6250 + qnorm(c(.025, .975)) * se.n.est

arrows(x0 = wald.95, y0 = rep(cut, 2),
       x1 = wald.95, y1 = rep(309.5, 2), lty = 2, length = .1, col = 2)

# a better look in the approximation around the MLE ----------------------
n.grid <- seq(5000, 8000, by = 10)

plot(n.grid, compute.lkl(n.grid), "l",
     xlab = "N", ylab = "log-likelihood")

abline(v = 6250, lty = 3, col = 4)
curve(quadprox.hypergeo(N = x), col = 2, add = TRUE)
# </r code> -----
@
         The quadratic approximation and the intervals were obtained in the same
         way that in the exercises before, with the only difference that here
         the hessian was obtained numerically, just for simplicity.
         
         Based in a cut in the likelihood, we obtain
         \((\Sexpr{paste(round(ic.95.prob, 0))})\) as a 95\% interval for the
         total number of fans in the game.
         With the wald interval (quadratic approximation) we obtain
         \((\Sexpr{paste(round(wald.95, 0))})\) as a 95\% interval.
         \textit{A considerable difference}.
         
   \item Repita e compare os resultados caso fossem 500 camisas e 20 com camisas
         dentre os 250.
         
         Solution:
         
         The procedure here is exactly the same.
         
         We have
         
          \begin{align*}
          \text{Pr}[X = k] =
          \frac{\binom{K}{k} \binom{N-K}{n-k}}{\binom{N}{n}}, \qquad
          \text{Pr}[X = 20] &=
          \frac{\binom{500}{20} \binom{N-500}{250-20}}{\binom{N}{250}}\\
          &= \frac{500!~250!}{480!~230!~20!}
             \frac{(N-500)!~(N-250)!}{(N-730)!~N!}\\
          &= \text{constant} \times \frac{(N-500)!~(N-250)!}{(N-730)!~N!}.
         \end{align*}
         
         With likelihood
         
         \begin{align*}
          L(N) &= \text{Pr}[X = 20]\\
               &= \text{constant} \frac{(N-500)!~(N-250)!}{(N-730)!~N!}\\
          l(N) = \log L(N) &= \log~\text{constant} +
                              \log~\frac{(N-500)!~(N-250)!}{(N-730)!~N!}\\
                           &\approx
                           \log~\frac{(N-500)!~(N-250)!}{(N-730)!~N!}.
         \end{align*}
         
         In Figure~\ref{fig:hypergeo-c} we provide the likelihood graphs and
         intervals for \(N\).

<<hypergeo-c, fig.height=5.5, fig.cap="log-likelihood function and MLE of \\(N\\) in Hypergeometric(N, K = 500, n = 250) based on \\(k = 20\\). In the topleft, a quadratic approximation in red. In the topright, two intervals for \\(N\\) - one based in the likelihood (in black) and one based in the quadratic approximation (in red). In the bottom we provide a better look in the quadratic approximation.">>=
# <r code> -----
# likelihood -------------------------------------------------------------
# always starting from N = 731
lkl.hypergeo <- function(N) {
  N.init <- 731
  size <- N - N.init
  n1 <- n2 <- n3 <- n4 <- numeric(size)
  N <- N.init
  for (i in 1:size) {
    n1[i] = log(N - 500)
    n2[i] = log(N - 250)
    n3[i] = log(N - 730)
    n4[i] = log(N)
    N = N + 1
  }
  lkl <- sum(n1) + sum(n2) - sum(n3) - sum(n4)
  return(lkl)
}
# plotting ---------------------------------------------------------------
par(mar = c(4, 4, 2, 2) + .1) # cosmetics

layout(matrix(c(1, 2,
                3, 3), nrow = 2, byrow = TRUE),
       heights = c(1.5, 1)) # more cosmetics
## likelihood ------------------------------------------------------------
n.grid <- seq(3000, 17000, by = 20)

plot(n.grid, compute.lkl(n.grid), "l",
     xlab = "N", ylab = "log-likelihood")
abline(v = 6250, lty = 3, col = 4)
## quadratic approximation -----------------------------------------------
hess <- numDeriv:::hessian(lkl.hypergeo, x = 6250)

curve(quadprox.hypergeo(x), col = 2, add = TRUE)
legend(10500, 382.5, c("log-like", "Quadratic\napprox.", "MLE"),
       col = c(1, 2, 4), lty = c(1, 1, 3), bty = "n")
# intervals for N --------------------------------------------------------
# first, we plot again the likelihood and the quadratic approx.
n.grid <- seq(2750, 11000, by = 20)

plot(n.grid, compute.lkl(n.grid), "l",
     xlab = "N", ylab = "log-likelihood")
abline(v = 6250, lty = 3, col = 4)

curve(quadprox.hypergeo(N = x), col = 2, add = TRUE)
## probability-based interval --------------------------------------------
# cutoff's corresponding to a 95\% confidence interval for N
cut <- log(.15) + lkl.hypergeo(6250)
abline(h = cut, lty = 2)

ic.95.prob <- rootSolve::uniroot.all(ic.lkl.hypergeo, range(n.grid))

arrows(x0 = ic.95.prob, y0 = rep(cut, 2),
       x1 = ic.95.prob, y1 = rep(370.25, 2), lty = 2, length = .1)
## wald confidence interval ---------------------------------------------
cut <- log(.15) + quadprox.hypergeo(N = 6250)
abline(h = cut, lty = 2, col = 2)

se.n.est <- sqrt(as.vector(-1 / hess))
wald.95 <- 6250 + qnorm(c(.025, .975)) * se.n.est

arrows(x0 = wald.95, y0 = rep(cut, 2),
       x1 = wald.95, y1 = rep(370.25, 2), lty = 2, length = .1, col = 2)
# a better look in the approximation around the MLE ----------------------
n.grid <- seq(5000, 7500, by = 10)

plot(n.grid, compute.lkl(n.grid), "l",
     xlab = "N", ylab = "log-likelihood")
abline(v = 6250, lty = 3, col = 4)
curve(quadprox.hypergeo(N = x), col = 2, add = TRUE)
# </r code> -----
@
         The MLE is exactly the same that the one in the previous scenario,
         since
         
         \[ \hat{N} = \left\lfloor Kn/k \right\rfloor
                    = \left\lfloor 300 \times 250~/~12 \right\rfloor
                    = \left\lfloor 500 \times 250~/~20 \right\rfloor = 6250.
         \]
         
         \begin{itemize}
          \item Based in a cut in the likelihood, we obtain
                \((\Sexpr{paste(round(ic.95.prob, 0))})\) as a 95\% interval for
                the total number of fans in the game.
          \item With the wald interval (quadratic approximation) we obtain
                \((\Sexpr{paste(round(wald.95, 0))})\) as a 95\% interval for
                the total number of fans.
         \end{itemize}
         \textit{A considerable difference}.
         
         Comparing with the previous scenario the intervals changed
         considerably, even with MLE retain the same.
         
         The likelihood shape is the same that before, just with a
         vertical increase. Before the maximum log-likelihood estimate was
         around 318, now is around 382.
  \end{enumerate}
 
 \item Foram tomadas as seguintes observa\c{c}\~{o}es independentes de uma v.a.
       \(X \sim N(\mu, \sigma^{2})\).
  \begin{itemize}
   \item
<<echo=FALSE>>=
# <r code> -----
(data <- c(56.4, 54.2, 65.3, 49.2, 50.1, 56.9, 58.9, 62.5, 70, 61))
# </r code> -----
@
   \item sabe-se que outras 5 observa\c{c}\~{o}es s\~{a}o menores que 50.
   \item sabe-se que outras 3 observa\c{c}\~{o}es s\~{a}o maiores que 65.
  \end{itemize}
  
  \begin{enumerate}[(a)]
   \item Escrever a fun\c{c}\~{a}o de verossimilhan\c{c}a.
   
         Solution:
         
         The likelihood can be expressed as
         
         \begin{align*}
          L(\theta) &= \left( \prod_{i = 1}^{10} f(x_{i}) \right) \times
                       (F(50))^{5} \times (1 - F(65))^{3}\\
                    &= \left( \prod_{i = 1}^{10}
                              \phi \left( \frac{y_{i} - \mu}{\sigma} \right)
                                   \right) \times
                       \left( \Phi \left( \frac{50 - \mu}{\sigma} \right)
                       \right)^{5} \times
                       \left( 1 - \Phi \left( \frac{65 - \mu}{\sigma} \right)
                       \right)^{3}\\
          l(\theta) &=
          \sum_{i = 1}^{10} \log \phi \left( \frac{y_{i} - \mu}{\sigma} \right)
          + 5~\log \Phi \left( \frac{50 - \mu}{\sigma} \right)
          + 3~\log \left( 1 - \Phi \left( \frac{65 - \mu}{\sigma} \right)
                   \right).
         \end{align*}
<<>>=
# <r code> -----
# likelihood -------------------------------------------------------------
## if logsigma = TRUE, we do the parametrization using log(sigma)
lkl.norm <- function(par, xs, less.than, more.than, logsigma = FALSE) {
  if (logsigma) par[2] <- exp(par[2])
  l1 <- sum(dnorm(xs, mean = par[1], sd = par[2], log = TRUE))
  l2 <- 5 * log(pnorm(less.than, mean = par[1], sd = par[2]))
  l3 <- 3 * log(1 - pnorm(more.than, mean = par[1], sd = par[2]))
  lkl <- l1 + l2 + l3
  # we return the negative of the log-likelihood,
  # since the optim routine performs a minimization
  return(-lkl)
}
# </r code> -----
@
   \item Obter as estimativas de m\'{a}xima verossimilhan\c{c}a.
   
         Solution:
<<>>=
# <r code> -----
# providing an initial guess based uniquely in the ten points sample
init.guess <- c(mean(data), sd(data))
# parameters estimation
(par.est <- optim(init.guess, lkl.norm, logsigma = FALSE,
                  xs = data, less.than = 65, more.than = 50)$par)
# </r code> -----
@
         \[ \hat{\mu} = \Sexpr{round(par.est[1], 3)}, \quad
            \hat{\sigma} = \Sexpr{round(par.est[2], 3)}.
         \]
         
         FYI: In the ten points sample we have
              \(\bar{x} = \Sexpr{round(mean(data), 3)}\) and
              \(s^{2} = \Sexpr{round(sd(data), 3)}\).
         
   \item Obter as verossimilhan\c{c}as perfilhadas.
         
         Solution:
         
         First, we do the graph of the likelihood surface in the deviance
         scale, since this scale is much more convenient. This requires the
         value of the likelihood evaluated in the parameter estimates.
         
         The deviance here is basically
         
         \[ D(\theta) = -2 (l(\theta) - l(\hat{\theta})), \]
         
         with \(\theta\) being the vector of parameters \(\mu\) and \(\sigma\).
         
         In Figure~\ref{fig:deviances-norm} we have the likelihood surfaces
         for two parametrizations: \(\sigma\) and \(\log \sigma\).
         
<<deviances-norm, fig.height=3.5, fig.cap="Deviances of (\\(\\mu, \\sigma\\)) and (\\(\\mu, \\log \\sigma\\)) for a Gaussian sample made of interval data.">>=
# <r code> -----
# deviance ---------------------------------------------------------------
dev.norm <- function(theta, est, ...) {
  # remember: here we have the negative of the log-likelihoods,
  # so the sign the different
  return(2 * (lkl.norm(theta, ...) - lkl.norm(est, ...)))
}
dev.norm.surf <- Vectorize(function(x, y, ...) dev.norm(c(x, y), ...))

par(mfrow = c(1, 2), mar = c(4, 4, 2, 2) + .1) # cosmetics
## parametrization: sigma ------------------------------------------------
mu.grid <- seq(52, 63.75, length.out = 100)
sigma.grid <- seq(3.25, 12.5, length.out = 100)

outer.grid <- outer(mu.grid, sigma.grid, FUN = dev.norm.surf,
                    est = par.est, xs = data,
                    less.than = 65, more.than = 50)

contour.levels <- c(.99, .95, .9, .7, .5, .3, .1, .05)
contour(mu.grid, sigma.grid, outer.grid,
        levels = qchisq(contour.levels, df = 2),
        labels = contour.levels,
        xlab = expression(mu), ylab = expression(sigma))
# converting par.est to a matricial object
points(t(par.est), col = 2, pch = 19, cex = .75)
## parametrization: log sigma --------------------------------------------
mu.grid <- seq(52, 63.75, length.out = 100)
sigma.grid <- seq(1.225, 2.5, length.out = 100)

par.est <- optim(init.guess, lkl.norm, logsigma = TRUE,
                 xs = data, less.than = 65, more.than = 50)$par

outer.grid <- outer(mu.grid, sigma.grid, FUN = dev.norm.surf,
                    est = par.est, xs = data,
                    logsigma = TRUE, less.than = 65, more.than = 50)

contour.levels <- c(.99, .95, .9, .7, .5, .3, .1, .05)
contour(mu.grid, sigma.grid, outer.grid,
        levels = qchisq(contour.levels, df = 2),
        labels = contour.levels,
        xlab = expression(mu), ylab = expression(log(sigma)))
points(t(par.est), col = 2, pch = 19, cex = .75)
# </r code> -----
@
         From Figure~\ref{fig:deviances-norm} two things are important to
         mention. First, with interval data, the surface doesn't exhibit
         orthogonally between the parameters. Second, in the log parametrization
         of sigma we have better behavior in the surface.
<<profiles-norm, fig.height=3.5, fig.cap="Deviances of (\\(\\mu, \\sigma\\)) and (\\(\\mu, \\log \\sigma\\)) for a Gaussian sample made of interval data.">>=
# <r code> -----
# profiles ---------------------------------------------------------------

# </r code> -----
@
   \item Obter os erro-padr\~{a}o das estimativas.
  \end{enumerate}
  
 \item Considere os dados da tabela a seguir (adaptados/modificados de
       Montgomery \& Runger, 1994) aos quais deseja-se ajustar um modelo de
       regress\~{a}o linear simples relacionando a vari\'{a}vel resposta \(Y\)
       (pureza em \%) a uma vari\'{a}vel explicativa \(X\) (n\'{i}vel de
       hidrocarbonetos).
       
       \begin{center}
        \begin{tabular}[H]{c|*{10}{c}}
         \hline
         X & 0.99 & 1.02 & 1.15 & 1.29 & 1.46 & 1.36 & 0.87 & 1.23 & 1.55 & 1.40\\
         \hline
         Y & 99.01 & 89.05 & 91.43 & 93.74 & 96.73 & 94.45 & 87.59 & 91.77 & 99.42 & 93.65\\
         \hline \hline
         X & 1.19 & 1.15 & 0.98 & 1.01 & 1.11 & 1.20 & 1.26 & 1.32 & 1.43 & 0.95\\
         \hline
         Y & 93.54 & 92.52 & 90.56 & 89.54 & 89.85 & 90.39 & 93.25 & 93.41 & 94.98 & 87.33\\
         \hline
        \end{tabular}
       \end{center}
  \begin{enumerate}[(a)]
   \item Encontre a fun\c{c}\~{a}o de verossimilhan\c{c}a.
   \item Encontre as estimativas de m\'{a}xima verossimilhan\c{c}a.
   \item Obtenha a verossimilhan\c{c}a conjunta para os par\^{a}metros
         \(\beta_{0}\) e \(\beta_{1}\):
    \begin{enumerate}[i.]
     \item considerando \(\sigma\) fixo com valor igual \`{a} sua estimativa.
     \item obtendo a verossimilhan\c{c}a (conjunta - 2D) perfilhada em
           rela\c{c}\~{a}o a \(\sigma\).
    \end{enumerate}
   \item Obtenha a verossimilhan\c{c}a perfilhada para os par\^{a}metros
         \(\beta_{0}\) e \(\beta_{1}\) individualmente.
  \end{enumerate}
  
 \item Considere uma amostra de uma v.a. \(Y\) em que assume-se que
       \(Y_{i} \sim P(\lambda_{i})\) em que o parâmetro \(\lambda_{i}\) \'{e}
       descrito por uma fun\c{c}\~{a}o de uma vari\'{a}vel explicativa
       \(\log (\lambda_{i}) = \beta_{0} + \beta_{1} x_{i}\) com valores
       conhecidos de \(x_{i}\).\\
       Dados simulados com (\(\beta_{0} = 2, \beta_{1} = 0.5\)) s\~{a}o
       mostrados a seguir.
       
       \begin{center}
        \begin{tabular}[H]{c|*{10}{c}}
         Y & 10 & 15 & 11 & 37 & 70 & 19 & 12 & 12 & 13 & 88\\
         \hline
         X & 1.7 & 1.5 & 0.5 & 2.8 & 4.4 & 1.8 & 0.4 & 0.7 & 1.3 & 5.1\\
        \end{tabular}
       \end{center}
  \begin{enumerate}[(a)]
   \item Obtenha o gr\'{a}fico da fun\c{c}\~{a}o de verossimilhan\c{c}a
         indicando a posi\c{c}\~{a}o dos valores verdadeiros dos
         par\^{a}metros.
  text \item Obtenha os perfis de verossimilhan\c{c}a dos par\^{a}metros.
   \item Obtenha estimativas.
   \item Obtenha intervalos (use diferentes m\'{e}todos).
   \item Compare os resultados anteriores com os fornecidos pela função
         \texttt{glm()} e discuta os achados.
  \end{enumerate}
\end{enumerate}

\vfill
\noindent
\textit{last modification on ...}

<<cache=FALSE, echo=FALSE>>=
# <r code> -----
Sys.time()
# </r code> -----
@ 

\end{document}