\documentclass[12pt]{article}
\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry}
\usepackage{listings}
\usepackage{color} % red, green, blue, yellow, cyan, magenta, black, white
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[hidelinks]{hyperref}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{cite}
\usepackage{bbding} % checkmarks
\usepackage{fancyhdr} % change the position of page number
\usepackage[flushleft]{threeparttable} % footnote in tables
\usepackage{lipsum}
\usepackage{caption}
\captionsetup{format = hang}
\usepackage{color, colortbl} % coloring rows or columns in tables
\definecolor{Gray}{gray}{.9} % coloring rows or columns in tables
\definecolor{darkgreen}{RGB}{0,100,0}
\definecolor{darkred}{RGB}{100,0,0}
\usepackage{listings}
\lstset{ 
 language = R, % the language of the code
 numbers = left, % where to put the line-numbers
 numbersep = 6pt, % how far the line-numbers are from the code
 backgroundcolor = \color{white}, % choose the background color. You must add
                                  % \usepackage{color}
 showspaces = false, % show spaces adding particular underscores
 showstringspaces = false, % underline spaces within strings
 showtabs = false, % show tabs within strings adding particular underscores
 frame = single, % adds a frame around the code
 rulecolor = \color{black}, % if not set, the frame-color may be changed on
                            % line-breaks within not-black text
 tabsize = 2, % sets default tabsize to 2 spaces
 captionpos = b, % sets the caption-position to bottom
 breaklines = true, % sets automatic line breaking
 breakatwhitespace = false, % sets if automatic breaks should only happen at
                            % whitespace
 keywordstyle = \color{black}, % keyword style
 commentstyle = \color{darkgreen}, % comment style
 stringstyle = \color{darkred}, % string literal style
 basicstyle = \footnotesize\ttfamily,
}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\fancyhf{} % clear all header and footers
\renewcommand{\headrulewidth}{0pt} % remove the header rule
% changing the description label from bold to italic
\renewcommand{\descriptionlabel}[1]{\hspace{\labelsep}\textit{#1}}
\rfoot{\thepage} % puts the page number on the right side
\pagestyle{fancy}
\title{Exerc\'{i}cios sobre Verossimilhan\c{c}a - VI}
\date{Junho de 2019}
\author{Henrique Ap. Laureano\\
        \texttt{henriquelaureano@outlook.com} \(\wedge\)
        \texttt{http://mynameislaure.github.io/} \vspace{.25cm}\\
        \texttt{/UFPR/DEST/LEG/}\\
        \includegraphics[width=.225\textwidth]{logo_leg.jpg}}

\begin{document}

\maketitle
\thispagestyle{empty}

%\vfill
%\noindent \horrule{.5pt} \vspace{-.95cm} \tableofcontents \noindent \horrule{.5pt}
\noindent \horrule{.5pt}

<<setup, include=FALSE>>=
# <r code> -----
library(knitr)

tema <- knit_theme$get("clarity") # acid

knit_theme$set(tema)

opts_chunk$set(size='small'
               , cache=TRUE
               , cache.path='cache/'
               , comment=NA
               , warning=FALSE
               , message=FALSE
               , fig.align='center'
               , dpi=100
               , fig.path='iBagens/'
               , fig.pos='H'
               , results='hold'
               , fig.show='hold')
# </r code> -----
@

\begin{enumerate}
 \item Seja \(X\) uma v.a. com distribui\c{c}\~{a}o uniforme
       \(X \sim U(0, \theta)\).\\
       Uma amostra aleat\'{o}ria forneceu os seguintes valores:
       5,5; 3,2; 4;8, 5,3; 3,8 e 5,0. Obtenha a fun\c{c}\~{a}o de
       verossimilhan\c{c}a e um (ou mais) intervalo(s) adequado(s) especificando
       a forma de obten\c{c}\~{a}o.
<<>>=
# <r code> -----
data <- c(5.5, 3.2, 4.8, 5.3, 3.8, 5)
# </r code> -----
@

      Solution:
      
      The likelihood of a \({\rm Uniform}(0, \theta)\) is
      
      \begin{align*}
       L(\theta) &= \theta^{-n}, \quad \text{for } x_{i} < \theta
                                       \text{ for all } i\\
                 &= \theta^{-n}, \quad \text{for } \theta > x_{(n)},
      \end{align*}

      and equal to zero otherwise.\\
      Given the data above, we get \(x_{(n)} = \Sexpr{max(data)}\) and the
      likelihood is shown in Figure~\ref{fig:uniform}, together with some
      intervals.

<<uniform, fig.height=3.5, fig.cap=paste0("Likelihood function of \\(\\theta\\) in Uniform(0, \\(\\theta\\)) based on \\(x_{(6)}\\) = ", max(data), ". In the left, likelihood intervals at 15\\% (dashed line) and 4\\% (dotted line) cutoff. In the right, likelihood intervals at 5\\% (dashed line) and 1\\% (dotted line) cutoff. These cutoffs correspond, in both sides, to a 95\\% and 99\\% confidence interval, respectively.")>>=
# <r code> -----
lkl.unif <- function(theta, data) {
  n <- length(data) ; xn <- max(data)
  # this next step, in the way presented, isn't very efficient, since I'll
  # compute the likelihood for all theta's and then, if necessary, convert to
  # zero. however, since we have here a very simple likelihood (and this
  # notation is extremely clear, btw), I keep in this way
  lkl = theta**(-n)*{theta >= xn}
  return(lkl)
}
theta.seq <- seq(4, 12, length.out = 100)
lklseq.unif <- lkl.unif(theta.seq, data)

par(mfrow = c(1, 2), mar = c(4, 4, 2, 2) + .1) # cosmetics
# plotting the normalized likelihood function to have unit maximum 
plot(theta.seq, lklseq.unif/max(lklseq.unif), type = "l",
     xlab = expression(theta), ylab = "Likelihood")

# probability-based interval ------------------------------------------------
# cutoff's corresponding to a 95\% and 99\% confidence interval for the mean 
cuts <- c(.15, .04) * lkl.unif(max(data), data) / max(lklseq.unif)
abline(h = cuts, lty = 2:3)

# uniroot.all finds the zeros, so we need to subtract the cutoff point from
# the likelihood to be able to find the points where the likelihood is cut
ic.lkl.unif <- function(theta, cut, ...) {
  lkl.unif(theta, ...)/max(lklseq.unif) - cut
}
ic.95.prob <- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq),
                                     data = data, cut = cuts[1])
ic.99.prob <- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq),
                                     data = data, cut = cuts[2])
abline(v = ic.95.prob, lty = 2) ; abline(v = ic.99.prob, lty = 3)

plot(theta.seq, lklseq.unif/max(lklseq.unif), type = "l",
     xlab = expression(theta), ylab = "Likelihood")
# pure likelihood interval --------------------------------------------------
# cutoff's corresponding to a 95\% and 99\% confidence interval for the mean 
cuts <- c(.05, .01) * lkl.unif(max(data), data) / max(lklseq.unif)
abline(h = cuts, lty = 2:3)

ic.95.pure <- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq),
                                     data = data, cut = cuts[1])
ic.99.pure <- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq),
                                     data = data, cut = cuts[2])
abline(v = ic.95.pure, lty = 2) ; abline(v = ic.99.pure, lty = 3)
# </r code> -----
@

      In both graphs of Figure~\ref{fig:uniform} we have two confidence
      intervals for \(\theta\), one corresponding to a 95\% confidence interval
      (dashed line) and one corresponding to a 99\% confidence interval for the
      mean (dotted line). In the left we have a probability-based likelihood
      interval, in the right we have a pure likelihood interval.
      
      The left ones are bad intervals, since they're based in a large-sample
      theory that results in an exact interval for the Gaussian case, in a good
      approximation for a reasonably regular case, and as we can see by the
      figure... here we doesn't have a regular likelihood (the likelihood isn't
      well approximated by a quadratic function), thus, here this interval
      should not be so good.
      
      \begin{description}
       \item[Explaining better how we arrived in these intervals:]
       \hfill\\
       For a normalized likelihood, \(L(\theta)/L(\hat{\theta})\), we have the
       following Wilk's likelihood ratio statistic, defined as \(W\)

       \[
        W \equiv 2 \log \frac{L(\hat{\theta})}{L(\theta)} \sim \chi_{1}^{2}.
       \]
       Its \(\chi^{2}\) distribution is exact only in the normal mean model,
       and approximately true when the likelihood is reasonably regular.
       
       Based in this Wilk's statistic we are able to find the probability that
       the likelihood interval covers \(\theta\),

       \[
        {\rm Pr}\left\{\frac{L(\theta)}{L(\hat{\theta})} > c\right\} =
        {\rm Pr}\left\{2 \log \frac{L(\theta)}{L(\hat{\theta})} < -2 \log c
               \right\} = {\rm Pr}\{\chi_{1}^{2} < -2 \log c\}.
       \]

       So, if for some \(0 < \alpha < 1\) we choose a cutoff
      
       \[ c = e^{-\frac{1}{2} \chi_{1, (1-\alpha)}^{2}}, \]
      
       where \(\chi_{1, (1-\alpha)}^{2}\) is the \(100 (1-\alpha)\) percentile
       of \(\chi_{1}^{2}\).\\
       For \(\alpha = 0.05\) and \(0.01\) we have a cutoff \(c = 0.15\) and
       \(0.04\), that corresponds to a 95\% and 99\% confidence interval,
       respectively. \hfill \(\square\)
      \end{description}

      Having the cutoff values we need now to find the interval values,
      i.e., in which points the cutoff horizontal line cuts the likelihood.
      For this purpose we use the function \texttt{rootSolve::uniroot.all}.\\
      A likelihood interval at 15\% and 4\% cutoff for \(\theta\) are
      (\Sexpr{paste(round(ic.95.prob, 3), collapse = ", ")}) and
      (\Sexpr{paste(round(ic.99.prob, 3), collapse = ", ")}).
    
      Already on the right side of Figure~\ref{fig:uniform} we have a more
      coherent, let's say, confidence interval for the Uniform likelihood.
      While the likelihood isn't regular, it is still possible to provide an
      exact theoretical justification for a confidence interval interpretation.
      Now
      
      \[
       {\rm Pr}\left\{\frac{L(\theta)}{L(\hat{\theta})} > c\right\} =
       {\rm Pr}\left\{\frac{X_{(n)}}{\theta} > c^{1/n}\right\} =
       1 - {\rm Pr}\left\{\frac{X_{(n)}}{\theta} < c^{1/n}\right\} =
       1 - (c^{1/n})^{n} = 1 - c.
      \]
      So the likelihood interval with cutoff \(c\) is a \(100 (1-c)\)\%
      confidence interval.\\
      A likelihood interval at 15\% and 4\% cutoff for \(\theta\) are
      (\Sexpr{paste(round(ic.95.pure, 3), collapse = ", ")}) and
      (\Sexpr{paste(round(ic.99.pure, 3), collapse = ", ")}). \\
      
      Comparing the obtained intervals we see a broader range with the pure
      likelihood intervals. In other words, with the pure likelihood intervals
      we see a higher uncertainty than with the (not so recommended here)
      probability-based likelihood intervals.
      
      \vfill
      \textit{For doing this exercise I read and used it Pawitan's book:}
<<eval=FALSE>>=
@book{pawitan,
  author    = {Yudi Pawitan},
  title     = {In All Likelihood:
               Statistical Modelling and Inference Using Likelihood},
  year      = {1991},
  publisher = {Oxford University Press},
  address   = {Great Clarendon Street, Oxford OX2 6DP},
}
@
\newpage

 \item Seja \(X\) uma v.a. de uma distribui\c{c}\~{a}o de Poisson
       (\(X \sim P(\lambda))\)) para a qual foi obtida a seguinte amostra
       aleat\'{o}ria: (3, 1, 0, 2, 1, 1, 0, 0).
<<>>=
# <r code> -----
data <- c(3, 1, 0, 2, 1, 1, 0, 0)
# </r code> -----
@
  \begin{enumerate}[(a)]
   \item Obtenha a fun\c{c}\~{a}o de verossimilhan\c{c}a, sua
         aproxima\c{c}\~{a}o quadr\'{a}tica e intervalos para \(\lambda\).
         
         Solution:
         
         The likelihood and the log-likelihood of a Poisson(\(\lambda\)) is
         
         \[
          L(\lambda) = \prod_{i=1}^{n}
                       \frac{e^{-\lambda} \lambda^{x_{i}}}{x_{i}!}
                       \quad \text{and} \quad
          \log L(\lambda) =
          l(\lambda) =
          -n \lambda + \log \lambda \sum_{i=1}^{n} x_{i}
                     - \sum_{i=1}^{n} \log x_{i}!.
         \]
         
         \textit{We work here with the log-likelihood to make our life easier,
                 since the computations are much simpler in the log scale.}
         
         By computing the Score (derivative of \(l(\lambda)\) wrt to
         \(\lambda\)) and making it equal to zero, we find the MLE
         \(\hat{\lambda} = \bar{x}\).
         Computing the second derivative we find the observed information
         
         \[
          I_{O}(\lambda) = \frac{n \bar{x}}{\lambda^{2}}
          \quad \rightarrow \quad
          I_{O}(\hat{\lambda}) = \frac{n}{\bar{x}}.
         \]
         
         A graph of the Poisson log-likelihood is provided in black in
         Figure~\ref{fig:poisson}.
         
         Doing a quadratic approximation (Taylor expansion of second order) in
         \(l(\lambda)\) around \(\hat{\lambda}\) we have
         
         \begin{align*}
          l(\lambda) &\approx l(\hat{\lambda})
                     + (\lambda - \hat{\lambda}) {l}'(\hat{\lambda})
                     + \frac{1}{2} (\lambda - \hat{\lambda})^{2}
                                   {l}''(\hat{\lambda})\\
                     &\quad(\text{the Score is zero at the MLE})\\
                     &= l(\hat{\lambda})
                     + \frac{1}{2} (\lambda - \hat{\lambda})^{2}
                                   {l}''(\hat{\lambda})
                     = l(\hat{\lambda})
                     + \frac{1}{2} (\lambda - \hat{\lambda})^{2}
                                   I_{O}(\hat{\lambda}).
         \end{align*}
         
         A graph of the quadratic approximation of the Poisson log-likelihood
         is provided in red in Figure~\ref{fig:poisson}. There we can see how
         good is the approximation around the maximum likelihood estimator
         (MLE). Here the sample size is very small, the idea is that as the
         sample size increase the quality, in this case the range, of the
         approximation also increase. This should happen because as the sample
         size increase the Poisson likelihood should be more symmetric.
         
         In the topright graph of Figure~\ref{fig:poisson} we have two
         intervals for \(\lambda\).
         
<<poisson, fig.height=5.5, fig.cap="log-likelihood function and MLE of \\(\\lambda\\) in Poisson(\\(\\lambda\\)) based on \\texttt{data}. In the topleft, a quadratic approximation in red. In the topright, two intervals for \\(\\lambda\\) - one based in the likelihood (in black) and one based in the quadratic approximation (in red). In the bottom we provide a better look in the quadratic approximation.">>=
# <r code> -----
# likelihood -------------------------------------------------------------
lkl.poi <- function(lambda, data) {
  n <- length(data)
  # log-likelihood ignoring irrelevant constant terms
  lkl = -n * lambda + sum(data) * log(lambda)
  return(lkl)
}
lambda.seq <- seq(0, 4.5, length.out = 100)
lklseq.poi <- lkl.poi(lambda.seq, data)
par(mar = c(4, 4, 2, 2) + .1) # cosmetics
layout(matrix(c(1, 3, 2, 3), 2, 2), heights = c(1.5, 1)) # more cosmetics
plot(lambda.seq, lklseq.poi, type = "l",
     xlab = expression(lambda), ylab = "log-likelihood")
lambda.est <- mean(data) # MLE
abline(v = lambda.est, lty = 3, col = 4)
# quadratic approximation ------------------------------------------------
quadprox.poi <- function(lambda, lambda.est, data) {
  n <- length(data)
  obs.info <- n / lambda.est # observed information
  lkl.poi(lambda.est, data) - .5 * obs.info * (lambda - lambda.est)**2
}
curve(quadprox.poi(x, lambda.est, data), col = 2, add = TRUE)
legend(2.4, -6.25, c("log-like", "Quadratic\napprox.", "MLE"),
       col = c(1, 2, 4), lty = c(1, 1, 3), bty = "n")
# intervals for lambda ---------------------------------------------------
plot(lambda.seq, lklseq.poi, type = "l",
     xlab = expression(lambda), ylab = "log-likelihood")
curve(quadprox.poi(x, lambda.est, data), col = 2, add = TRUE)
abline(v = lambda.est, col = 4, lty = 3)
## probability-based interval --------------------------------------------
# cutoff's corresponding to a 95\% confidence interval for the mean
cut <- log(.15) + lkl.poi(lambda.est, data) ; abline(h = cut, lty = 2)
ic.lkl.poi <- function(lambda, cut, ...) {
  lkl.poi(lambda, ...) - cut
}
ic.95.prob <- rootSolve::uniroot.all(ic.lkl.poi, range(lambda.seq),
                                     data = data, cut = cut)
arrows(x0 = ic.95.prob, y0 = rep(cut, 2),
       x1 = ic.95.prob, y1 = rep(-25, 2), lty = 2, length = .1)
## wald confidence interval  ---------------------------------------------
cut <- log(.15) + quadprox.poi(lambda.est, lambda.est, data)
abline(h = cut, lty = 2, col = 2)
se.lambda.est <- sqrt(lambda.est / length(data))
wald.95 <- lambda.est + qnorm(c(.025, .975)) * se.lambda.est
arrows(x0 = wald.95, y0 = rep(cut, 2),
       x1 = wald.95, y1 = rep(-25, 2), lty = 2, length = .1, col = 2)
# a better look in the approximation around the MLE ----------------------
lambda.seq <- seq(.8, 1.2, length.out = 25)
plot(lambda.seq, lkl.poi(lambda.seq, data), type = "l",
     xlab = expression(lambda), ylab = "log-likelihood")
curve(quadprox.poi(x, lambda.est, data), col = 2, add = TRUE)
abline(v = lambda.est, lty = 3, col = 4)
# </r code> -----
@

         In the topright, in black, we have a interval based in the likelihood,
         \(\lambda \in (\Sexpr{paste(round(ic.95.prob, 3), collapse = ", ")})\),
         but with a cutoff criterion based in a \(\chi^{2}\) distribution.
         Thus, to have a nominal 95\% confidence interval we use a cutoff
         \(c = 15\%\). As we saw before, this interval is based in a
         large-sample theory. Since we're dealing here with a reasonable
         regular case, this interval shows as a good approximation.
         
         Still in the topright of Figure~\ref{fig:poisson}, in red we have a
         interval based in the quadratic approximation of the log-likelihood,
         \(\lambda \in (\Sexpr{paste(round(wald.95, 3), collapse = ", ")})\).
         From the quadratic approximation we get
         
         \[
          \log \frac{L(\lambda)}{L(\hat{\lambda})} \approx
          -\frac{1}{2} I_{O}(\hat{\lambda}) (\lambda - \hat{\lambda})^{2},
         \]
         
         that also follows a \(\chi^{2}\) distribution, since we have a r.v.
         \(\hat{\lambda}\) normalized (with its expected value subtracted and
         divided by its variance) and squared. From this we get the following
         exact (in the normal case) 95\% confidence interval
         
         \[
          \hat{\lambda} \pm 1.96~I_{O}(\hat{\lambda})^{-1/2}
          \qquad (\hat{\lambda} \pm 1.96~\text{se}(\hat{\lambda})).
         \]
         
         In the nonnormal cases this is an approximate 95\% CI.
         
         \textit{The actual variance is} \(I_{E}(\hat{\lambda})^{-1/2}\),
         \textit{but for the Poisson case the Fisher (expected) information is
                 equal to the observed one}, \(I_{O}(\hat{\lambda})^{-1/2}\).
                 
         A very nice thing that we can see from this intervals is that a Wald
         interval (a \(\pm\) interval) corresponds to a cut in the quadratic
         approximation exactly in the same point that the probability-based
         interval cuts the (log-)likelihood. Thus, as more regular the
         likelihood, better will be the fit of the approximation and more
         reliable will be the Wald interval.
                 
   \item Repita a quest\~{a}o anterior para a reparametriza\c{c}\~{a}o
         \(\theta = \log (\lambda)\).
         
         Solution:
         
         By the invariance property of the MLE we have
         
         \[
          \hat{\lambda} = \bar{x} \quad \Rightarrow \quad
          g(\hat{\lambda}) = \log \hat{\lambda} = \hat{\theta}
                           = \log \bar{x} = g(\bar{x}).
         \]
         
         i.e., the MLE of \(\hat{\theta}\) is \(\log \bar{x}\).
         
         By the Delta Method we compute the variance of \(\hat{\theta}\),
         
         \[
          V[\theta] = V[g(\lambda)]
                    = \left[\frac{\partial}{\partial \lambda} g(\lambda)
                      \right]^{2} V[\lambda]
                    = \left[\frac{1}{\lambda}\right]^{2} \frac{\lambda}{n}
                    = \frac{1}{\lambda~n}
                    \quad \rightarrow \quad
          V[\hat{\theta}] = \frac{1}{\bar{x}~n}.
         \]
         
         From this we can take the observed information for the
         reparametrization
         
         \[ V[\hat{\theta}] = I_{O}^{-1}(\hat{\theta}) = (\bar{x}~n)^{-1}. \]
         
         \textit{Now we do, in the same manner, everything that we did in the
                 previous letter.}
         
<<poisson-reparametrization, fig.height=5.5, fig.cap="log-likelihood function and MLE of \\(\\theta\\) in Poisson(\\(\\lambda = e^{\\theta}\\)) based on \\texttt{data}. In the topleft, a quadratic approximation in red. In the topright, two intervals for \\(\\theta\\) - one based in the likelihood (in black) and one based in the quadratic approximation (in red). In the bottom we provide a better look in the quadratic approximation.">>=
# <r code> -----
# likelihood -------------------------------------------------------------
## we can still use the lkl.poi function, but now we do
## lambda = exp(theta),
## with theta being a real rate (positive or negative)
theta.seq <- seq(-2, 2, length.out = 100)
lklseq.poi <- lkl.poi(exp(theta.seq), data)
par(mar = c(4, 4, 2, 2) + .1) # cosmetics
layout(matrix(c(1, 3, 2, 3), 2, 2), heights = c(1.5, 1)) # more cosmetics
plot(theta.seq, lklseq.poi, type = "l",
     xlab = expression(theta), ylab = "log-likelihood")
theta.est <- log(mean(data)) # MLE
abline(v = theta.est, lty = 3, col = 4)
# quadratic approximation for the reparametrization ----------------------
quadprox.poi.repa <- function(theta, theta.est, data) {
  obs.info <- sum(data) # observed information
  lkl.poi(exp(theta.est), data) - .5 * obs.info * (theta - theta.est)**2
}
curve(quadprox.poi.repa(x, theta.est, data), col = 2, add = TRUE)

legend(-2.15, -27.5, c("log-like", "Quadratic\napprox.", "MLE"),
       col = c(1, 2, 4), lty = c(1, 1, 3), bty = "n")
# intervals for lambda ---------------------------------------------------
plot(theta.seq, lklseq.poi, type = "l",
     xlab = expression(theta), ylab = "log-likelihood")
curve(quadprox.poi.repa(x, theta.est, data), col = 2, add = TRUE)
abline(v = theta.est, col = 4, lty = 3)
## probability-based interval --------------------------------------------
# cutoff's corresponding to a 95\% confidence interval for the mean
cut <- log(.15) + lkl.poi(exp(theta.est), data) ; abline(h = cut, lty = 2)
ic.lkl.poi <- function(theta, cut, ...) {
  lambda <- exp(theta)
  lkl.poi(lambda, ...) - cut
}
ic.95.prob <- rootSolve::uniroot.all(ic.lkl.poi, range(theta.seq),
                                     data = data, cut = cut)
arrows(x0 = ic.95.prob, y0 = rep(cut, 2),
       x1 = ic.95.prob, y1 = rep(-43, 2), lty = 2, length = .1)
## wald confidence interval  ---------------------------------------------
cut <- log(.15) + quadprox.poi.repa(theta.est, theta.est, data)
abline(h = cut, lty = 2, col = 2)

se.theta.est <- sqrt(1 / sum(data))
wald.95 <- theta.est + qnorm(c(.025, .975)) * se.theta.est

arrows(x0 = wald.95, y0 = rep(cut, 2),
       x1 = wald.95, y1 = rep(-43, 2), lty = 2, length = .1, col = 2)
# a better look in the approximation around the MLE ----------------------
theta.seq <- seq(-.75, .75, length.out = 25)
plot(theta.seq, lkl.poi(exp(theta.seq), data), type = "l",
     xlab = expression(theta), ylab = "log-likelihood")
curve(quadprox.poi.repa(x, theta.est, data), col = 2, add = TRUE)
abline(v = theta.est, lty = 3, col = 4)
# </r code> -----
@        
         
         We obtained here two intervals for \(\theta\). One based in a cut in
         the likelihood,
         \(\theta \in (\Sexpr{paste(round(ic.95.prob, 3), collapse = ", ")})\) -
         in black on the topright of Figure~\ref{fig:poisson-reparametrization},
         and one based in a cut in the quadratic approximation of the
         likelihood,
         \(\theta \in (\Sexpr{paste(round(wald.95, 3), collapse = ", ")})\) - in
         red on the topright of Figure~\ref{fig:poisson-reparametrization}.
         
         With the parametrization \(\theta = \log \lambda\) the two intervals
         are closer than the ones obtained for \(\lambda\). i.e., for \(\theta\)
         (with the use of the \(\log\)) we get a more regular likelihood.
         
   \item Obtenha ainda (por pelo menos dois m\'{e}todos diferentes) intervalos
         de confian\c{c}a para o par\^{a}metro \(\lambda\) a partir da
         fun\c{c}\~{a}o de verossimilhan\c{c}a (aproximada ou n\~{a}o) de
         \(\theta\).
         
         Solution:
         
         Since the MLE has the invariance property, a very simple idea is: take
         the obtained interval for \(\theta\) and apply a transformation,
         \(\lambda = e^{\theta}\). This simple idea is true for the interval
         obtained via likelihood function, as we can see in
         Figure~\ref{fig:poisson-c}. The obtained interval is the same that the
         one in letter \texttt{a)}
         
         However, this idea doesn't work for the interval based in the
         quadratic approximation.
         
<<poisson-c, fig.height=3.5, fig.width=5, fig.cap="log-likelihood function and quadratic approximation of \\(\\theta\\), and MLE of \\(\\lambda\\) in Poisson(\\(\\lambda = e^{\\theta}\\)) based on \\texttt{data}. In dashed, 95\\% confidence intervals for \\(\\lambda\\).">>=
# <r code> -----
# likelihood -------------------------------------------------------------
theta.seq <- seq(-2, 2, length.out = 100)
lklseq.poi <- lkl.poi(exp(theta.seq), data)
par(mar = c(4, 4, 2, 2) + .1) # cosmetics
plot(theta.seq, lklseq.poi, type = "l",
     xlab = expression(theta), ylab = "log-likelihood")
abline(v = lambda.est, lty = 3, col = 4)
# quadratic approximation for the reparametrization ----------------------
curve(quadprox.poi.repa(x, theta.est, data), col = 2, add = TRUE)
legend(-2.15, -25, c("log-like", "Quadratic\napprox.", "MLE"),
       col = c(1, 2, 4), lty = c(1, 1, 3), bty = "n")
# intervals for lambda ---------------------------------------------------
## probability-based interval --------------------------------------------
arrows(x0 = exp(ic.95.prob), y0 = c(-9, -36.5),
       x1 = exp(ic.95.prob), y1 = rep(-43, 2), lty = 2, length = .1)
## wald confidence interval  ---------------------------------------------
arrows(x0 = exp(wald.95), y0 = c(-9, -24),
       x1 = exp(wald.95), y1 = rep(-43, 2), lty = 2, length = .1, col = 2)
# </r code> -----
@

         In the letter \texttt{a)}, via the quadratic approximation we got
         \(\lambda \in (0.307, 1.693)\). Here, applying the relation
         \(\lambda = e^{\theta}\) we get
         \(\lambda \in (\Sexpr{paste(round(exp(wald.95), 3), collapse = ", ")})
         \). i.e., this shows that the invariance property applies to the
         likelihood, not to the quadratic approximation of the likelihood.
  \end{enumerate}
 
 \item A fim de se obter uma estimativa do p\'{u}blico de um jogo sem utilizar
       dados de venda de ingressos ou registros das roletas do est\'{a}dio,
       foram distribu\'{i}das camisas especiais para 300 torcedores sob
       condi\c{c}\~{a}o que estes a utilizassem durante um jogo. Durante o jogo
       foram selecionados ao acaso 250 torcedores verificando-se 12 destes
       possuiam a camisa.
  \begin{enumerate}[a)]
   \item Obtenha a fun\c{c}\~{a}o de verossimilhan\c{c}a para o n\'{u}mero total
         de torcedores.
         
         Solution:
         
         We have here a random variable, let's say, \(X\), representing the
         number of observed successes, \(k\). Here an observed success is select
         a fan using a special shirt. So, we have
         
         \[ X \sim \text{Hypergeometric}(N, K = 300, n = 250), \]
         
         with probability \(p = K/N\).
         
         \(N\) is the population size, that we want estimate. \(K\) is the
         unknown number of fans using a special shirt, and \(n\) is the number
         of, randomly, selected fans.
         
         \begin{align*}
          \text{Pr}[X = k] &=
          \frac{\binom{K}{k} \binom{N-K}{n-k}}{\binom{N}{n}}\\
          \text{Pr}[X = 12] &=
          \frac{\binom{300}{12} \binom{N-300}{250-12}}{\binom{N}{250}}\\
          &= \frac{300!~250!}{288!~238!~12!}
             \frac{(N-300)!~(N-250)!}{(N-538)!~N!}\\
          &= \text{constant} \times \frac{(N-300)!~(N-250)!}{(N-538)!~N!}.
         \end{align*}
         
         Since we already have a \(k\), the likelihood is equal to
         \(\text{Pr}[X = k]\).
         
         \begin{align*}
          L(N) &= \text{Pr}[X = 12]\\
               &= \text{constant} \frac{(N-300)!~(N-250)!}{(N-538)!~N!}\\
          l(N) = \log L(N) &= \log~\text{constant} +
                              \log~\frac{(N-300)!~(N-250)!}{(N-538)!~N!}\\
                           &\approx
                           \log~\frac{(N-300)!~(N-250)!}{(N-538)!~N!}.
         \end{align*}
         
         A plot of the likelihood is provided in Figure~\ref{fig:hypergeo-lkl}.

<<hypergeo-lkl, fig.height=6.75, fig.cap="log-likelihood of \\(N\\) in Hypergeometric(N, K = 300, n = 250) based on \\(k = 12\\). In dashed, the MLE. Different ranges of \\(N\\) to have a better understanding of the likelihood behaviour.">>=
# <r code> -----
# likelihood -------------------------------------------------------------
# always starting from N = 539
lkl.hypergeo <- function(N = 539, M) {
  size <- M - N
  n1 <- n2 <- n3 <- n4 <- numeric(size)
  for (i in 1:size) {
    n1[i] = log(N - 300)
    n2[i] = log(N - 250)
    n3[i] = log(N - 538)
    n4[i] = log(N)
    N = N + 1
  }
  lkl <- sum(n1) + sum(n2) - sum(n3) - sum(n4)
  return(lkl)
}

compute.lkl <- function(grid, ...) {
  size.grid = length(grid)
  lkl.grid = numeric(size.grid)
  i = 1
  for (j in grid) {
    lkl.grid[i] = lkl.hypergeo(M = j)
    i = i + 1
  }
  return(lkl.grid)
}

# plotting
par(mar = c(4, 4, 2, 2) + .1) # cosmetics
layout(matrix(c(0, 1, 1, 0, # more cosmetics
                2, 2, 3, 3), nrow = 2, byrow = TRUE))

m.grid <- seq(3000, 17000, by = 20)
plot(m.grid, compute.lkl(m.grid), "l", xlab = "N", ylab = "log-like")
abline(v = 6250, lty = 2)

m.grid <- seq(539, 1e4, by = 20)
plot(m.grid, compute.lkl(m.grid), "l", xlab = "N", ylab = "log-like")
abline(v = 6250, lty = 2)

m.grid <- seq(5000, 8000, by = 10)
plot(m.grid, compute.lkl(m.grid), "l", xlab = "N", ylab = "log-like")
abline(v = 6250, lty = 2)
# </r code> -----
@

         The MLE here is given by
         \(\hat{N} = \left\lfloor Kn/k \right\rfloor = 6250\).
         
         Figure~\ref{fig:hypergeo-lkl} present some very interesting behaviours.
         \\
         In the top, going from \(N = 3000\) to \(N = 17000\) we can see clearly
         the likelihood asymmetry. In the bottomleft, starting from the first
         valid possible value, 539, we see basically nothing. i.e., we don't see
         any possible curvature in the likelihood. In the bottomright we focus
         around the MLE to see better the curvature. Around that point we see
         a very small variation in the likelihood, \(0.3\), for a range of
         \(3000\) in \(N\). This shows how smooth is the curvature around the
         MLE in a large region.

   \item Obtenha a estimativa pontual e a intervalar, esta \'{u}ltima por pelo
         menos dois m\'{e}todos diferentes.
   \item Repita e compare os resultados caso fossem 500 camisas e 20 com camisas
         dentre os 250.
  \end{enumerate}
 
 \item Foram tomadas as seguintes observa\c{c}\~{o}es independentes de uma v.a.
       \(X \sim N(\mu, \sigma^{2})\).
  \begin{itemize}
   \item
<<echo=FALSE>>=
# <r code> -----
(data <- c(56.4, 54.2, 65.3, 49.2, 50.1, 56.9, 58.9, 62.5, 70, 61))
# </r code> -----
@  
   \item sabe-se que outras 5 observa\c{c}\~{o}es s\~{a}o menores que 50.
   \item sabe-se que outras 3 observa\c{c}\~{o}es s\~{a}o maiores que 65.
  \end{itemize}
  
  \begin{enumerate}[(a)]
   \item Escrever a fun\c{c}\~{a}o de verossimilhan\c{c}a.
   \item Obter as estimativas de m\'{a}xima verossimilhan\c{c}a.
   \item Obter as verossimilhan\c{c}as perfilhadas.
   \item Obter os erro-padr\~{a}o das estimativas.
  \end{enumerate}
  
 \item Considere os dados da tabela a seguir (adaptados/modificados de
       Montgomery \& Runger, 1994) aos quais deseja-se ajustar um modelo de
       regress\~{a}o linear simples relacionando a vari\'{a}vel resposta \(Y\)
       (pureza em \%) a uma vari\'{a}vel explicativa \(X\) (n\'{i}vel de
       hidrocarbonetos).
       
       \begin{center}
        \begin{tabular}[H]{c|*{10}{c}}
         \hline
         X & 0.99 & 1.02 & 1.15 & 1.29 & 1.46 & 1.36 & 0.87 & 1.23 & 1.55 & 1.40\\
         \hline
         Y & 99.01 & 89.05 & 91.43 & 93.74 & 96.73 & 94.45 & 87.59 & 91.77 & 99.42 & 93.65\\
         \hline \hline
         X & 1.19 & 1.15 & 0.98 & 1.01 & 1.11 & 1.20 & 1.26 & 1.32 & 1.43 & 0.95\\
         \hline
         Y & 93.54 & 92.52 & 90.56 & 89.54 & 89.85 & 90.39 & 93.25 & 93.41 & 94.98 & 87.33\\
         \hline
        \end{tabular}
       \end{center}
  \begin{enumerate}[(a)]
   \item Encontre a fun\c{c}\~{a}o de verossimilhan\c{c}a.
   \item Encontre as estimativas de m\'{a}xima verossimilhan\c{c}a.
   \item Obtenha a verossimilhan\c{c}a conjunta para os par\^{a}metros
         \(\beta_{0}\) e \(\beta_{1}\):
    \begin{enumerate}[i.]
     \item considerando \(\sigma\) fixo com valor igual \`{a} sua estimativa.
     \item obtendo a verossimilhan\c{c}a (conjunta - 2D) perfilhada em
           rela\c{c}\~{a}o a \(\sigma\).
    \end{enumerate}
   \item Obtenha a verossimilhan\c{c}a perfilhada para os par\^{a}metros
         \(\beta_{0}\) e \(\beta_{1}\) individualmente.
  \end{enumerate}
  
 \item Considere uma amostra de uma v.a. \(Y\) em que assume-se que
       \(Y_{i} \sim P(\lambda_{i})\) em que o parâmetro \(\lambda_{i}\) \'{e}
       descrito por uma fun\c{c}\~{a}o de uma vari\'{a}vel explicativa
       \(\log (\lambda_{i}) = \beta_{0} + \beta_{1} x_{i}\) com valores
       conhecidos de \(x_{i}\).\\
       Dados simulados com (\(\beta_{0} = 2, \beta_{1} = 0.5\)) s\~{a}o
       mostrados a seguir.
       
       \begin{center}
        \begin{tabular}[H]{c|*{10}{c}}
         Y & 10 & 15 & 11 & 37 & 70 & 19 & 12 & 12 & 13 & 88\\
         \hline
         X & 1.7 & 1.5 & 0.5 & 2.8 & 4.4 & 1.8 & 0.4 & 0.7 & 1.3 & 5.1\\
        \end{tabular}
       \end{center}
  \begin{enumerate}[(a)]
   \item Obtenha o gr\'{a}fico da fun\c{c}\~{a}o de verossimilhan\c{c}a
         indicando a posi\c{c}\~{a}o dos valores verdadeiros dos
         par\^{a}metros.
  text \item Obtenha os perfis de verossimilhan\c{c}a dos par\^{a}metros.
   \item Obtenha estimativas.
   \item Obtenha intervalos (use diferentes m\'{e}todos).
   \item Compare os resultados anteriores com os fornecidos pela função
         \texttt{glm()} e discuta os achados.
  \end{enumerate}
\end{enumerate}

\vfill
\textit{last modification on ...}

<<cache=FALSE, echo=FALSE>>=
# <r code> -----
Sys.time()
# </r code> -----
@ 

\end{document}