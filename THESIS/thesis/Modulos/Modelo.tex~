Este Capítulo apresenta o novo modelo de regressão usado para análise de variáveis respostas limitadas multivariada, o qual chamaremos por modelo de regressão quase-beta multivariado. A~\autoref{cap:modelo} apresenta a especificação do modelo, e na~\autoref{cap:estimacao} encontra-se o método proposto para estimação dos parâmetros de regressão e dispersão. 

%=====================================================

\section{Modelo de regressão quase-beta multivariado}
\label{cap:modelo}

%=====================================================

Considere $\mathbf{Y}_{i} = ( \mathrm{Y}_{1i}, \ldots, \mathrm{Y}_{Ri} )^{\top}$ um vetor de variáveis respostas $R \times 1$ e seja $\boldsymbol{\mu}_{i} = ( \mu_{1i}, \ldots, \mu_{Ri} )^{\top}$ seu correspondente vetor de médias $R \times 1$, para $r = 1, \ldots, R$ variáveis respostas e $i = 1, \ldots, n$ indivíduos. Nesta notação, a média da $r$-ésima variável resposta para o $i$-ésimo indivíduo é dada por
$$\mu_{ri} = g^{-1}(\mathbf{x}_{i}^{\top}\boldsymbol{\beta}_{r}),$$ 
\noindent
onde $\mathbf{x}_{i}$ e $\boldsymbol{\beta}_r$ são $J_r \times 1$ vetores de covariáveis conhecidas e desconhecidos parâmetros de regressão e $g$ é uma função de ligação padrão. Logo, o modelo de regressão quase-beta multivariado é definido por
\begin{eqnarray}
\label{Model}
\mathrm{E}(\mathbf{Y}_i) &=&  \boldsymbol{\mu}_i  \nonumber    \\
\mathrm{Var}(\mathbf{Y}_i) &=& \boldsymbol{\Sigma}_i = \mathrm{V}(\boldsymbol{\mu}_{i})^{\tfrac{1}{2}}\boldsymbol{\Omega}(\boldsymbol{\lambda}) \mathrm{V}(\boldsymbol{\mu}_{i})^{\tfrac{1}{2}}
\end{eqnarray}
\noindent
onde $\boldsymbol{\Sigma}_i$ é uma matriz $R \times R$ e $\mathrm{V}(\boldsymbol{\mu}_i)$ denota uma matriz diagonal cujas entradas principais são dadas por $ \vartheta(\mu_{ri}) = \mu_{ri}(1 - \mu_{ri})$. Já a matriz  $\boldsymbol{\Omega}(\boldsymbol{\lambda})$ descreve a parte da covariância que não depende da estrutura de média. A ideia é semelhante a abordagem por equações de estimação generalizadas proposta por \citeonline{LiangZeger:1986} e \citeonline{Zeger:1988}, que usam uma matriz de correlação de ``trabalho"~para modelar dependência. Essa estrutura de depêndencia, geralmente é usada na análise de dados longitudinais, dados com medidas repetidas e dados agrupados. Como este modelo é baseado apenas em suposições de segundo momentos ele pode ser expresso, alternativamente, da seguinte forma
$$
\begin{pmatrix}
\mathrm{Y}_{1i} \vspace{-0.1cm} \\
\vdots \vspace{-0.1cm} \\
\mathrm{Y}_{Ri} \\
\end{pmatrix} \sim \bullet \begin{bmatrix}
\begin{pmatrix}
\mu_{1i} \vspace{-0.1cm}  \\
\vdots \vspace{-0.1cm}  \\
\mu_{Ri} \\
\end{pmatrix}; \boldsymbol{\Sigma}_i
\end{bmatrix},~i = 1, \ldots, n.
$$ 
\noindent
Note que, o modelo não supõe distribuição de probabilidade para as variáveis respostas, sendo que a notação $\bullet$ substitui tal suposição. De maneira geral, a matriz $\boldsymbol{\Sigma}_i$ tem a seguinte representação:

$$
\boldsymbol{\Sigma}_i =
\begin{footnotesize}
 \begin{bmatrix}
\sqrt{\mu_{1i}(1 - \mu_{1i})} \hspace{-0.2cm} &  & \hspace{-0.2cm} 0 \\
& \ddots & \\
0 & \hspace{-0.2cm} & \hspace{-0.2cm} \sqrt{\mu_{Ri}(1 - \mu_{Ri})}  \\
\end{bmatrix} \hspace{-0.04cm} \begin{bmatrix}
\sigma_1^2 \hspace{-0.2cm}  &  \hspace{-0.2cm} \ldots \hspace{-0.2cm}  &  \hspace{-0.2cm}~\rho_{1R} \sigma_1 \sigma_R \\
\vdots \hspace{-0.1cm} & \hspace{-0.2cm}  \ddots \hspace{-0.2cm} &  \hspace{-0.2cm} \vdots \\
\rho_{1R} \sigma_1 \sigma_R  \hspace{-0.1cm}  &  \hspace{-0.1cm} \ldots \hspace{-0.2cm}  &  \hspace{-0.2cm} \sigma_R^2 \\
\end{bmatrix} \hspace{-0.04cm} \begin{bmatrix}
\sqrt{\mu_{1i}(1 - \mu_{1i})}   \hspace{-0.2cm}  &  &  \hspace{-0.2cm} 0  \\
 & \ddots &  \\
0 & \hspace{-0.2cm} & \hspace{-0.2cm} \sqrt{\mu_{Ri}(1 - \mu_{Ri})}  \\
\end{bmatrix}.
\end{footnotesize}
$$
Assim, o modelo de regressão proposto nesta dissertação segue o estilo de quase-verossimilhança apresentado por \citeonline{Wedderburn:1974}, que combina a função de variância da distribuição binomial com as tradicionais funções de ligação para dados binários como, por exemplo, as ligações \textit{logit}, \textit{probit}, \textit{cloglog} e \textit{cauchit}.  


%=====================================================

\section{Estimação e inferência}
\label{cap:estimacao}

%=====================================================


Nesta seção, apresenta-se uma visão geral do método usado para estimação dos parâmetros do modelo de regressão quase-beta multivariado. A abordagem proposta é baseada em funções de estimação. Para maiores detalhes sobre o método e principais resultados, ver \citeonline{Jorgensen:2004} e \citeonline{Bonat:2016}.

Para estimar os parâmetros do modelo, a abordagem proposta combina as funções de estimação quase-score e Pearson para a estimação dos parâmetros de regressão e dispersão, respectivamente.  

Seja $\boldsymbol{\theta} = (\boldsymbol{\beta}^{\top}, \boldsymbol{\lambda}^{\top})^{\top}$ um vetor composto por dois conjuntos de parâmetros, em que $\boldsymbol{\beta} = (\boldsymbol{\beta}_1^{\top}, \ldots, \boldsymbol{\beta}_R^{\top})$ e $\boldsymbol{\lambda} = (\sigma_1^2, \sigma_2^2, \ldots, \sigma_R^2,\rho_{1}, \ldots, \rho_{R(R-1)/2})^{\top}$ são vetores $J \times 1$ e $Q \times 1$ de parâmetros associados aos coeficientes de regressão e dispersão, respectivamente. A função quase-escore para $\boldsymbol{\beta}$ é definida por:
\begin{equation*}
\psi_{\boldsymbol{\beta}}(\boldsymbol{\beta}, \boldsymbol{\lambda}) = \sum_{i=1}^{n} \mathbf{D}_i^{\top} \boldsymbol{\Sigma}_i^{-1} (\mathbf{Y}_i - \boldsymbol{\mu}_i),
\end{equation*}
onde $\mathbf{D} = \nabla_{\boldsymbol{\beta}}~\boldsymbol{\mu}_i$ é uma matriz $R \times J$ e $ \nabla_{\boldsymbol{\beta}}$ denota o operador gradiente. A matriz de sensibilidade $J \times J$ de $\psi_{\boldsymbol{\beta}}$ é dada por

% \left(\sum_{i=1}^{n} \dfrac{\partial \boldsymbol{\mu}_i}{\partial \beta_1} \boldsymbol{\Sigma}_i^{-1}(\mathbf{Y}_i - \boldsymbol{\mu}_i)^{\top}, \ldots, \sum_{i=1}^{n} \dfrac{\partial \boldsymbol{\mu}_i}{\partial \beta_Q} \boldsymbol{\Sigma}_i^{-1}(\mathbf{Y}_i - \boldsymbol{\mu}_i)^{\top}\right)^{\top},
%em que $\partial \mu_i/\partial \beta_j = \mu_i(1 - \mu_i) x_{ij}$ para $j = 1, \ldots Q$. A entrada $(j,k)$ da matriz de sensibilidade $Q \times Q$ de $\psi_{\boldsymbol{\beta}}$ é obtida por
\begin{equation}
\label{Sbeta}
\mbox{S}_{\boldsymbol{\beta}} = - \sum_{i=1}^{n} \mathbf{D}_i^{\top} \boldsymbol{\Sigma}_i^{-1} \mathbf{D}_i,
\end{equation}
onde a soma é elemento-a-elemento.
De maneira análoga, a matriz de variabilidade $J \times J$ para $\psi_{\boldsymbol{\beta}}$ é obtida por
\begin{equation*}
%\label{Vbeta}
\mbox{V}_{\boldsymbol{\beta}} = \sum_{i=1}^{n} \mathbf{D}_i^{\top} \boldsymbol{\Sigma}_i^{-1} \mathbf{D}_i.
\end{equation*}



A função de estimação de Pearson foi usada para estimar os parâmetros de dispersão e de acordo com \citeonline{Jorgensen:2004,Bonat:2016,Bonat:2017b} ela tem a seguinte forma:
$$
\psi_{\boldsymbol{\lambda}_q}(\boldsymbol{\lambda}, \boldsymbol{\beta}) = \sum_{i=1}^n \mathrm{tr} \left\{ W_{i \boldsymbol{\lambda}_q} \big[ \Delta_i^{\top}\Delta_i - \boldsymbol{\Sigma}_i \big]  \right\},~q = 1, \ldots, Q,
$$
onde o operador $\mathrm{tr}$ denota o traço da matriz, $\Delta_i = \mathbf{Y}_i - \boldsymbol{\mu}_i$ e $W_{i \boldsymbol{\lambda}_q} = - \partial \boldsymbol{\Sigma}_i^{-1}/\partial \boldsymbol{\lambda}_q$. Detalhes sobre como calcular essa matriz podem ser vistos em \citeonline{Bonat:2016} Seção 3.1.

A entrada $(q,q^{\prime})$ da matriz $Q \times Q$ de sensitividade para $\psi_{\boldsymbol{\lambda}}$ é dada por
\begin{equation}
\label{Slambda}
\mathrm{S}_{\boldsymbol{\lambda}_{qq^{\prime}}} = \mathrm{E}\left ( \frac{\partial}{\partial \boldsymbol{\lambda}_q}\psi_{\boldsymbol{\lambda}_{q^{\prime}} } (\boldsymbol{\lambda}, \boldsymbol{\beta})  \right ) = -\sum_{i=1}^n \mathrm{tr} \Big(  W_{i \boldsymbol{\lambda}_q} \boldsymbol{\Sigma}_i W_{i \boldsymbol{\lambda}_{q^\prime} }\boldsymbol{\Sigma}_i  \Big).
\end{equation}

A matriz de sensitividade cruzada para $\boldsymbol{\beta}$ e $\boldsymbol{\lambda}$ é dada por
\begin{equation}
\label{Sbetalambda}
\mathrm{S}_{ \boldsymbol{\beta}_j  \boldsymbol{\lambda}_q} = \mathrm{E}\left ( \frac{\partial}{\partial  \boldsymbol{\lambda}_q}\psi_{ \boldsymbol{\beta}_j }(\boldsymbol{\beta}, \boldsymbol{\lambda})  \right ) = \mathbf{0}
\end{equation}
e
\begin{equation}
\label{Slambdabeta}
\mathrm{S}_{\boldsymbol{\lambda}_q \boldsymbol{\beta}_j} = \mathrm{E}\left ( \frac{\partial}{\partial \boldsymbol{\beta_j}}\psi_{\boldsymbol{\lambda}_q}(\boldsymbol{\lambda}, \boldsymbol{\beta})  \right ) = -\sum_{i=1}^n \mathrm{tr} \Big( W_{i\boldsymbol{\lambda_q}}\boldsymbol{\Sigma}_i W_{i\boldsymbol{\beta}_j}\boldsymbol{\Sigma}_i  \Big),
\end{equation}
em que $W_{i\boldsymbol{\beta}_j} = -\partial \boldsymbol{\Sigma}^{-1}/\partial \boldsymbol{\beta}_j$. 

A matriz de sensitividade conjunta para o vetor $\boldsymbol{\theta}$ fica representada por

\begin{equation*}
\mathrm{S}_{\boldsymbol{\theta}} = \begin{pmatrix}
\mathrm{S}_{\boldsymbol{\beta}} & \boldsymbol{0} \medskip \\ 
\mathrm{S}_{\boldsymbol{\lambda}\boldsymbol{\beta}} & \mathrm{S}_{\boldsymbol{\lambda}}
\end{pmatrix},
\end{equation*}
cujas entradas são definidas pelas Equações (\ref{Sbeta})-(\ref{Slambdabeta}).

A variância assintótica dos estimadores baseados em funções de estimação denotado por $\boldsymbol{\hat{\theta}}$ é obtido pela inversa da matriz de informação Godambe, cuja forma geral para o vetor de parâmetros $\boldsymbol{\theta}$ é $\mathrm{J}_{\boldsymbol{\theta}}^{-1} = \mathrm{S}_{\boldsymbol{\theta}}^{-1}\mathrm{V}_{\boldsymbol{\theta}}\mathrm{S}_{\boldsymbol{\theta}}^{-\top}$, onde $-\top$ denota a transposta inversa. A matriz de variabilidade para $\boldsymbol{\theta}$ tem a forma

\begin{equation}
\label{eq:VarMatrix}
\mathrm{V}_{\boldsymbol{\theta}} = \begin{pmatrix}
\mathrm{V}_{\boldsymbol{\beta}} & \mathrm{V}_{\boldsymbol{\beta}\boldsymbol{\lambda}} \medskip \\
\mathrm{V}_{\boldsymbol{\lambda}\boldsymbol{\beta}} & \mathrm{V}_{\boldsymbol{\lambda}}
\end{pmatrix}
\end{equation}
\noindent
onde $\mathrm{V}_{\boldsymbol{\lambda}\boldsymbol{\beta}} = \mathrm{V}^{\top}_{\boldsymbol{\beta}\boldsymbol{\lambda}}$ e $\mathrm{V}_{\boldsymbol{\lambda}}$ dependem do terceiro e quarto momentos de $\mathbf{Y}_i$, respectivamente.

 As entradas para matriz de variabilidade empírica são dadas por:
\[
\tilde{\mathrm{V}}_{\boldsymbol{\lambda}_{q{q^\prime}}} = \sum_{i=1}^n \psi_{\boldsymbol{\lambda}_j}(\boldsymbol{\lambda}, \boldsymbol{\beta})_i \psi_{\boldsymbol{\lambda}_{q^\prime}}(\boldsymbol{\lambda}, \boldsymbol{\beta})_i \quad \text{e}  \\  \quad \tilde{\mathrm{V}}_{\boldsymbol{\lambda}_q \boldsymbol{\beta}_{q^\prime}} = \sum_{i=1}^n \psi_{\boldsymbol{\lambda}_{q^\prime}}(\boldsymbol{\lambda}, \boldsymbol{\beta})_i \psi_{\boldsymbol{\beta}_{q^\prime}}(\boldsymbol{\lambda}, \boldsymbol{\beta})_i.
\]


Denote $\boldsymbol{\hat{\theta}}$ o estimador função de estimação de $\boldsymbol{\theta}$. De acordo com \citeonline{Godambe:1978,Jorgensen:2004} a distribuição assintótica de $\boldsymbol{\hat{\theta}}$ é dada por
\begin{equation*}
\boldsymbol{\hat{\theta}} \sim \mathcal{N}(\boldsymbol{\theta}, \mathrm{J}_{\boldsymbol{\theta}}^{-1}), \quad
\end{equation*}
onde~$\mathrm{J}_{\boldsymbol{\theta}}^{-1} =  \mathrm{S}_{\boldsymbol{\theta}}^{-1}\mathrm{V}_{\boldsymbol{\theta}}\mathrm{S}_{\boldsymbol{\theta}}^{-\top}$ é a matriz de informação de Godambe.

O Algoritmo Chaser foi proposto por \citeonline{Jorgensen:2004} para resolver o sistema de equações $\psi_{\boldsymbol{\beta}} = \mathbf{0}$ e $\psi_{\boldsymbol{\lambda}} = \mathbf{0}$ e sua definição é dada por:
\begin{eqnarray}
\label{chaser}
\boldsymbol{\beta}^{(i+1)} &=& \boldsymbol{\beta}^{(i)} - \mathrm{S}_{\boldsymbol{\beta}}^{-1} \psi_{\boldsymbol{\beta}}(\boldsymbol{\beta}^{(i)}, \boldsymbol{\lambda}^{(i)}) \nonumber \\
\boldsymbol{\lambda}^{(i+1)} &=& \boldsymbol{\lambda}^{(i)} - \alpha \mathrm{S}_{\boldsymbol{\lambda}}^{-1} \psi_{\boldsymbol{\lambda}}(\boldsymbol{\beta}^{(i+1)}, \boldsymbol{\lambda}^{(i)}).
\end{eqnarray}

A principal característica do algoritmo chaser é a propriedade de insensitividade, que permite separar $\boldsymbol{\beta}$ e $\boldsymbol{\lambda}$ em duas equações para serem atualizadas em cada passo. Além disso, o parâmetro $\alpha$ foi introduzido como um \textit{tuning} constante para controlar o tamanho do passo. É importante observar que esse algoritmo é um caso especial do algoritmo flexível apresentado por \citeonline{Bonat:2016}, no contexto de modelos lineares generalizados multivariados com estrutura de covariância (McGLMs), e foi adaptado para o modelo proposto nesta dissertação. Desse modo, toda a implementação computacional do modelo foi feita no \textit{software} estatístico \texttt{R} \cite{R} e está disponível no pacote \texttt{mcglm} \cite{Bonat:2015a} por meio da função \texttt{mcglm()}.


%=====================================================

\section{Testes de hipóteses multivariado}
\label{cap:testehipoteses}

%=====================================================

Esta seção introduz um novo teste de hipóteses para dados limitados multivariados. Devido às suposições feitas ao modelo proposto nesta dissertação, os tradicionais pressupostos de normalidade assim como de homogeneidade entre grupos são relaxados. O principal objetivo do teste proposto nesta seção, é testar o efeito conjunto dos coeficientes de regressão para cada variável resposta. Para isso, foi proposta uma abordagem semelhante a estatística de Hotelling-Lowley para dados não gaussinaos multivariados. As hipóteses do teste são definidas por:

\begin{equation*}
H_0: \mathbf{L}\boldsymbol{\beta} = \mathbf{0}  \quad  \mathrm{versus}  \quad  H_1: \mathbf{L}\boldsymbol{\beta} \neq \mathbf{0}
\end{equation*}



\begin{equation*}
\mathcal{W}_s = (\mathbf{L} \boldsymbol{\beta})^{\top} (\mathbf{L} \mathbf{J}_{\boldsymbol{\beta}} \mathbf{L}^{\top})^{-1}  (\mathbf{L} \boldsymbol{\beta}),
\end{equation*}
onde $\mathbf{J}_{\boldsymbol{\beta}} = \mathbf{D}^{\top} \boldsymbol{\Sigma}^{-1} \mathbf{D}$.
