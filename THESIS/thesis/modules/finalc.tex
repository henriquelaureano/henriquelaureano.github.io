The general goal of this master thesis was the proposition of a new
regression model for the analysis of clustered competing risks
data. Focused on the modeling of the cumulative incidence function
(CIF), in the probability scale, instead of the hazard scale usual on
the survival modeling literature. We model the clustered competing risks
on a latent-effects framework, a generalized linear mixed model (GLMM)
\cite{GLMM}, with a multinomial distribution for the competing risks and
censorship, conditioned on the possible covariates and
latent-effects. The within-cluster dependency is accommodated by a
multivariate Gaussian distribution and is modeled via its covariance
matrix parameters.

The failures by the competing causes and their respective censorships
are modeled in the probability scale, by means of the CIF
\cite{kalb&prentice, andersen12}. The CIF is accommodated in our GLMM
framework in terms of the link function, as the product of two
functions, one responsible to model the immediate risk and the other the
trajectory time. The shape of these functions is described in detail in
\autoref{cap:model}. This particular GLMM formulation is what makes our
model, particular. Thus, we have what we call a multiGLMM: a multinomial
GLMM for clustered competing risks data.

The two-function product CIF formulation was taken from
\citeonline{SCHEIKE} but there they use a different framework, a
composite likelihood framework \cite{lindsay88, cox&reid04, varin11}.
Here we do a full likelihood analysis instead. A composite approach is
generally used when a full likelihood approach is impossible or
computationally impracticable. Our goal here was to try a full
likelihood framework taking advantage of state-of-art computational
libraries and very efficient algorithm implementations. We have all this
with the \texttt{R} \cite{R21} package TMB \cite{TMB}.

In \autoref{cap:results} our main results are presented, and it is clear
that the multiGLMM works under certain circumstances. The next step was
to compare our results with the ones obtained in \citeonline{SCHEIKE},
with the composite approach. In the GitHub repository
\url{https://github.com/kkholst/mcif/} the authors provide their code.
In \texttt{mcif/inst/examples/datasim.R} they show how to simulate from
the model, and in \texttt{mcif/src/loglik.cpp} they have their marginal
log-likelihood function. We tried to optimize their marginal
log-likelihood over its parameters using basically all \texttt{R}
\texttt{base::optim()} and \texttt{base::nlminb()} available methods, in
the paper was used the BFGS, one of them. We made several scenarios,
using their own simulation scripts and ours, and to our surprise, the
model basically does not work.

The optimization in its majority fails, via any gradient-based algorithm
(BFGS \cite{nocedal&wright}, PORT \cite{PORTreport, PORTpaper},
conjugate gradient (CG) \cite{CG}), generally by Hessian matrix
instability problems, a problem which our multiGLMM also suffers from
when we try to compute the parameters standard errors. When the model
works, it is because we are using the parameter true values as initial
guesses i.e. if the algorithm needs to walk on the log-likelihood
surface following the gradient, it fails. Even when it works, the
estimates are not always good. We also tried with a SANN and a
Nelder-Mead algorithm. SANN \cite{SANN} is a variant of a simulated
annealing method, based on a Metropolis algorithm. Since it is based on
simulation, it takes a lot of time and as the gradient-based methods, do
not work most of the time. The best results were with the Nelder-Mead
\cite{neldermead}, a free-gradient method. Still, it only works when we
use the parameter true values as initial guesses. This situation,
completely the opposite of what is shown in the paper, made impossible
any reasonable comparison between the models. We will enter in contact
with the authors to see what is happening.

All models from the simulation study were run, in a parallelized
fashion, or in a Linux system with
\begin{itemize}
 \item 12 Intel (R) Core (TM) i7-8750H CPU @ 2.20GHz processors,
 \item 16GB RAM,
\end{itemize} 
or in a, also, Linux system with
\begin{itemize}
 \item 30 Intel (R) Xeon (R) CPU E5-2690 v2 @ 3.00GHz processors,
 \item 206GB RAM.
\end{itemize}

The risk and time models are not so time-consuming, generally never
taking more than 5 minutes (at max) to run. The inherent idea is that we
are always performing two-dimension integral approximations and we have
\textit{just} three covariance parameters. With the block-diag model, we
are theoretically in four dimensions. However, since the covariance
matrix is, block-diagonal, we experienced several numerical instability
problems. The solution, as can be seen in the code
in \autoref{cap:blockdiagModel} (\autoref{cap:appendixD}), was to split
it into two two-dimension matrices, since the \(4\times4\) covariance
matrix is block-diagonal. This simple solution solved all numerical
instability problems. The computational time was only a little bit
bigger than with the risk and time models. 

Finally, the complete model. In the biggest scenario, with 60 thousand
data points and clusters of size 2 i.e., 30 thousand four-dimension (ten
parameters in the covariance matrix) integral approximations, with TMB
this takes 30 minutes, parallelizing between the threads.

\section{FUTURE WORKS}
\label{cap:future}

% END ==================================================================
