The general goal of this master thesis was the proposition of a new
regression model for the analysis of clustered competing risks
data. Focused on the modeling of the cumulative incidence function
(CIF), in the probability scale, instead of the hazard scale usual on
the survival modeling literature. We model the clustered competing risks
on a latent-effects framework, a generalized linear mixed model (GLMM),
with a multinomial distribution for the competing risks and censorship,
conditioned on the possible covariates and latent-effects. The
within-cluster dependency is accommodated by a multivariate Gaussian
distribution and is modeled via its covariance matrix parameters.

The failures by the competing causes and their respective censorship are
modeled in the probability scale, by means of the CIF. The CIF is
accommodated in our GLMM framework in terms of the link function, as the
product of two functions, one responsible to model the immediate risk
and the other the trajectory time. The shape of these functions is
described in detail in \autoref{cap:model}. This particular GLMM
formulation is what makes our model, particular. Thus, we have what we
call a multiGLMM: a multinomial GLMM for clustered competing risks data.

The two-product CIF formulation was taken from \citeonline{SCHEIKE} but
there they use a different framework, a composite likelihood
framework. Here we do a full likelihood analysis instead. A composite
approach is generally used when a full likelihood approach is impossible
or computationally impracticable. Our goal here was to try a full
likelihood framework taking advantage of state-of-art computational
libraries and very efficient algorithm implementations. We have all this
with the \texttt{R} \cite{R21} package TMB \cite{TMB}.

In \autoref{cap:results} our main results are presented, and it is clear
that the multiGLMM works under certain circumstances. The next step was
to compare our results with the ones obtained in \citeonline{SCHEIKE},
with a composite approach. In the GitHub repository
\url{https://github.com/kkholst/mcif/} the authors provide their code.
In \texttt{mcif/inst/examples/datasim.R} they show how to simulate from
the model, and in \texttt{mcif/src/loglik.cpp} they have their marginal
log-likelihood function.

We tried to optimize the marginal log-likelihood over its parameters
using basically all base::optim() and base::nlminb() available methods,
in the paper was used the BFGS, one of them. We made several scenarios,
using their own simulation scripts and ours, and to our surprise, the
model basically does not work. The optimization in its majority fails,
via any gradient-based algorithm (BFGS, PORT, CG), generally by Hessian
matrix instability problems, a problem which our model also suffers from
when we try to compute the parameters standard errors. When the model
works, it is because we are using the parameter true values as initial
guesses i.e. if the algorithm needs to walk on the log-likelihood
surface, it fails. We also tried with a SANN and a Nelder-Mead
algorithm. SANN is a variant of a simulated annealing method, based on a
Metropolis algorithm. Since it is based on simulation, it takes a lot of
time and as the gradient-based methods, do not work most of the
time. The best results were with the Nelder-Mead, a free-gradient
method. Still, it only works when we use the parameter true values as
initial guesses. This situation, completely the opposite of what is
shown in the paper, made any reasonable comparison between the models
impossible. We will still enter in contact with the authors to try to
see what is happening.

\section{FUTURE WORKS}
\label{cap:future}

% END ==================================================================
