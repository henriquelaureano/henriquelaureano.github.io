[
["index.html", "README", " README A pdf version of this material is available here Last modification on 2019-07-10 15:32:03 "],
["uniform-distribution.html", "Uniform distribution", " Uniform distribution Let \\(X\\) be a r.v. with uniform distribution \\(X \\sim U(0, \\theta)\\). A random sample gave the following values: 5.5; 3.2; 4, 8, 5.3; 3.8 and 5.0. Obtain the likelihood function and one or more appropriate interval(s) specifying the form of obtaining. data &lt;- c(5.5, 3.2, 4.8, 5.3, 3.8, 5) The likelihood of a \\({\\rm Uniform}(0, \\theta)\\) is \\[\\begin{align*} L(\\theta) &amp;= \\theta^{-n}, \\quad \\text{for } x_{i} &lt; \\theta \\text{ for all } i\\\\ &amp;= \\theta^{-n}, \\quad \\text{for } \\theta &gt; x_{(n)}, \\end{align*}\\] and equal to zero otherwise. Given the data above, we get \\(x_{(n)} = 5.5\\) and the likelihood is shown in Figure 1, together with some intervals. lkl.unif &lt;- function(theta, data) { n &lt;- length(data) ; xn &lt;- max(data) # this next step, in the way presented, isn&#39;t very efficient, since I&#39;ll # compute the likelihood for all theta&#39;s and then, if necessary, convert to # zero. however, since we have here a very simple likelihood (and this # notation is extremely clear, btw), I keep in this way lkl = theta**(-n)*{theta &gt;= xn} return(lkl) } theta.seq &lt;- seq(4, 12, length.out = 100) lklseq.unif &lt;- lkl.unif(theta.seq, data) par(mfrow = c(1, 2), par(mar = c(4, 4, 2, 2) + .1)) # plot window definition # plotting the normalized likelihood function to have unit maximum plot(theta.seq, lklseq.unif/max(lklseq.unif), type = &quot;l&quot;, xlab = expression(theta), ylab = &quot;Likelihood&quot;) # probability-based interval ----------------------------------------------------- # cutoff&#39;s corresponding to a 95\\% and 99\\% confidence interval for the mean cuts &lt;- c(.15, .04) * lkl.unif(max(data), data) / max(lklseq.unif) abline(h = cuts, lty = 2:3) # uniroot.all finds the zeros, so we need to subtract the cutoff point from # the likelihood to be able to find the points where the likelihood is cut ic.lkl.unif &lt;- function(theta, cut, ...) { lkl.unif(theta, ...)/max(lklseq.unif) - cut } ic.95.prob &lt;- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq), data = data, cut = cuts[1]) ic.99.prob &lt;- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq), data = data, cut = cuts[2]) abline(v = ic.95.prob, lty = 2) ; abline(v = ic.99.prob, lty = 3) plot(theta.seq, lklseq.unif/max(lklseq.unif), type = &quot;l&quot;, xlab = expression(theta), ylab = &quot;Likelihood&quot;) # pure likelihood interval ------------------------------------------------------- # cutoff&#39;s corresponding to a 95\\% and 99\\% confidence interval for the mean cuts &lt;- c(.05, .01) * lkl.unif(max(data), data) / max(lklseq.unif) abline(h = cuts, lty = 2:3) ic.95.pure &lt;- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq), data = data, cut = cuts[1]) ic.99.pure &lt;- rootSolve::uniroot.all(ic.lkl.unif, range(theta.seq), data = data, cut = cuts[2]) abline(v = ic.95.pure, lty = 2) ; abline(v = ic.99.pure, lty = 3) Figure 1: Likelihood function of \\(\\theta\\) in Uniform(0, \\(\\theta\\)) based on \\(x_{(6)}\\) = 5.5. In the left, likelihood intervals at 15% (dashed line) and 4% (dotted line) cutoff. In the right, likelihood intervals at 5% (dashed line) and 1% (dotted line) cutoff. These cutoffs correspond, in both sides, to a 95% and 99% confidence interval, respectively. In both graphs of Figure 1 we have two confidence intervals for \\(\\theta\\), one corresponding to a 95% confidence interval (dashed line) and one corresponding to a 99% confidence interval for the mean (dotted line). In the left we have a probability-based likelihood interval, in the right we have a pure likelihood interval. The left ones are bad intervals, since they’re based in a large-sample theory that results in an exact interval for the Gaussian case, in a good approximation for a reasonably regular case, and as we can see by the figure… here we doesn’t have a regular likelihood (the likelihood isn’t well approximated by a quadratic function), thus, here this interval should not be so good. Explaining better how we arrived in these intervals: For a normalized likelihood, \\(L(\\theta)/L(\\hat{\\theta})\\), we have the following Wilk’s likelihood ratio statistic, defined as \\(W\\) \\[ W \\equiv 2 \\log \\frac{L(\\hat{\\theta})}{L(\\theta)} \\sim \\chi_{1}^{2}. \\] Its \\(\\chi^{2}\\) distribution is exact only in the normal mean model, and approximately true when the likelihood is reasonably regular. Based in this Wilk’s statistic we are able to find the probability that the likelihood interval covers \\(\\theta\\), \\[ {\\rm Pr}\\left\\{\\frac{L(\\theta)}{L(\\hat{\\theta})} &gt; c\\right\\} = {\\rm Pr}\\left\\{2 \\log \\frac{L(\\theta)}{L(\\hat{\\theta})} &lt; -2 \\log c \\right\\} = {\\rm Pr}\\{\\chi_{1}^{2} &lt; -2 \\log c\\}. \\] So, if for some \\(0 &lt; \\alpha &lt; 1\\) we choose a cutoff \\[ c = e^{-\\frac{1}{2} \\chi_{1, (1-\\alpha)}^{2}}, \\] where \\(\\chi_{1, (1-\\alpha)}^{2}\\) is the \\(100 (1-\\alpha)\\) percentile of \\(\\chi_{1}^{2}\\).\\ For \\(\\alpha = 0.05\\) and \\(0.01\\) we have a cutoff \\(c = 0.15\\) and \\(0.04\\), that corresponds to a 95% and 99% confidence interval, respectively. Having the cutoff values we need now to find the interval values, i.e., in which points the cutoff horizontal line cuts the likelihood. For this purpose we use the function \\(\\texttt{rootSolve::uniroot.all}\\). A likelihood interval at 15% and 4% cutoff for \\(\\theta\\) are (5.5, 7.545) and (5.5, 9.405). Already on the right side of Figure 1 we have a more coherent, let’s say, confidence interval for the Uniform likelihood. While the likelihood isn’t regular, it is still possible to provide an exact theoretical justification for a confidence interval interpretation. Now \\[ {\\rm Pr}\\left\\{\\frac{L(\\theta)}{L(\\hat{\\theta})} &gt; c\\right\\} = {\\rm Pr}\\left\\{\\frac{X_{(n)}}{\\theta} &gt; c^{1/n}\\right\\} = 1 - {\\rm Pr}\\left\\{\\frac{X_{(n)}}{\\theta} &lt; c^{1/n}\\right\\} = 1 - (c^{1/n})^{n} = 1 - c. \\] So the likelihood interval with cutoff \\(c\\) is a \\(100 (1-c)\\)% confidence interval. A likelihood interval at 15% and 4% cutoff for \\(\\theta\\) are (5.5, 9.062) and (5.5, 11.849). Comparing the obtained intervals we see a broader range with the pure likelihood intervals. In other words, with the pure likelihood intervals we see a higher uncertainty than with the (not so recommended here) probability-based likelihood intervals. For doing this exercise I read and used it Pawitan’s book: @book{pawitan, author = {Yudi Pawitan}, title = {In All Likelihood: Statistical Modelling and Inference Using Likelihood}, year = {1991}, publisher = {Oxford University Press}, address = {Great Clarendon Street, Oxford OX2 6DP}, } "],
["poisson-distribution.html", "Poisson distribution Lkl, quad. approx. and intervals Another parametrization More intervals", " Poisson distribution Let \\(X\\) be a r.v. of a Poisson distribution (\\(X \\sim P(\\lambda)\\)) for which the following random sample was obtained: (3, 1, 0, 2, 1, 1, 0, 0). data &lt;- c(3, 1, 0, 2, 1, 1, 0, 0) Lkl, quad. approx. and intervals Obtain the likelihood function, its quadratic approximation and intervals for \\(\\lambda\\). The likelihood and the log-likelihood of a Poisson(\\(\\lambda\\)) is \\[ L(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{x_{i}}}{x_{i}!} \\quad \\text{and} \\quad \\log L(\\lambda) = l(\\lambda) = -n \\lambda + \\log \\lambda \\sum_{i=1}^{n} x_{i} - \\sum_{i=1}^{n} \\log x_{i}!. \\] We work here with the log-likelihood to make our life easier, since the computations are much simpler in the log scale. By computing the Score (derivative of \\(l(\\lambda)\\) wrt to \\(\\lambda\\)) and making it equal to zero, we find the MLE \\(\\hat{\\lambda} = \\bar{x}\\). Computing the second derivative we find the observed information \\[ I_{O}(\\lambda) = \\frac{n \\bar{x}}{\\lambda^{2}} \\quad \\rightarrow \\quad I_{O}(\\hat{\\lambda}) = \\frac{n}{\\bar{x}}. \\] A graph of the Poisson log-likelihood is provided in black in Figure 2. Doing a quadratic approximation (Taylor expansion of second order) in \\(l(\\lambda)\\) around \\(\\hat{\\lambda}\\) we have \\[ \\begin{align*} l(\\lambda) &amp;\\approx l(\\hat{\\lambda}) + (\\lambda - \\hat{\\lambda}) {l}&#39;(\\hat{\\lambda}) + \\frac{1}{2} (\\lambda - \\hat{\\lambda})^{2} {l}&#39;&#39;(\\hat{\\lambda})\\\\ &amp;\\quad(\\text{the Score is zero at the MLE})\\\\ &amp;= l(\\hat{\\lambda}) + \\frac{1}{2} (\\lambda - \\hat{\\lambda})^{2} {l}&#39;&#39;(\\hat{\\lambda}) = l(\\hat{\\lambda}) + \\frac{1}{2} (\\lambda - \\hat{\\lambda})^{2} I_{O}(\\hat{\\lambda}). \\end{align*} \\] A graph of the quadratic approximation of the Poisson log-likelihood is provided in gray in Figure 2. There we can see how good is the approximation around the maximum likelihood estimator (MLE). Here the sample size is very small, the idea is that as the sample size increase the quality, in this case the range, of the approximation also increase. This should happen because as the sample size increase the Poisson likelihood should be more symmetric. In the topright graph of Figure 2 we have two intervals for \\(\\lambda\\). # likelihood --------------------------------------------------------------------- lkl.poi &lt;- function(lambda, data) { n &lt;- length(data) # log-likelihood ignoring irrelevant constant terms lkl = -n * lambda + sum(data) * log(lambda) return(lkl) } lambda.seq &lt;- seq(0, 4.5, length.out = 100) lklseq.poi &lt;- lkl.poi(lambda.seq, data) par(mar = c(4, 4, 2, 2) + .1) layout(matrix(c(1, 3, 2, 3), 2, 2), heights = c(1.5, 1)) # plot window definition plot(lambda.seq, lklseq.poi, type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;log-likelihood&quot;) lambda.est &lt;- mean(data) # MLE abline(v = lambda.est, lty = 3) # quadratic approximation -------------------------------------------------------- quadprox.poi &lt;- function(lambda, lambda.est, data) { n &lt;- length(data) obs.info &lt;- n / lambda.est # observed information lkl.poi(lambda.est, data) - .5 * obs.info * (lambda - lambda.est)**2 } curve(quadprox.poi(x, lambda.est, data), col = &quot;darkgray&quot;, add = TRUE) legend(2.4, -6.25, c(&quot;log-like&quot;, &quot;Quadratic\\napprox.&quot;, &quot;MLE&quot;), col = c(1, &quot;darkgray&quot;, 1), lty = c(1, 1, 3), bty = &quot;n&quot;) # intervals for lambda ----------------------------------------------------------- plot(lambda.seq, lklseq.poi, type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;log-likelihood&quot;) curve(quadprox.poi(x, lambda.est, data), col = &quot;darkgray&quot;, add = TRUE) abline(v = lambda.est, lty = 3) ## probability-based interval ---------------------------------------------------- # cutoff&#39;s corresponding to a 95\\% confidence interval for the mean cut &lt;- log(.15) + lkl.poi(lambda.est, data) ; abline(h = cut, lty = 2) ic.lkl.poi &lt;- function(lambda, cut, ...) { lkl.poi(lambda, ...) - cut } ic.95.prob &lt;- rootSolve::uniroot.all(ic.lkl.poi, range(lambda.seq), data = data, cut = cut) arrows(x0 = ic.95.prob, y0 = rep(cut, 2), x1 = ic.95.prob, y1 = rep(-25, 2), lty = 2, length = .1) ## wald confidence interval ----------------------------------------------------- cut &lt;- log(.15) + quadprox.poi(lambda.est, lambda.est, data) abline(h = cut, lty = 2, col = &quot;darkgray&quot;) se.lambda.est &lt;- sqrt(lambda.est / length(data)) wald.95 &lt;- lambda.est + qnorm(c(.025, .975)) * se.lambda.est arrows(x0 = wald.95, y0 = rep(cut, 2), x1 = wald.95, y1 = rep(-25, 2), lty = 2, length = .1, col = &quot;darkgray&quot;) # a better look in the approximation around the MLE ------------------------------ lambda.seq &lt;- seq(.8, 1.2, length.out = 25) plot(lambda.seq, lkl.poi(lambda.seq, data), type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;log-likelihood&quot;) curve(quadprox.poi(x, lambda.est, data), col = &quot;darkgray&quot;, add = TRUE) abline(v = lambda.est, lty = 3) Figure 2: log-likelihood function and MLE of \\(\\lambda\\) in Poisson(\\(\\lambda\\)) based on . In the topleft, a quadratic approximation in gray. In the topright, two intervals for \\(\\lambda\\) - one based in the likelihood (in black) and one based in the quadratic approximation (in gray). In the bottom we provide a better look in the quadratic approximation. In the topright, in black, we have a interval based in the likelihood, \\(\\lambda \\in (0.459, 1.855)\\), but with a cutoff criterion based in a \\(\\chi^{2}\\) distribution. Thus, to have a nominal 95% confidence interval we use a cutoff \\(c = 15\\%\\). As we saw before, this interval is based in a large-sample theory. Since we’re dealing here with a reasonable regular case, this interval shows as a good approximation. Still in the topright of Figure 2, in red we have a interval based in the quadratic approximation of the log-likelihood, \\(\\lambda \\in (0.307, 1.693)\\). From the quadratic approximation we get \\[ \\log \\frac{L(\\lambda)}{L(\\hat{\\lambda})} \\approx -\\frac{1}{2} I_{O}(\\hat{\\lambda}) (\\lambda - \\hat{\\lambda})^{2}, \\] that also follows a \\(\\chi^{2}\\) distribution, since we have a r.v. \\(\\hat{\\lambda}\\) normalized (with its expected value subtracted and divided by its variance) and squared. From this we get the following exact (in the normal case) 95% confidence interval \\[ \\hat{\\lambda} \\pm 1.96~I_{O}(\\hat{\\lambda})^{-1/2} \\qquad (\\hat{\\lambda} \\pm 1.96~\\text{se}(\\hat{\\lambda})). \\] In the nonnormal cases this is an approximate 95% CI. The actual variance is \\(I_{E}(\\hat{\\lambda})^{-1/2}\\), but for the Poisson case the Fisher (expected) information is equal to the observed one, \\(I_{O}(\\hat{\\lambda})^{-1/2}\\). A very nice thing that we can see from this intervals is that a Wald interval (a \\(\\pm\\) interval) corresponds to a cut in the quadratic approximation exactly in the same point that the probability-based interval cuts the (log-)likelihood. Thus, as more regular the likelihood, better will be the fit of the approximation and more reliable will be the Wald interval. Another parametrization Repeat the previous question for reparametrization \\(\\theta = \\log (\\lambda)\\). By the invariance property of the MLE we have \\[ \\hat{\\lambda} = \\bar{x} \\quad \\Rightarrow \\quad g(\\hat{\\lambda}) = \\log \\hat{\\lambda} = \\hat{\\theta} = \\log \\bar{x} = g(\\bar{x}). \\] i.e., the MLE of \\(\\hat{\\theta}\\) is \\(\\log \\bar{x}\\). By the Delta Method we compute the variance of \\(\\hat{\\theta}\\), \\[ V[\\theta] = V[g(\\lambda)] = \\left[\\frac{\\partial}{\\partial \\lambda} g(\\lambda) \\right]^{2} V[\\lambda] = \\left[\\frac{1}{\\lambda}\\right]^{2} \\frac{\\lambda}{n} = \\frac{1}{\\lambda~n} \\quad \\rightarrow \\quad V[\\hat{\\theta}] = \\frac{1}{\\bar{x}~n}. \\] From this we can take the observed information for the reparametrization \\[ V[\\hat{\\theta}] = I_{O}^{-1}(\\hat{\\theta}) = (\\bar{x}~n)^{-1}. \\] Now we do, in the same manner, everything that we did in the previous letter. # likelihood --------------------------------------------------------------------- ## we can still use the lkl.poi function, but now we do ## lambda = exp(theta), ## with theta being a real rate (positive or negative) theta.seq &lt;- seq(-2, 2, length.out = 100) lklseq.poi &lt;- lkl.poi(exp(theta.seq), data) par(mar = c(4, 4, 2, 2) + .1) layout(matrix(c(1, 3, 2, 3), 2, 2), heights = c(1.5, 1)) # plot window definition plot(theta.seq, lklseq.poi, type = &quot;l&quot;, xlab = expression(theta), ylab = &quot;log-likelihood&quot;) theta.est &lt;- log(mean(data)) # MLE abline(v = theta.est, lty = 3) # quadratic approximation for the reparametrization ------------------------------ quadprox.poi.repa &lt;- function(theta, theta.est, data) { obs.info &lt;- sum(data) # observed information lkl.poi(exp(theta.est), data) - .5 * obs.info * (theta - theta.est)**2 } curve(quadprox.poi.repa(x, theta.est, data), col = &quot;darkgray&quot;, add = TRUE) legend(-2.15, -27.5, c(&quot;log-like&quot;, &quot;Quadratic\\napprox.&quot;, &quot;MLE&quot;), col = c(1, &quot;darkgray&quot;, 1), lty = c(1, 1, 3), bty = &quot;n&quot;) # intervals for lambda ----------------------------------------------------------- plot(theta.seq, lklseq.poi, type = &quot;l&quot;, xlab = expression(theta), ylab = &quot;log-likelihood&quot;) curve(quadprox.poi.repa(x, theta.est, data), col = &quot;darkgray&quot;, add = TRUE) abline(v = theta.est, lty = 3) ## probability-based interval ---------------------------------------------------- # cutoff&#39;s corresponding to a 95\\% confidence interval for the mean cut &lt;- log(.15) + lkl.poi(exp(theta.est), data) ; abline(h = cut, lty = 2) ic.lkl.poi &lt;- function(theta, cut, ...) { lambda &lt;- exp(theta) lkl.poi(lambda, ...) - cut } ic.95.prob &lt;- rootSolve::uniroot.all(ic.lkl.poi, range(theta.seq), data = data, cut = cut) arrows(x0 = ic.95.prob, y0 = rep(cut, 2), x1 = ic.95.prob, y1 = rep(-43, 2), lty = 2, length = .1) ## wald confidence interval ----------------------------------------------------- cut &lt;- log(.15) + quadprox.poi.repa(theta.est, theta.est, data) abline(h = cut, lty = 2, col = &quot;darkgray&quot;) se.theta.est &lt;- sqrt(1 / sum(data)) wald.95 &lt;- theta.est + qnorm(c(.025, .975)) * se.theta.est arrows(x0 = wald.95, y0 = rep(cut, 2), x1 = wald.95, y1 = rep(-43, 2), lty = 2, length = .1, col = &quot;darkgray&quot;) # a better look in the approximation around the MLE ------------------------------ theta.seq &lt;- seq(-.75, .75, length.out = 25) plot(theta.seq, lkl.poi(exp(theta.seq), data), type = &quot;l&quot;, xlab = expression(theta), ylab = &quot;log-likelihood&quot;) curve(quadprox.poi.repa(x, theta.est, data), col = &quot;darkgray&quot;, add = TRUE) abline(v = theta.est, lty = 3) Figure 3: log-likelihood function and MLE of \\(\\theta\\) in Poisson(\\(\\lambda = e^{\\theta}\\)) based on . In the topleft, a quadratic approximation in gray. In the topright, two intervals for \\(\\theta\\) - one based in the likelihood (in black) and one based in the quadratic approximation (in gray). In the bottom we provide a better look in the quadratic approximation. We obtained here two intervals for \\(\\theta\\). One based in a cut in the likelihood, \\(\\theta \\in (-0.778, 0.618)\\) - in black on the topright of Figure 3, and one based in a cut in the quadratic approximation of the likelihood, \\(\\theta \\in (-0.693, 0.693)\\) - in gray on the topright of Figure 3. With the parametrization \\(\\theta = \\log \\lambda\\) the two intervals are closer than the ones obtained for \\(\\lambda\\). i.e., for \\(\\theta\\) (with the use of the \\(\\log\\)) we get a more regular likelihood. More intervals Also obtain (by at least two different methods) confidence intervals for the parameter \\(\\lambda\\) from the likelihood function (approximate or not) of \\(\\theta\\). # likelihood --------------------------------------------------------------------- theta.seq &lt;- seq(-2, 2, length.out = 100) lklseq.poi &lt;- lkl.poi(exp(theta.seq), data) par(mar = c(4, 4, 2, 2) + .1) plot(theta.seq, lklseq.poi, type = &quot;l&quot;, xlab = expression(theta), ylab = &quot;log-likelihood&quot;) abline(v = lambda.est, lty = 3) # quadratic approximation for the reparametrization ------------------------------ curve(quadprox.poi.repa(x, theta.est, data), col = &quot;darkgray&quot;, add = TRUE) legend(-2.15, -25, c(&quot;log-like&quot;, &quot;Quadratic\\napprox.&quot;, &quot;MLE&quot;), col = c(1, &quot;darkgray&quot;, 1), lty = c(1, 1, 3), bty = &quot;n&quot;) # intervals for lambda ----------------------------------------------------------- ## probability-based interval ---------------------------------------------------- arrows(x0 = exp(ic.95.prob), y0 = c(-9, -36.5), x1 = exp(ic.95.prob), y1 = rep(-43, 2), lty = 2, length = .1) ## wald confidence interval ----------------------------------------------------- arrows(x0 = exp(wald.95), y0 = c(-9, -24), x1 = exp(wald.95), y1 = rep(-43, 2), lty = 2, length = .1, col = &quot;darkgray&quot;) Figure 4: log-likelihood function and quadratic approximation of \\(\\theta\\), and MLE of \\(\\lambda\\) in Poisson(\\(\\lambda = e^{\\theta}\\)) based on . In dashed, 95% confidence intervals for \\(\\lambda\\). Since the MLE has the invariance property, a very simple idea is: take the obtained interval for \\(\\theta\\) and apply a transformation, \\(\\lambda = e^{\\theta}\\). This simple idea is true for the interval obtained via likelihood function, as we can see in Figure 4. The obtained interval is the same that the one in letter \\(\\texttt{a)}\\). However, this idea doesn’t work for the interval based in the quadratic approximation. In the letter \\(\\texttt{a)}\\), via the quadratic approximation we got \\(\\lambda \\in (0.307, 1.693)\\). Here, applying the relation \\(\\lambda = e^{\\theta}\\) we get \\(\\lambda \\in (0.5, 2)\\). i.e., this shows that the invariance property applies to the likelihood, not to the quadratic approximation of the likelihood. "],
["hypergeometric-distribution.html", "Hypergeometric distribution Likelihood MLE and intervals Again but different", " Hypergeometric distribution In order to obtain an audience estimate of a game without using ticket sales data or stadium roulette records, special shirts were distributed to 300 supporters on condition that they were used during a game. During the game 250 fans were randomly selected and 12 of them had the shirt. Likelihood Obtain the likelihood function for the total number of supporters. We have here a random variable, let’s say, \\(X\\), representing the number of observed successes, \\(k\\). Here an observed success is select a fan using a special shirt. So, we have \\[ X \\sim \\text{Hypergeometric}(N, K = 300, n = 250), \\] with probability \\(p = K/N\\). \\(N\\) is the population size, that we want estimate. \\(K\\) is the unknown number of fans using a special shirt, and \\(n\\) is the number of, randomly, selected fans. \\[ \\begin{align*} \\text{Pr}[X = k] &amp;= \\frac{\\binom{K}{k} \\binom{N-K}{n-k}}{\\binom{N}{n}}\\\\ \\text{Pr}[X = 12] &amp;= \\frac{\\binom{300}{12} \\binom{N-300}{250-12}}{\\binom{N}{250}}\\\\ &amp;= \\frac{300!~250!}{288!~238!~12!} \\frac{(N-300)!~(N-250)!}{(N-538)!~N!}\\\\ &amp;= \\text{constant} \\times \\frac{(N-300)!~(N-250)!}{(N-538)!~N!}. \\end{align*} \\] Since we already have a \\(k\\), the likelihood is equal to \\(\\text{Pr}[X = k]\\). \\[ \\begin{align*} L(N) &amp;= \\text{Pr}[X = 12] = \\text{constant} \\frac{(N-300)!~(N-250)!}{(N-538)!~N!}\\\\ l(N) = \\log L(N) &amp;= \\log~\\text{constant} + \\log~\\frac{(N-300)!~(N-250)!}{(N-538)!~N!}\\\\ &amp;\\approx \\log~\\frac{(N-300)!~(N-250)!}{(N-538)!~N!}. \\end{align*} \\] A plot of the likelihood is provided in Figure 5. # likelihood --------------------------------------------------------------------- lkl.hypergeo &lt;- function(N, N.init, n, K, k) { size &lt;- N - N.init n1 &lt;- n2 &lt;- n3 &lt;- n4 &lt;- numeric(size) N &lt;- N.init for (i in 1:size) { n1[i] = log(N - K) n2[i] = log(N - n) n3[i] = log(N - K - n + k) n4[i] = log(N) N = N + 1 } lkl &lt;- sum(n1) + sum(n2) - sum(n3) - sum(n4) return(lkl) } compute.lkl &lt;- function(grid, N.init, n, K, k) { size.grid = length(grid) lkl.grid = numeric(size.grid) i = 1 for (j in grid) { lkl.grid[i] = lkl.hypergeo(N = j, N.init = N.init, n = n, K = K, k = k) i = i + 1 } return(lkl.grid) } # plotting par(mar = c(4, 4, 2, 2) + .1) layout(matrix(c(0, 1, 1, 0, 2, 2, 3, 3), nrow = 2, byrow = TRUE)) # plot window definition n.grid &lt;- seq(3000, 17000, by = 20) plot(n.grid, compute.lkl(n.grid, N.init = 539, n = 250, K = 300, k = 12), type = &quot;l&quot;, xlab = &quot;N&quot;, ylab = &quot;log-likelihood&quot;) abline(v = 6250, lty = 2) n.grid &lt;- seq(539, 1e4, by = 20) plot(n.grid, compute.lkl(n.grid, N.init = 539, n = 250, K = 300, k = 12), type = &quot;l&quot;, xlab = &quot;N&quot;, ylab = &quot;log-likelihood&quot;) abline(v = 6250, lty = 2) n.grid &lt;- seq(5000, 8000, by = 10) plot(n.grid, compute.lkl(n.grid, N.init = 539, n = 250, K = 300, k = 12), type = &quot;l&quot;, xlab = &quot;N&quot;, ylab = &quot;log-likelihood&quot;) abline(v = 6250, lty = 2) Figure 5: log-likelihood of \\(N\\) in Hypergeometric(N, K = 300, n = 250) based on \\(k = 12\\). In dashed, the MLE. Different ranges of \\(N\\) to have a better understanding of the likelihood behaviour. The MLE here is given by \\(\\hat{N} = \\left\\lfloor Kn/k \\right\\rfloor = 6250\\). Figure 5 present some very interesting behaviours. In the top, going from \\(N = 3000\\) to \\(N = 17000\\) we can see clearly the likelihood asymmetry. In the bottomleft, starting from the first valid possible value, 539, we see basically nothing. i.e., we don’t see any possible curvature in the likelihood. In the bottomright we focus around the MLE to see better the curvature. Around that point we see a very small variation in the likelihood, \\(0.3\\), for a range of \\(3000\\) in \\(N\\). This shows how smooth is the curvature around the MLE in a large region. MLE and intervals Get the point estimate and the interval estimate, the latter by at least two different methods. # point estimate ----------------------------------------------------------------- optimize(lkl.hypergeo, N.init = 539, n = 250, K = 300, k = 12, lower = 539, upper = 2e4, maximum = TRUE) $maximum [1] 6251.045 $objective [1] 317.8437 Using the routine to do the optimization we obtain \\(\\hat{N} = 6251\\), and a log-likelihood of 317.85. This value for \\(\\hat{N}\\) is practically equal to what the ML theory says, \\(\\hat{N} = \\left\\lfloor Kn/k \\right\\rfloor = 6250\\). Next, we compute a quadratic approximation for the hypergeometric (log-)likelihood followed by two intervals, one based in the likelihood, and a wald interval based in the quadratic approximation. All this is presented in Figure 6. # quadratic approximation of the likelihood -------------------------------------- hess &lt;- numDeriv:::hessian(lkl.hypergeo, x = 6250, N.init = 539, n = 250, K = 300, k = 12) # quadratic approx. quadprox.hypergeo &lt;- function(N, N.est = 6250, N.init, n, K, k) { obs.info &lt;- as.vector(-hess) # observed information lkl &lt;- lkl.hypergeo(N.est, N.init, n, K, k) lkl - .5 * obs.info * (N - N.est)**2 } par(mar = c(4, 4, 2, 2) + .1) layout(matrix(c(1, 2, 3, 3), nrow = 2, byrow = TRUE), heights = c(1.5, 1)) # plot window definition # likelihood plot n.grid &lt;- seq(3000, 17000, by = 20) plot(n.grid, compute.lkl(n.grid, N.init = 539, n = 250, K = 300, k = 12), type = &quot;l&quot;, xlab = &quot;N&quot;, ylab = &quot;log-likelihood&quot;) abline(v = 6250, lty = 3) # quadratic approximation plot curve(quadprox.hypergeo(x, N.init = 539, n = 250, K = 300, k = 12), col = &quot;darkgray&quot;, add = TRUE) legend(10500, 318.25, c(&quot;log-like&quot;, &quot;Quadratic\\napprox.&quot;, &quot;MLE&quot;), col = c(1, &quot;darkgray&quot;, 1), lty = c(1, 1, 3), bty = &quot;n&quot;) # intervals for N ---------------------------------------------------------------- # first, we plot again the likelihood and the quadratic approx. n.grid &lt;- seq(2500, 12000, by = 20) plot(n.grid, compute.lkl(n.grid, N.init = 539, n = 250, K = 300, k = 12), type = &quot;l&quot;, xlab = &quot;N&quot;, ylab = &quot;log-likelihood&quot;) abline(v = 6250, lty = 3) curve(quadprox.hypergeo(x, N.init = 539, n = 250, K = 300, k = 12), col = &quot;darkgray&quot;, add = TRUE) ## probability-based interval ---------------------------------------------------- # cutoff&#39;s corresponding to a 95\\% confidence interval for N cut &lt;- log(.15) + lkl.hypergeo(6250, N.init = 539, n = 250, K = 300, k = 12) abline(h = cut, lty = 2) ic.lkl.hypergeo &lt;- function(grid, N.init, n = 250, K, k) { compute.lkl(grid, N.init, n, K, k) - cut } ic.95.prob &lt;- rootSolve::uniroot.all(ic.lkl.hypergeo, range(n.grid), N.init = 539, K = 300, k = 12) arrows(x0 = ic.95.prob, y0 = rep(cut, 2), x1 = ic.95.prob, y1 = rep(309.5, 2), lty = 2, length = .1) ## wald confidence interval ------------------------------------------------------ cut &lt;- log(.15) + quadprox.hypergeo(6250, N.init = 539, n = 250, K = 300, k = 12) abline(h = cut, lty = 2, col = &quot;darkgray&quot;) se.n.est &lt;- sqrt(as.vector(-1 / hess)) wald.95 &lt;- 6250 + qnorm(c(.025, .975)) * se.n.est arrows(x0 = wald.95, y0 = rep(cut, 2), x1 = wald.95, y1 = rep(309.5, 2), lty = 2, length = .1, col = &quot;darkgray&quot;) # a better look in the approximation around the MLE ------------------------------ n.grid &lt;- seq(5000, 8000, by = 10) plot(n.grid, compute.lkl(n.grid, N.init = 539, n = 250, K = 300, k = 12), type = &quot;l&quot;, xlab = &quot;N&quot;, ylab = &quot;log-likelihood&quot;) abline(v = 6250, lty = 3) curve(quadprox.hypergeo(x, N.init = 539, n = 250, K = 300, k = 12), col = &quot;darkgray&quot;, add = TRUE) Figure 6: log-likelihood function and MLE of \\(N\\) in Hypergeometric(N, K = 300, n = 250) based on \\(k = 12\\). In the topleft, a quadratic approximation in gray. In the topright, two intervals for \\(N\\) - one based in the likelihood (in black) and one based in the quadratic approximation (in gray). In the bottom we provide a better look in the quadratic approximation. The quadratic approximation and the intervals were obtained in the same way that in the exercises before, with the only difference that here the hessian was obtained numerically, just for simplicity. Based in a cut in the likelihood, we obtain \\((3850, 11387)\\) as a 95% interval for the total number of fans in the game. With the wald interval (quadratic approximation) we obtain \\((2892, 9608)\\) as a 95% interval. A considerable difference. Again but different Repeat and compare the results if there were 500 shirts and 20 with shirts out of the 250. The procedure here is exactly the same. We have \\[ \\begin{align*} \\text{Pr}[X = k] = \\frac{\\binom{K}{k} \\binom{N-K}{n-k}}{\\binom{N}{n}}, \\qquad \\text{Pr}[X = 20] &amp;= \\frac{\\binom{500}{20} \\binom{N-500}{250-20}}{\\binom{N}{250}}\\\\ &amp;= \\frac{500!~250!}{480!~230!~20!} \\frac{(N-500)!~(N-250)!}{(N-730)!~N!}\\\\ &amp;= \\text{constant} \\times \\frac{(N-500)!~(N-250)!}{(N-730)!~N!}. \\end{align*} \\] With likelihood \\[ \\begin{align*} L(N) &amp;= \\text{Pr}[X = 20] = \\text{constant} \\frac{(N-500)!~(N-250)!}{(N-730)!~N!}\\\\ l(N) = \\log L(N) &amp;= \\log~\\text{constant} + \\log~\\frac{(N-500)!~(N-250)!}{(N-730)!~N!}\\\\ &amp;\\approx \\log~\\frac{(N-500)!~(N-250)!}{(N-730)!~N!}. \\end{align*} \\] In Figure 7 we provide the likelihood graphs and intervals for \\(N\\). The MLE is exactly the same that the one in the previous scenario, since \\[ \\hat{N} = \\left\\lfloor Kn/k \\right\\rfloor = \\left\\lfloor 300 \\times 250~/~12 \\right\\rfloor = \\left\\lfloor 500 \\times 250~/~20 \\right\\rfloor = 6250. \\] Based in a cut in the likelihood, we obtain \\((3850, 11387)\\) as a 95% interval for the total number of fans in the game. With the wald interval (quadratic approximation) we obtain \\((2892, 9608)\\) as a 95% interval for the total number of fans. A considerable difference. # plotting ----------------------------------------------------------------------- par(mar = c(4, 4, 2, 2) + .1) layout(matrix(c(1, 2, 3, 3), nrow = 2, byrow = TRUE), heights = c(1.5, 1)) # plot window definition ## likelihood -------------------------------------------------------------------- n.grid &lt;- seq(3000, 17000, by = 20) plot(n.grid, compute.lkl(n.grid, N.init = 731, n = 250, K = 500, k = 20), type = &quot;l&quot;, xlab = &quot;N&quot;, ylab = &quot;log-likelihood&quot;) abline(v = 6250, lty = 3) ## quadratic approximation ------------------------------------------------------- hess &lt;- numDeriv:::hessian(lkl.hypergeo, x = 6250, N.init = 731, n = 250, K = 500, k = 20) curve(quadprox.hypergeo(x, N.init = 731, n = 250, K = 500, k = 20), col = &quot;darkgray&quot;, add = TRUE) legend(10500, 382.5, c(&quot;log-like&quot;, &quot;Quadratic\\napprox.&quot;, &quot;MLE&quot;), col = c(1, &quot;darkgray&quot;, 1), lty = c(1, 1, 3), bty = &quot;n&quot;) # intervals for N ---------------------------------------------------------------- # first, we plot again the likelihood and the quadratic approx. n.grid &lt;- seq(2750, 11000, by = 20) plot(n.grid, compute.lkl(n.grid, N.init = 731, n = 250, K = 500, k = 20), type = &quot;l&quot;, xlab = &quot;N&quot;, ylab = &quot;log-likelihood&quot;) abline(v = 6250, lty = 3) curve(quadprox.hypergeo(N = x, N.init = 731, n = 250, K = 500, k = 20), col = &quot;darkgray&quot;, add = TRUE) ## probability-based interval ---------------------------------------------------- # cutoff&#39;s corresponding to a 95\\% confidence interval for N cut &lt;- log(.15) + lkl.hypergeo(6250, N.init = 731, n = 250, K = 500, k = 20) abline(h = cut, lty = 2) ic.95.prob &lt;- rootSolve::uniroot.all(ic.lkl.hypergeo, range(n.grid), N.init = 731, K = 500, k = 20) arrows(x0 = ic.95.prob, y0 = rep(cut, 2), x1 = ic.95.prob, y1 = rep(370.25, 2), lty = 2, length = .1) ## wald confidence interval ------------------------------------------------------ cut &lt;- log(.15) + quadprox.hypergeo(6250, N.init = 731, n = 250, K = 500, k = 20) abline(h = cut, lty = 2, col = &quot;darkgray&quot;) se.n.est &lt;- sqrt(as.vector(-1 / hess)) wald.95 &lt;- 6250 + qnorm(c(.025, .975)) * se.n.est arrows(x0 = wald.95, y0 = rep(cut, 2), x1 = wald.95, y1 = rep(370.25, 2), lty = 2, length = .1, col = &quot;darkgray&quot;) # a better look in the approximation around the MLE ------------------------------ n.grid &lt;- seq(5000, 7500, by = 10) plot(n.grid, compute.lkl(n.grid, N.init = 731, n = 250, K = 500, k = 20), type = &quot;l&quot;, xlab = &quot;N&quot;, ylab = &quot;log-likelihood&quot;) abline(v = 6250, lty = 3) curve(quadprox.hypergeo(x, N.init = 731, n = 250, K = 500, k = 20), col = &quot;darkgray&quot;, add = TRUE) Figure 7: log-likelihood function and MLE of \\(N\\) in Hypergeometric(N, K = 500, n = 250) based on \\(k = 20\\). In the topleft, a quadratic approximation in gray. In the topright, two intervals for \\(N\\) - one based in the likelihood (in black) and one based in the quadratic approximation (in gray). In the bottom we provide a better look in the quadratic approximation. Comparing with the previous scenario the intervals changed considerably, even with MLE retain the same. The likelihood shape is the same that before, just with a vertical increase. Before the maximum log-likelihood estimate was around 318, now is around 382. "],
["gaussian-distribution-with-interval-data.html", "Gaussian distribution with interval data Likelihood MLE Profile likelihood Standard error", " Gaussian distribution with interval data The following independent observations were taken from one r.v. \\(X \\sim N(\\mu, \\sigma^{2})\\). 56.4, 54.2, 65.3, 49.2, 50.1, 56.9, 58.9, 62.5, 70, 61 another 5 observations are known to be less than 50. it is known that 3 other observations are greater than 65. Likelihood Write the likelihood function. The likelihood can be expressed as \\[ \\begin{align*} L(\\theta) &amp;= \\left( \\prod_{i = 1}^{10} f(x_{i}) \\right) \\times (F(50))^{5} \\times (1 - F(65))^{3}\\\\ &amp;= \\left( \\prod_{i = 1}^{10} \\phi \\left( \\frac{y_{i} - \\mu}{\\sigma} \\right) \\right) \\times \\left( \\Phi \\left( \\frac{50 - \\mu}{\\sigma} \\right) \\right)^{5} \\times \\left( 1 - \\Phi \\left( \\frac{65 - \\mu}{\\sigma} \\right) \\right)^{3}\\\\ l(\\theta) &amp;= \\sum_{i = 1}^{10} \\log \\phi \\left( \\frac{y_{i} - \\mu}{\\sigma} \\right) + 5~\\log \\Phi \\left( \\frac{50 - \\mu}{\\sigma} \\right) + 3~\\log \\left( 1 - \\Phi \\left( \\frac{65 - \\mu}{\\sigma} \\right) \\right). \\end{align*} \\] # likelihood --------------------------------------------------------------------- ## if logsigma = TRUE, we do the parametrization using log(sigma) lkl.norm &lt;- function(par, xs, less.than, more.than, logsigma = FALSE) { if (logsigma) par[2] &lt;- exp(par[2]) l1 &lt;- sum(dnorm(xs, mean = par[1], sd = par[2], log = TRUE)) l2 &lt;- 5 * log(pnorm(less.than, mean = par[1], sd = par[2])) l3 &lt;- 3 * log(1 - pnorm(more.than, mean = par[1], sd = par[2])) lkl &lt;- l1 + l2 + l3 # we return the negative of the log-likelihood, # since the optim routine performs a minimization return(-lkl) } MLE Obtain estimates of maximum likelihood. # providing an initial guess based uniquely in the ten points sample init.guess &lt;- c(mean(data), sd(data)) # parameters estimation (par.est &lt;- optim(init.guess, lkl.norm, logsigma = FALSE, xs = data, less.than = 65, more.than = 50)$par) [1] 58.10630 5.66888 \\[ \\hat{\\mu} = 58.106, \\quad \\hat{\\sigma} = 5.669. \\] FYI: In the ten points sample we have \\(\\bar{x} = 58.45\\) and \\(s^{2} = 6.527\\). Profile likelihood Obtain profiles of likelihood. First, we do the graph of the likelihood surface in the deviance scale, since this scale is much more convenient. This requires the value of the likelihood evaluated in the parameter estimates. The deviance here is basically \\[ D(\\theta) = -2 (l(\\theta) - l(\\hat{\\theta})), \\] with \\(\\theta\\) being the vector of parameters \\(\\mu\\) and \\(\\sigma\\). In Figure 8 we have the likelihood surfaces for two parametrizations: \\(\\sigma\\) and \\(\\log \\sigma\\). # deviance ----------------------------------------------------------------------- dev.norm &lt;- function(theta, est, ...) { # remember: here we have the negative of the log-likelihoods, # so the sign the different return(2 * (lkl.norm(theta, ...) - lkl.norm(est, ...))) } dev.norm.surf &lt;- Vectorize(function(x, y, ...) dev.norm(c(x, y), ...)) par(mar = c(4, 4, 2, 2) + .1) par(mfrow = c(1, 2)) # plot window denifition, one row and two columns ## parametrization: sigma -------------------------------------------------------- mu.grid &lt;- seq(52, 63.75, length.out = 100) sigma.grid &lt;- seq(3.25, 12.5, length.out = 100) outer.grid &lt;- outer(mu.grid, sigma.grid, FUN = dev.norm.surf, est = par.est, xs = data, less.than = 65, more.than = 50) contour.levels &lt;- c(.99, .95, .9, .7, .5, .3, .1, .05) contour(mu.grid, sigma.grid, outer.grid, levels = qchisq(contour.levels, df = 2), labels = contour.levels, xlab = expression(mu), ylab = expression(sigma)) # converting par.est to a matricial object points(t(par.est), col = &quot;darkgray&quot;, pch = 19, cex = .75) ## parametrization: log sigma ---------------------------------------------------- par.est &lt;- optim(init.guess, lkl.norm, logsigma = TRUE, xs = data, less.than = 65, more.than = 50)$par outer.grid &lt;- outer(mu.grid, log(sigma.grid), FUN = dev.norm.surf, est = par.est, xs = data, logsigma = TRUE, less.than = 65, more.than = 50) contour.levels &lt;- c(.99, .95, .9, .7, .5, .3, .1, .05) contour(mu.grid, log(sigma.grid), outer.grid, levels = qchisq(contour.levels, df = 2), labels = contour.levels, xlab = expression(mu), ylab = expression(log(sigma))) points(t(par.est), col = &quot;darkgray&quot;, pch = 19, cex = .75) Figure 8: Deviances of (\\(\\mu, \\sigma\\)) and (\\(\\mu, \\log \\sigma\\)) for a Gaussian sample made of interval data. From Figure 8 two things are important to mention. First, with interval data, the surface doesn’t exhibit orthogonally between the parameters. Second, in the log parametrization of sigma we have better behavior in the surface. Now, we obtain the likelihood profiles for each parameter. The profiles are presented in Figure 9. To obtain the likelihood profile for \\(\\mu\\), let’s say, we need to create a grid of values of \\(\\mu\\) and for each value of this grid find the value, let’s say, \\(\\hat{\\sigma}_{\\mu}\\) that maximizes the profile likelihood. We do this below for \\(\\mu\\) and \\(\\sigma\\), the results can be seen in Figure 9. # profiles ----------------------------------------------------------------------- # optimizing one parameter per time (optimize routine: single par. optimization) # we need to split the par argument in two lkl.norm.profiles &lt;- function(mu, sigma, xs, less.than, more.than) { l1 &lt;- sum(dnorm(xs, mean = mu, sd = sigma, log = TRUE)) l2 &lt;- 5 * log(pnorm(less.than, mean = mu, sd = sigma)) l3 &lt;- 3 * log(1 - pnorm(more.than, mean = mu, sd = sigma)) lkl &lt;- l1 + l2 + l3 return(lkl)} par(mar = c(4, 4, 2, 2) + .1, mfrow = c(1, 2)) # plot window definition ## mu ---------------------------------------------------------------------------- mu.grid &lt;- seq(45, 69.25, length.out = 100) # with the optimize routine we obtain two numbers: # the optimal sigma for the given value of mu.grid, and the log-likelihood value mu.perf &lt;- matrix(0, nrow = length(mu.grid), ncol = 2) for (i in 1:length(mu.grid)) { mu.perf[i, ] &lt;- unlist( optimize(lkl.norm.profiles, lower = 0, upper = 50, mu = mu.grid[i], xs = data, less.than = 65, more.than = 50, maximum = TRUE))} plot(mu.grid, mu.perf[ , 2], type = &quot;l&quot;, xlab = expression(mu), ylab = expression(l(mu[sigma]))) abline(v = par.est[1], lty = 2) ## sigma ------------------------------------------------------------------------- sigma.grid &lt;- seq(3.25, 13.75, length.out = 100) sigma.perf &lt;- matrix(0, nrow = length(sigma.grid), ncol = 2) for (i in 1:length(sigma.grid)) { sigma.perf[i, ] &lt;- unlist( optimize(lkl.norm.profiles, lower = 0, upper = 100, sigma = sigma.grid[i], xs = data, less.than = 65, more.than = 50, maximum = TRUE))} plot(sigma.grid, sigma.perf[ , 2], type = &quot;l&quot;, xlab = expression(sigma), ylab = expression(l(sigma[mu]))) # we apply the exponential, since we did log sigma optimization by last abline(v = exp(par.est[2]), lty = 2) Figure 9: Profile log-likelihoods for \\(\\mu\\) and \\(\\sigma\\) for a Gaussian sample made of interval data. In dashed, the respective MLE’s. Standard error Obtain the standard error estimates. With the \\(\\texttt{optim}\\) routine we obtain the hessian. init.guess &lt;- c(mean(data), sd(data)) # initial guess # parameters estimation, now also computing the hessian (hess &lt;- optim(init.guess, lkl.norm, xs = data, less.than = 65, more.than = 50, hessian = TRUE)$hessian) [,1] [,2] [1,] 0.38190632 0.04403561 [2,] 0.04403561 0.80092515 And we know that the hessian, \\(H(\\theta)\\), is the negative of the observed information, \\(I_{O}(\\theta)\\), and that asymptotically the variance of \\(\\hat{\\theta}\\) is \\(I_{O}(\\theta)^{-1}\\). Thus, (se &lt;- diag(sqrt(1 / hess))) # standard error [1] 1.618160 1.117388 The standard errors, se, are \\[ \\text{se}(\\hat{\\mu}) = 1.618, \\quad \\text{se}(\\hat{\\sigma}) = 1.117. \\] "],
["gaussian-regression.html", "Gaussian regression Likelihood MLE Joint likelihood Profile likelihood", " Gaussian regression Consider the following data table (adapted/modified from Montgomery &amp; Runger, 1994) to which we wish to fit a simple linear regression model by relating the response variable \\(Y\\) (purity in %) to an explanatory variable \\(X\\) (hydrocarbon level). Likelihood Find the likelihood function. data &lt;- data.frame( x = c(.99, 1.02, 1.15, 1.29, 1.46, 1.36, .87, 1.23, 1.55, 1.4, 1.19, 1.15, .98, 1.01, 1.11, 1.2, 1.26, 1.32, 1.43, .95), y = c(99.01, 89.05, 91.43, 93.74, 96.73, 94.45, 87.59, 91.77, 99.42, 93.65, 93.54, 92.52, 90.56, 89.54, 89.85, 90.39, 93.25, 93.41, 94.98, 87.33)) par(mar = c(4, 4, 2, 2) + .1) plot(y ~ x, data, xlab = &quot;hydrocarbon level&quot;, ylab = expression(&quot;purity&quot;~(&quot;%&quot;))) Figure 10: Dispersion between hydrocarbon level \\(\\times\\) purity (%). Assuming a Normal distribution for \\(Y | X\\), we have the following log-likelihood, in matricial form \\[ \\begin{align*} l(\\theta; Y) &amp;= - \\frac{n}{2} \\log 2\\pi - n \\log \\sigma - \\frac{1}{2\\sigma^{2}} \\left \\| Y - X \\beta \\right \\|^{2}\\\\ &amp;= \\text{constant} - n \\log \\sigma - \\frac{1}{2\\sigma^{2}} \\left \\| Y - X \\beta \\right \\|^{2}, \\end{align*} \\] were \\(\\theta = [\\beta_{0}~\\beta_{1}~\\sigma]^{\\top}\\), \\(\\beta = [\\beta_{0}~\\beta_{1}]^{\\top}\\), and \\(\\mu_{i} = \\beta_{0} + \\beta_{1} x_{i}\\). MLE Find the estimates of maximum likelihood. # likelihood --------------------------------------------------------------------- lkl.model.norm &lt;- function(b0, b1, sigma, y, x, logsigma = FALSE) { ## if logsigma = TRUE, we do the parametrization using log(sigma) if (logsigma) sigma &lt;- exp(sigma) mu &lt;- b0 + b1 * x lkl &lt;- sum(dnorm(y, mean = mu, sd = sigma, log = TRUE)) # we return the negative of the log-likelihood, # since the optim routine performs a minimization return(-lkl) } library(bbmle) par.est &lt;- mle2(lkl.model.norm, start = list(b0 = min(data$y), b1 = 1.5, sigma = sd(data$y)), data = list(y = data$y, x = data$x), method = &quot;BFGS&quot;) par.est@coef b0 b1 sigma 77.988429 12.225489 2.343799 Just checking… lm(y ~ x, data)$coef # beta_0 and beta_1 (Intercept) x 77.98996 12.22453 par.est@coef[[3]]**2 # sigma^2 [1] 5.493392 (summary(lm(y ~ x, data))$sigma**2) * (nrow(data) - 2) / nrow(data) [1] 5.493386 Checked, \\(\\checkmark\\). Joint likelihood Obtain the joint likelihood for the parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\): With \\(\\sigma\\) fixed in its MLE considering \\(\\sigma\\) fixed with value equal to its estimate. # deviance ----------------------------------------------------------------------- dev.model.norm &lt;- function(beta, theta.est, ...) { b0 &lt;- beta[1] ; b1 &lt;- beta[2] b0.est &lt;- theta.est[[1]] ; b1.est &lt;- theta.est[[2]] sigma.est &lt;- theta.est[[3]] lkl.grid &lt;- lkl.model.norm(b0, b1, sigma.est, ...) lkl.est &lt;- lkl.model.norm(b0.est, b1.est, sigma.est, ...) lkl.diff &lt;- lkl.grid - lkl.est # remember: here we have the negative of the log-likelihoods, # so the sign is different return(2 * lkl.diff) } # deviance, vectorized version --------------------------------------------------- dev.model.norm.surf &lt;- Vectorize(function(a, b, ...) dev.model.norm(c(a, b), ...)) # deviance contour, plotting ----------------------------------------------------- b0.grid &lt;- seq(67.5, 88.5, length.out = 100) b1.grid &lt;- seq(3.5, 21, length.out = 100) outer.grid &lt;- outer(b0.grid, b1.grid, FUN = dev.model.norm.surf, theta.est = par.est@coef, y = data$y, x = data$x) par(mar = c(4, 4, 2, 2) + .1) contour.levels &lt;- c(.99, .95, .9, .7, .5, .3, .1, .05) contour(b0.grid, b1.grid, outer.grid, levels = qchisq(contour.levels, df = 2), labels = contour.levels, xlab = expression(beta[0]), ylab = expression(beta[1])) # converting par.est@coef to a matricial object points(t(par.est@coef[1:2]), col = 1, pch = 19, cex = .75) Figure 11: Deviance of (\\(\\beta_{0}, \\beta_{1}\\)) with \\(\\sigma\\) fixed in it’s estimative, for a Gaussian regression. Profile likelihood wrt \\(\\sigma\\) obtaining the profile likelihood (joint - 2D) with respect to \\(\\sigma\\). # profile ------------------------------------------------------------------------ sigma.grid &lt;- seq(1.7, 3.5, length.out = 100) # with the optim routine we obtain three numbers: # the estimates of beta_0, beta_1, and the log-likelihood value beta.perf &lt;- matrix(0, nrow = length(sigma.grid), ncol = 3) for (i in 1:length(sigma.grid)) { beta.perf[i, ] &lt;- unlist( mle2(lkl.model.norm, start = list(b0 = min(data$y), b1 = 1.5), data = list(y = data$y, x = data$x, sigma = sigma.grid[i]), method = &quot;BFGS&quot; )@details[1:2]) } par(mar = c(4, 4, 2, 2) + .1) plot(sigma.grid, -1 * beta.perf[ , 3], type = &quot;l&quot;, xlab = expression(sigma), ylab = &quot;log-likelihood&quot;) abline(v = par.est@coef[[3]], lty = 2) Figure 12: Profile log-likelihood for \\(\\sigma\\), for a Gaussian regression. In dashed, the MLE. And how the \\(\\beta\\)’s fluctuates across the grid of \\(\\sigma\\)? summary(beta.perf[ , 1]) # beta_0 Min. 1st Qu. Median Mean 3rd Qu. Max. 77.98 77.99 77.99 77.99 78.00 78.01 summary(beta.perf[ , 2]) # beta_1 Min. 1st Qu. Median Mean 3rd Qu. Max. 12.21 12.22 12.22 12.22 12.22 12.23 Very few, as expected. Since the estimation of \\(\\beta\\) doesn’t depend on \\(\\sigma\\). Profile likelihood Obtain the profile likelihood for the parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\) individually. par(mfrow = c(1, 2)) # plot window definition, one row and two columns # profiling ---------------------------------------------------------------------- coef.profile &lt;- function(grid, coef = c(&quot;b0&quot;, &quot;b1&quot;)) { n &lt;- length(grid) prof &lt;- numeric(n) for (i in 1:n) { prof[i] &lt;- switch( coef, &quot;b0&quot; = mle2(lkl.model.norm, start = list(b1 = 1.5), data = list(y = data$y, x = data$x, b0 = grid[i], sigma = par.est@coef[[3]]), method = &quot;BFGS&quot; )@details$value, &quot;b1&quot; = mle2(lkl.model.norm, start = list(b0 = min(data$y)), data = list(y = data$y, x = data$x, b1 = grid[i], sigma = par.est@coef[[3]]), method = &quot;BFGS&quot; )@details$value) } return((-1) * prof) } par(mar = c(4, 4, 2, 2) + .1) # beta_0 ------------------------------------------------------------------------- b0.grid &lt;- seq(67.5, 88.5, length.out = 100) plot(b0.grid, coef.profile(b0.grid, &quot;b0&quot;), type = &quot;l&quot;, xlab = expression(beta[0]), ylab = &quot;log-likelihood&quot;) abline(v = par.est@coef[[1]], lty = 2) # beta_1 ------------------------------------------------------------------------- b1.grid &lt;- seq(3.5, 21, length.out = 100) plot(b1.grid, coef.profile(b1.grid, &quot;b1&quot;), type = &quot;l&quot;, xlab = expression(beta[1]), ylab = &quot;log-likelihood&quot;) abline(v = par.est@coef[[2]], lty = 2) Figure 13: Profile log-likelihoods for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) for a Gaussian regression. In dashed, MLE’s. "],
["poisson-regression.html", "Poisson regression Likelihood Profile likelihood MLE Intervals Did we obtained the right values?", " Poisson regression Consider a sample of a r.v. \\(Y\\) where it is assumed that \\(Y_{i} \\sim P(\\lambda_{i})\\) where the parameter \\(\\lambda_{i}\\) is described by a function of an explanatory variable \\(\\log (\\lambda_{i}) = \\beta_{0} + \\beta_{1} x_{i}\\) with values known from \\(x_{i}.\\) Simulated data with (\\(\\beta_{0} = 2, \\beta_{1} = 0.5\\)) are shown below. data &lt;- data.frame(y = c(10, 15, 11, 37, 70, 19, 12, 12, 13, 88), x = c(1.7, 1.5, .5, 2.8, 4.4, 1.8, .4, .7, 1.3, 5.1)) par(mar = c(4, 4, 2, 2) + .1) plot(y ~ x, data) Figure 14: Dispersion between the variables and . Likelihood Obtain the graph of the likelihood function indicating the position of the true values of the parameters. The log-likelihood is given by \\[ l(\\lambda) = -n \\lambda + \\log \\lambda \\sum_{i=1}^{n} y_{i} - \\sum_{i=1}^{n} \\log y_{i}!~, \\quad \\lambda_{i} = \\exp \\{\\beta_{0} + \\beta_{1} x_{i}\\}. \\] # likelihood --------------------------------------------------------------------- lkl.model.poi &lt;- function(b0, b1, data) { n &lt;- nrow(data) lambda &lt;- exp(b0 + b1 * data$x) lkl &lt;- sum(dpois(data$y, lambda = lambda, log = TRUE)) # we return the negative of the log-likelihood, # since the optim routine performs a minimization return(-lkl) } # deviance ----------------------------------------------------------------------- dev.model.poi &lt;- function(beta, beta.est, ...) { b0 &lt;- beta[1] ; b1 &lt;- beta[2] b0.est &lt;- beta.est[1] ; b1.est &lt;- beta.est[2] lkl.grid &lt;- lkl.model.poi(b0, b1, ...) lkl.est &lt;- lkl.model.poi(b0.est, b1.est, ...) lkl.diff &lt;- lkl.grid - lkl.est # remember: here we have the negative of the log-likelihoods, # so the sign the different return(2 * lkl.diff) } # deviance, vectorized version --------------------------------------------------- dev.model.poi.surf &lt;- Vectorize( function(a, b, ...) dev.model.poi(c(a, b), ...) ) # deviance contour, plotting ----------------------------------------------------- b0.grid &lt;- seq(1.625, 2.475, length.out = 100) b1.grid &lt;- seq(.375, .6, length.out = 100) outer.grid &lt;- outer(b0.grid, b1.grid, FUN = dev.model.poi.surf, beta.est = c(2, .5), data = data) contour.levels &lt;- c(.99, .95, .9, .7, .5, .3, .1, .05) par(mar = c(4, 4, 2, 2) + .1) contour(b0.grid, b1.grid, outer.grid, levels = qchisq(contour.levels, df = 2), labels = contour.levels, xlab = expression(beta[0]), ylab = expression(beta[1])) # true parameter values points(c(2, .5), pch = 19, cex = .75) Figure 15: Deviance of (\\(\\beta_{0}, \\beta_{1}\\)) for a Poisson regression based on . Profile likelihood Obtain the likelihood profiles of the parameters. par(mar = c(4, 4, 2, 2) + .1, mfrow = c(1, 2)) # plot window definition # profiling ---------------------------------------------------------------------- coef.profile &lt;- function(grid, coef = c(&quot;b0&quot;, &quot;b1&quot;)) { n &lt;- length(grid) ; prof &lt;- numeric(n) for (i in 1:n) { # for some points can happen that the hessian not be invertible tryCatch({ # with tryCatch we can skip these points prof[i] &lt;- switch( coef, &quot;b0&quot; = mle2(lkl.model.poi, start = list(b1 = .5), data = list(data = data, b0 = grid[i]), method = &quot;BFGS&quot;)@details$value, &quot;b1&quot; = mle2(lkl.model.poi, start = list(b0 = min(data$y)), data = list(data = data, b1 = grid[i]), method = &quot;BFGS&quot;)@details$value)}, warning = function(w) w)} return((-1) * prof)} # beta_0 ------------------------------------------------------------------------- b0.grid &lt;- seq(1.25, 2.75, length.out = 100) plot(b0.grid, coef.profile(b0.grid, &quot;b0&quot;), type = &quot;l&quot;, xlab = expression(beta[0]), ylab = &quot;log-likelihood&quot;) ; abline(v = 2, lty = 2) # beta_1 ------------------------------------------------------------------------- b1.grid &lt;- seq(.3, .675, length.out = 100) plot(b1.grid, coef.profile(b1.grid, &quot;b1&quot;), type = &quot;l&quot;, xlab = expression(beta[1]), ylab = &quot;log-likelihood&quot;) ; abline(v = .5, lty = 2) Figure 16: Profile log-likelihoods for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) for a Poisson regression. In dashed, MLE’s. MLE Get estimates. (par.est &lt;- mle2(lkl.model.poi, start = list(b0 = min(data$y), b1 = 0), data = list(data = data), method = &quot;BFGS&quot;)) Call: mle2(minuslogl = lkl.model.poi, start = list(b0 = min(data$y), b1 = 0), method = &quot;BFGS&quot;, data = list(data = data)) Coefficients: b0 b1 2.0675572 0.4816563 Log-likelihood: -27.86 Intervals Get intervals (use different methods). par(mar = c(4, 4, 2, 2) + .1, mfrow = c(2, 1)) # plot window definition library(rootSolve) # uniroot.all function library(numDeriv) # hessian function # beta_0 ------------------------------------------------------------------------- ## first, plotting the profile with point estimates ------------------------------ plot(b0.grid, coef.profile(b0.grid, &quot;b0&quot;), type = &quot;l&quot;, xlab = expression(beta[0]), ylab = &quot;log-likelihood&quot;) abline(v = 2, lty = 3) ; abline(v = par.est@coef[1], lty = 2) ## intervals --------------------------------------------------------------------- ### probability-based interval --------------------------------------------------- # cutoff&#39;s corresponding to a 95\\% confidence interval for beta # minus, since lkl.model.poi return -lkl cut &lt;- log(.15) - par.est@details$value ; abline(h = cut, lty = 2) # finding the cutoff values ic.poi &lt;- function(grid, coef = c(&quot;b0&quot;, &quot;b1&quot;)) { lkl.prof &lt;- switch( coef, &quot;b0&quot; = mle2(lkl.model.poi, start = list(b1 = 0), data = list(data = data, b0 = grid), method = &quot;BFGS&quot; )@details$value, &quot;b1&quot; = mle2(lkl.model.poi, start = list(b0 = min(data$y)), data = list(data = data, b1 = grid), method = &quot;BFGS&quot; )@details$value) (-1) * lkl.prof - cut} # for some &quot;mysterious&quot; reason the uniroot.all # doesn&#39;t work here, but uniroot works perfectly ic.95.prob.b0 &lt;- c(uniroot(ic.poi, c(1, par.est@coef[1]), &quot;b0&quot;)$root, uniroot(ic.poi, c(par.est@coef[1], 3), &quot;b0&quot;)$root) arrows(x0 = ic.95.prob.b0, y0 = rep(cut, 2), x1 = ic.95.prob.b0, y1 = rep(-43.5, 2), lty = 2, length = .1) ### wald confidence interval ----------------------------------------------------- # quadratic approximation qa.coef.poi &lt;- function(grid, coef = c(&quot;b0&quot;, &quot;b1&quot;)) { coef.hat &lt;- switch(coef, &quot;b0&quot; = par.est@coef[1], &quot;b1&quot; = par.est@coef[2]) lkl.prof &lt;- coef.profile(coef.hat, coef) hess &lt;- hessian(coef.profile, x = coef.hat, coef = coef) n &lt;- length(grid) ; qa &lt;- numeric(n) for (i in 1:n) qa[i] &lt;- lkl.prof + .5 * hess * (grid[i] - coef.hat)**2 return(qa)} qa.b0.poi &lt;- qa.coef.poi(b0.grid, &quot;b0&quot;) ; lines(b0.grid, qa.b0.poi, col = &quot;darkgray&quot;) cut &lt;- log(.15) + qa.coef.poi(par.est@coef[1], &quot;b0&quot;) abline(h = cut, lty = 2, col = &quot;darkgray&quot;) # now, finding the cutoff values ic.coef.qa.poi &lt;- function(grid, coef) qa.coef.poi(grid, coef) - cut wald.95.b0 &lt;- uniroot.all(ic.coef.qa.poi, range(b0.grid), coef = &quot;b0&quot;) arrows(x0 = wald.95.b0, y0 = rep(cut, 2), x1 = wald.95.b0, y1 = rep(-43.5, 2), lty = 2, length = .1, col = &quot;darkgray&quot;) ### from the mle2 output we get the standard error of each parameter ------------- se.b0 &lt;- summary(par.est)@coef[1, 2] mle2.95.b0 &lt;- par.est@coef[1] + qnorm(c(.025, .975)) * se.b0 arrows(x0 = mle2.95.b0, y0 = rep(cut, 2), x1 = mle2.95.b0, y1 = rep(-43.5, 2), lty = 3, length = .1) # beta_1 ------------------------------------------------------------------------- plot(b1.grid, coef.profile(b1.grid, &quot;b1&quot;), type = &quot;l&quot;, xlab = expression(beta[1]), ylab = &quot;log-likelihood&quot;) abline(v = .5, lty = 3) ; abline(v = par.est@coef[2], lty = 2) ## intervals --------------------------------------------------------------------- ### probability-based interval --------------------------------------------------- abline(h = cut, lty = 2) # remember, the cutoff is the same # for some &quot;mysterious&quot; reason the uniroot.all # doesn&#39;t work here, but uniroot works perfectly ic.95.prob.b1 &lt;- c(uniroot(ic.poi, c(.2, par.est@coef[2]), &quot;b1&quot;)$root, uniroot(ic.poi, c(par.est@coef[2], .8), &quot;b1&quot;)$root) arrows(x0 = ic.95.prob.b1, y0 = rep(cut, 2), x1 = ic.95.prob.b1, y1 = rep(-42, 2), lty = 2, length = .1) ### wald confidence interval ----------------------------------------------------- qa.b1.poi &lt;- qa.coef.poi(b1.grid, &quot;b1&quot;) ; lines(b1.grid, qa.b1.poi, col = &quot;darkgray&quot;) cut &lt;- log(.15) + qa.coef.poi(par.est@coef[2], &quot;b1&quot;) abline(h = cut, lty = 2, col = &quot;darkgray&quot;) wald.95.b1 &lt;- uniroot.all(ic.coef.qa.poi, range(b1.grid), coef = &quot;b1&quot;) arrows(x0 = wald.95.b1, y0 = rep(cut, 2), x1 = wald.95.b1, y1 = rep(-42, 2), lty = 2, length = .1, col = &quot;darkgray&quot;) ### mle2 standard error of ------------------------------------------------------- se.b1 &lt;- summary(par.est)@coef[2, 2] mle2.95.b1 &lt;- par.est@coef[2] + qnorm(c(.025, .975)) * se.b1 arrows(x0 = mle2.95.b1, y0 = rep(cut, 2), x1 = mle2.95.b1, y1 = rep(-42, 2), lty = 3, length = .1) Figure 17: Profile log-likelihoods, in black, for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) in a Poisson regression with some intervals. In black dotted, the original parameter value. In black dashed, the MLE. The gray curve is a quadratic approx. for the profile, with the gray dashed arrows corresponding to its 95% interval. The black dashed arrows correspond to the 95% interval based in a cut in the likelihood. The black dotted arrows correspond to the 95% interval based in the output. By the amount of code provided in this question, we can see that isn’t so straightforward to obtain intervals and quadratic approximations of profiled likelihoods of parameters. A very nice and interesting thing that we can see in Figure 17 is how all the three obtained intervals match. A brief, but enough, description is already given in the Figure caption, and for more details, you have the provided code. Did we obtained the right values? Compare the previous results with those provided by the \\(\\texttt{glm()}\\) function and discuss the findings. # fitting the model via glm() ---------------------------------------------------- model.glm &lt;- glm(y ~ x, data, family = poisson) # parameter estimates ------------------------------------------------------------ model.glm$coef (Intercept) x 2.0675451 0.4816599 # at three decimal cases, the estimates are equal round(model.glm$coef, 3) == round(par.est@coef, 3) (Intercept) x TRUE TRUE # maximum log-likelihood --------------------------------------------------------- logLik(model.glm) &#39;log Lik.&#39; -27.85527 (df=2) # at eight decimal cases, the max. log-likes are equal round(logLik(model.glm), 8) == round(par.est@details[[2]] * (-1), 8) [1] TRUE # standard erros ----------------------------------------------------------------- summary(model.glm)$coef[ , 2] (Intercept) x 0.13201821 0.03494989 ## at six decimal cases, the standard errors are equal round(summary(model.glm)$coef[ , 2], 6) == round(summary(par.est)@coef[ , 2], 6) (Intercept) x TRUE TRUE As a general reference guide for questions 2 to 6, I read and used (in portuguese): MCIE @conference{mcie, author = {Wagner Hugo Bonat and Paulo Justiniano RIbeiro Jr and Elias Teixeira Krainski and Walmes Marques Zeviani}, title = {Computational Methods in Statistical Inference}, year = {2012}, publisher = {20th National Symposium on Probability and Statistics (SINAPE)}, address = {Jo\\~{a}o Pessoa - PB - Brazil}, } "],
["poisson-regression-extra.html", "Poisson regression (extra)", " Poisson regression (extra) Poisson regression (focus on the profile likelihood) In the previous exercise, we computed the profile likelihood for each parameter of a simple Poisson regression with just one covariate, i.e., an intercept, \\(\\beta_{0}\\), and an angular coefficient, \\(\\beta_{1}\\). Together with the profile and an interval based in a cut in this profile likelihood (let’s say, a probability-based interval), we did a quadratic approximation in this profile likelihood. In the “real” world with “real” problems this isn’t a very smart thing to do, but here just as an exercise, I believe that is extremely valid. Using the same functions created before, we first show, in black, in Figure 18, the profiles likelihood together with its respective quadratic approximations. Now, something that we can look at is to the conditional, or estimated likelihood (as \\(\\texttt{Pawitan}\\) calls). In the profiled one, for \\(\\beta_{0}\\) let’s say, in a grid of \\(\\beta_{0}\\)’s, we estimate the \\(\\beta_{1}\\)’s and then get the corresponding likelihoods. In the conditional one, as the name suggests, we fix (conditionate) \\(\\beta_{1}\\) in a value, and in a grid of \\(\\beta_{0}\\)’s, we get the likelihoods. The conditional likelihood for each parameter and its correspondent quadratic approximation is computed and presented, in gray, in Figure 18. # lkl.est.poi: estimated (conditional) poisson likelihood ------------------------ lkl.est.poi &lt;- function(grid, coef = c(&quot;b0&quot;, &quot;b1&quot;)) { coef.hat &lt;- switch(coef, &quot;b0&quot; = par.est@coef[2], &quot;b1&quot; = par.est@coef[1]) n &lt;- length(grid) lkl.est &lt;- numeric(n) for (i in 1:n) { lkl.est[i] &lt;- switch( coef, &quot;b0&quot; = lkl.model.poi(b0 = grid[i], b1 = coef.hat, data = data), &quot;b1&quot; = lkl.model.poi(b0 = coef.hat, b1 = grid[i], data = data)) } return((-1) * lkl.est) } # qa.est.poi: quadratic approximation of the estimated poisson likelihood -------- qa.est.poi &lt;- function(grid, coef = c(&quot;b0&quot;, &quot;b1&quot;)) { coef.hat &lt;- switch(coef, &quot;b0&quot; = par.est@coef[1], &quot;b1&quot; = par.est@coef[2]) lkl.hat &lt;- -par.est@min hess &lt;- switch(coef, &quot;b0&quot; = -par.est@details$hessian[1], &quot;b1&quot; = -par.est@details$hessian[4]) n &lt;- length(grid) qa &lt;- numeric(n) for (i in 1:n) { qa[i] &lt;- switch(coef, &quot;b0&quot; = lkl.hat + .5 * hess * (grid[i] - coef.hat)**2, &quot;b1&quot; = lkl.hat + .5 * hess * (grid[i] - coef.hat)**2) } return(qa) } # -------------------------------------------------------------------------------- par(mar = c(4, 4, 2, 2) + .1, mfrow = c(2, 1)) # plot window definition # beta_0 ------------------------------------------------------------------------- ## first, plotting the profile with point estimate ------------------------------- b0.profile &lt;- coef.profile(b0.grid, &quot;b0&quot;) plot(b0.grid, b0.profile, type = &quot;l&quot;, xlab = expression(beta[0]), ylab = &quot;log-likelihood&quot;) abline(v = par.est@coef[1], lty = 3) ## now, the quadratic approximation of the profile ------------------------------- lines(b0.grid, qa.b0.poi, lty = 2) ## performing and plotting the conditional likelihood ---------------------------- lkl.est.b0.poi &lt;- lkl.est.poi(b0.grid, &quot;b0&quot;) lines(b0.grid, lkl.est.b0.poi, col = &quot;darkgray&quot;) ## performing and plotting the quadratic approx. of the cond. likelihood --------- qa.est.b0.poi &lt;- qa.est.poi(b0.grid, &quot;b0&quot;) lines(b0.grid, qa.est.b0.poi, col = &quot;darkgray&quot;, lty = 2) # beta_1 ------------------------------------------------------------------------- b1.profile &lt;- coef.profile(b1.grid, &quot;b1&quot;) plot(b1.grid, b1.profile, type = &quot;l&quot;, xlab = expression(beta[1]), ylab = &quot;log-likelihood&quot;) abline(v = par.est@coef[2], lty = 3) lines(b1.grid, qa.b1.poi, lty = 2) lkl.est.b1.poi &lt;- lkl.est.poi(b1.grid, &quot;b1&quot;) lines(b1.grid, lkl.est.b1.poi, col = &quot;darkgray&quot;) qa.est.b1.poi &lt;- qa.est.poi(b1.grid, &quot;b1&quot;) lines(b1.grid, qa.est.b1.poi, col = &quot;darkgray&quot;, lty = 2) Figure 18: Profile log-likelihoods and quadratic approximations, in black (solid and dashed, respectively), for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) in a Poisson regression. In dotted, the MLE. Estimated (conditional) log-likelihoods and quadratic approximations, in gray (solid and dashed, respectively). The first and important note that we can take from Figure 18, and that makes all the possible sense, is how the estimated likelihood is narrow (compared with the profiled one). The explanation for this is that in the estimated (conditional) situation it’s assumed as known one of the coefficients, fixing it at the MLE. In the other hand, the profile likelihood takes into account that one of the coefficients is unknown. Hence, it’s sensible that the profile is greater than the estimated. Another important point to mention is about the manner that we compute the profile likelihood. A function with this goal was made in the previous question, \\(\\texttt{qa.coef.poi}\\). The important thing to note about these function is that the hessian there is computed from the profile likelihood. This is inefficient, and not necessary. Instead, we can just take the corresponding element in the variance-covariance matrix, invert, and use it. In other words, we don’t need to use the profile likelihood to compute a quadratic approximation to the profile likelihood. We implement this in \\(\\texttt{smart.qa.coef.poi}\\). # qa: quadratic approximation ---------------------------------------------------- smart.qa.coef.poi &lt;- function(model.mle2, coef = c(&quot;b0&quot;, &quot;b1&quot;), grid) { coef.hat &lt;- switch(coef, &quot;b0&quot; = model.mle2@coef[1], &quot;b1&quot; = model.mle2@coef[2]) lkl.hat &lt;- (-1) * model.mle2@details$value hess &lt;- switch(coef, &quot;b0&quot; = (-1) / model.mle2@vcov[1], &quot;b1&quot; = (-1) / model.mle2@vcov[4]) n &lt;- length(grid) ; qa &lt;- numeric(n) for (i in 1:n) { qa[i] &lt;- switch(coef, &quot;b0&quot; = lkl.hat + .5 * hess * (grid[i] - coef.hat)**2, &quot;b1&quot; = lkl.hat + .5 * hess * (grid[i] - coef.hat)**2)} return(qa)} # -------------------------------------------------------------------------------- par(mar = c(4, 4, 2, 2) + .1) layout(matrix(1:4, 2, 2), widths = c(1.5, 1)) # plot window definition # beta_0 ------------------------------------------------------------------------- ## first, plotting the profile with the point estimate --------------------------- plot(b0.grid, b0.profile, type = &quot;l&quot;, xlab = expression(beta[0]), ylab = &quot;log-likelihood&quot;) ; abline(v = par.est@coef[1], lty = 3) ## now, the &quot;old&quot; quadratic approximation of the profile ------------------------- lines(b0.grid, qa.b0.poi, lty = 2) ## and finally, the new and smart quadratic approximation of the profile --------- sqa.b0.poi &lt;- smart.qa.coef.poi(model.mle2 = par.est, coef = &quot;b0&quot;, grid = b0.grid) lines(b0.grid, sqa.b0.poi, col = &quot;gray&quot;) # beta_1 ------------------------------------------------------------------------- plot(b1.grid, b1.profile, type = &quot;l&quot;, xlab = expression(beta[1]), ylab = &quot;log-likelihood&quot;) ; abline(v = par.est@coef[2], lty = 3) lines(b1.grid, qa.b1.poi, lty = 2) sqa.b1.poi &lt;- smart.qa.coef.poi(model.mle2 = par.est, coef = &quot;b1&quot;, grid = b1.grid) lines(b1.grid, sqa.b1.poi, col = &quot;gray&quot;) # how much the quadratic approximations are similar? ----------------------------- compare.qas &lt;- function(dp, coef = c(&quot;b0&quot;, &quot;b1&quot;)) { compare &lt;- numeric(dp) for (i in 1:dp) { compare[i] &lt;- switch( coef, &quot;b0&quot; = table(round(sqa.b0.poi, i) == round(qa.b0.poi, i))[&quot;TRUE&quot;], &quot;b1&quot; = table(round(sqa.b1.poi, i) == round(qa.b1.poi, i))[&quot;TRUE&quot;])} compare[is.na(compare)] &lt;- 0 graph &lt;- plot(compare, type = &quot;h&quot;, xlab = &quot;decimal places&quot;, ylab = &quot;% of matching&quot;) return(invisible(graph))} compare.qas(6, &quot;b0&quot;) ; compare.qas(6, &quot;b1&quot;) Figure 19: Profile log-likelihoods, solid line, for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) in a Poisson regression. In dotted, MLEs. Quadratic approx.’s of the profiles in black dashed and gray, with a matching percentage between them considering six different decimal places. In Figure 19 we see that the dashed matches with the gray line, i.e., the quadratic approximations are equal. To see better how equal they’re, the right graphs of Figure 19 show the percentage of matching values (quadratic approximations of the profile log-likelihoods) considering different levels of rounding. As a conclusion statement, they’re quite equal. This completes and complement the last paragraph. A third and last note is that to compute the quadratic approx. for the estimated/conditional likelihood we use the corresponding element of the information matrix. To compute the quadratic approx. for the profile likelihood we use the inverse of that element in the variance-covariance matrix, that is the inverse of the information matrix. The insights for this exercise was taken from \\(\\texttt{Pawitan}\\)’s book. "]
]
