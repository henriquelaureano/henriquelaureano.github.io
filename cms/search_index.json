[
["index.html", "Computational Methods in Statistics README", " Computational Methods in Statistics Henrique Aparecido Laureano 2017-04-25 README Summaries and study notes of the discipline: Computational Methods in Statistics Program: Masterâ€™s degree in Statistics University: Unicamp (State University of Campinas) "],
["bootstrap.html", "Bootstrap Bootstrap estimation of standard error Bootstrap estimation of bias", " Bootstrap Definition 1 Bootstrap strategies compose a class of non-parametric Monte Carlo methods that estimate the distribution of a population by means of resampling. Resampling methods treat samples as a finite population, from which samples are taken to estimate features and make inferences about this population. To generate a random bootstrap sample by \\(x\\) resampling, generate \\(n\\) uniformly distributed random integers in the set \\(1, ..., n{}\\) and select the bootstrap sample as \\(x^{*} = (x_{i_{1}}, ..., x_{i_{n}})\\). \\[ F \\rightarrow X \\rightarrow F_{n} \\\\ F_{n} \\rightarrow X^{*} \\rightarrow F_{n}^{*}. \\] Example 1 Suppose we observe the following data: \\(x = {2, 2, 1, 1, 5, 4, 4, 3, 1, 2}\\). Resampling from \\(x\\), we select \\(\\{1, 2, 3, 4, 5\\}\\) with probabilities \\(\\{0.3, 0.3, 0.1, 0.2, 0.1\\}\\). The \\(F_{X^{*}}(x)\\) distribution of a sample taken at random is exactly the function \\(F_{n}\\): \\[ F_{X^{*}}(x) = F_{n}(x) = \\begin{cases} 0.0 &amp; x &lt; 1; \\\\ 0.3 &amp; 1 \\leq x &lt; 2; \\\\ 0.6 &amp; 2 \\leq x &lt; 3; \\\\ 0.7 &amp; 3 \\leq x &lt; 4; \\\\ 0.9 &amp; 4 \\leq x &lt; 5; \\\\ 1.0 &amp; x \\geq 5. \\end{cases} \\] Bootstrap estimation of standard error The bootstrap estimate of the standard error of an estimator \\(\\hat{\\theta}\\) is the sample standard error of the bootstrap replicates \\(\\hat{\\theta}^{(1)}, ..., \\hat{\\theta}^{(B)}\\): \\[ \\text{se}(\\hat{\\theta}^{*}) = \\sqrt{\\frac{1}{B - 1} \\sum_{b = 1}^{B} (\\hat{\\theta}^{(b)} - \\bar{\\theta}^{*})^{2}}, \\\\ \\bar{\\theta}^{*} = \\frac{1}{B} \\sum_{b = 1}^{B} \\hat{\\theta}^{(b)}. \\] Where \\(B\\) is the number of replicates. Example 2 (Correlation) lattice::xyplot(dist ~ speed, cars, pch = 16, ylab = list(rot = 0)) Figure 1: A scatterplot of the speed and dist variables of the cars dataframe. set.seed(1) B &lt;- 10e3 boot.corr &lt;- vector(&#39;numeric&#39;, B) for (b in 1:B){ ind &lt;- sample(nrow(cars), replace = TRUE) boot.corr[b] &lt;- with(cars[ind, ], cor(dist, speed)) } (theta.star &lt;- mean(boot.corr)) [1] 0.8062458 (se.theta.star &lt;- sd(boot.corr)) [1] 0.04782856 Bootstrap estimation of bias The definition of bias is given by: \\[ B(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta. \\] Thus, the bootsrap estimate of bias is: \\[ \\hat{B(\\hat{\\theta})} = \\bar{\\hat{\\theta}^{*}} - \\theta, \\] expression in which \\(\\bar{\\hat{\\theta}^{*}} = \\frac{1}{B} \\sum_{b = 1}^{B} \\hat{\\theta}^{(b)}\\) and \\(\\hat{\\theta}\\) is the estimate calculated using the original sample. Every statistic is an unbiased estimator of its expected value, and in particular, the sample mean of a random sample is an unbiased estimator of the mean of the distribution. An example of a biased estimator is the maximum likelihood estimator of variance, \\(\\hat{\\sigma}^{2} = \\frac{1}{n} \\sum_{i = 1}^{n} (X_{i} - \\bar{X})^{2}\\), which has expected value \\((1 - 1/n) \\sigma^{2}\\). Thus, \\(\\hat{\\sigma}^{2}\\) underestimates \\(\\sigma^{2}\\), and the bias is \\(-\\sigma^{2}/n\\). In bootstrap \\(F_{n}\\) is sampled in place of \\(F_{X}\\), so we replace \\(\\theta\\) with \\(\\hat{\\theta}\\) to estimate the bias. Positive bias indicates that \\(\\hat{\\theta}\\) on average tends to overestimate \\(\\theta\\). Example 3 (Correlation) theta.star - with(cars, cor(dist, speed)) [1] -0.0006490941 "],
["permutation-tests.html", "Permutation tests", " Permutation tests Resampling is used without replacement They are applied for general hypotheses tests: \\[ X_{1}, ..., X_{n} \\sim F \\quad \\text{ and } \\quad Y_{1}, ..., Y_{m} \\sim G \\\\ H_{0} : F = G \\quad \\text{ vs.} \\quad H_{1} : F \\neq G \\] Permutation tests can be performed to check for independence, homogeneity, tests for more than 2 groups, etc. Let \\(Z\\) be the ordered set \\(\\{X_{1}, ..., X_{n}, Y_{1}, ..., Y_{m}\\}\\) indexed by the indices \\(\\nu = \\{1, ..., n, n + 1, ..., n + m\\} = \\{1, ..., N\\}\\). Under \\(H_{0}\\), the chance to select \\(n\\) elements of \\(Z\\) is \\[ \\frac{1}{\\binom{N}{n}} = \\frac{n! m!}{N!}. \\] knitr::include_graphics(&quot;iBagens/permutation.png&quot;) Figure 2: A representation of a permutation test. Example 4 (Birth weight of chickens according 2 diets) data(&quot;chickwts&quot;) (x &lt;- with(chickwts, sort(as.vector(weight[feed == &quot;soybean&quot;])))) [1] 158 171 193 199 230 243 248 248 250 267 271 316 327 329 (y &lt;- with(chickwts, sort(as.vector(weight[feed == &quot;linseed&quot;])))) [1] 141 148 169 181 203 213 229 244 257 260 271 309 r = 100 z = c(x, y) k = length(z) t.s = vector(&quot;numeric&quot;, r) t = t.test(x, y)$statistic for (i in 1:r){ ks = sample(k, size = length(x), replace = FALSE) t.s[i] = t.test(z[ks], z[-ks])$statistic } (p &lt;- mean(c(t, t.s) &gt;= t)) [1] 0.1188119 "]
]
