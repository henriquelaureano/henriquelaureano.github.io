[
["index.html", "Computational Methods in Statistics README", " Computational Methods in Statistics Henrique Aparecido Laureano 2017-05-06 README Summaries and study notes of the discipline: Computational Methods in Statistics Summaries and study notes taken from the book: Maria L. Rizzo, 2008. Statistical Computing with R, Chapman &amp; Hall/CRC, ISBN 1-58488-545-9, 393 pages. Program: Master’s degree in Statistics University: Unicamp (State University of Campinas) "],
["bootstrap.html", "Bootstrap Bootstrap estimation of standard error Bootstrap estimation of bias", " Bootstrap Definition 1 Bootstrap strategies compose a class of non-parametric Monte Carlo methods that estimate the distribution of a population by means of resampling. Resampling methods treat samples as a finite population, from which samples are taken to estimate features and make inferences about this population. To generate a random bootstrap sample by \\(x\\) resampling, generate \\(n\\) uniformly distributed random integers in the set \\(1, ..., n{}\\) and select the bootstrap sample as \\(x^{*} = (x_{i_{1}}, ..., x_{i_{n}})\\). \\[ F \\rightarrow X \\rightarrow F_{n} \\\\ F_{n} \\rightarrow X^{*} \\rightarrow F_{n}^{*}. \\] Example 1 Suppose we observe the following data: \\(x = {2, 2, 1, 1, 5, 4, 4, 3, 1, 2}\\). Resampling from \\(x\\), we select \\(\\{1, 2, 3, 4, 5\\}\\) with probabilities \\(\\{0.3, 0.3, 0.1, 0.2, 0.1\\}\\). The \\(F_{X^{*}}(x)\\) distribution of a sample taken at random is exactly the function \\(F_{n}\\): \\[ F_{X^{*}}(x) = F_{n}(x) = \\begin{cases} 0.0 &amp; x &lt; 1; \\\\ 0.3 &amp; 1 \\leq x &lt; 2; \\\\ 0.6 &amp; 2 \\leq x &lt; 3; \\\\ 0.7 &amp; 3 \\leq x &lt; 4; \\\\ 0.9 &amp; 4 \\leq x &lt; 5; \\\\ 1.0 &amp; x \\geq 5. \\end{cases} \\] Bootstrap estimation of standard error The bootstrap estimate of the standard error of an estimator \\(\\hat{\\theta}\\) is the sample standard error of the bootstrap replicates \\(\\hat{\\theta}^{(1)}, ..., \\hat{\\theta}^{(B)}\\): \\[ \\text{se}(\\hat{\\theta}^{*}) = \\sqrt{\\frac{1}{B - 1} \\sum_{b = 1}^{B} (\\hat{\\theta}^{(b)} - \\bar{\\theta}^{*})^{2}}, \\\\ \\bar{\\theta}^{*} = \\frac{1}{B} \\sum_{b = 1}^{B} \\hat{\\theta}^{(b)}. \\] Where \\(B\\) is the number of replicates. Example 2 (Correlation) lattice::xyplot(dist ~ speed, cars, pch = 16, ylab = list(rot = 0)) Figure 1: A scatterplot of the speed and dist variables of the cars dataframe. set.seed(1) B &lt;- 10e3 boot.corr &lt;- vector(&#39;numeric&#39;, B) for (b in 1:B){ ind &lt;- sample(nrow(cars), replace = TRUE) boot.corr[b] &lt;- with(cars[ind, ], cor(dist, speed)) } (theta.star &lt;- mean(boot.corr)) [1] 0.8062458 (se.theta.star &lt;- sd(boot.corr)) [1] 0.04782856 Bootstrap estimation of bias The definition of bias is given by: \\[ B(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta. \\] Thus, the bootsrap estimate of bias is: \\[ \\hat{B(\\hat{\\theta})} = \\bar{\\hat{\\theta}^{*}} - \\theta, \\] expression in which \\(\\bar{\\hat{\\theta}^{*}} = \\frac{1}{B} \\sum_{b = 1}^{B} \\hat{\\theta}^{(b)}\\) and \\(\\hat{\\theta}\\) is the estimate calculated using the original sample. Every statistic is an unbiased estimator of its expected value, and in particular, the sample mean of a random sample is an unbiased estimator of the mean of the distribution. An example of a biased estimator is the maximum likelihood estimator of variance, \\(\\hat{\\sigma}^{2} = \\frac{1}{n} \\sum_{i = 1}^{n} (X_{i} - \\bar{X})^{2}\\), which has expected value \\((1 - 1/n) \\sigma^{2}\\). Thus, \\(\\hat{\\sigma}^{2}\\) underestimates \\(\\sigma^{2}\\), and the bias is \\(-\\sigma^{2}/n\\). In bootstrap \\(F_{n}\\) is sampled in place of \\(F_{X}\\), so we replace \\(\\theta\\) with \\(\\hat{\\theta}\\) to estimate the bias. Positive bias indicates that \\(\\hat{\\theta}\\) on average tends to overestimate \\(\\theta\\). Example 3 (Correlation) theta.star - with(cars, cor(dist, speed)) [1] -0.0006490941 "],
["permutation-tests.html", "Permutation tests Approximate permutation test procedure", " Permutation tests Resampling is used without replacement They are applied for general hypotheses tests: \\[ X_{1}, ..., X_{n} \\sim F \\quad \\text{ and } \\quad Y_{1}, ..., Y_{m} \\sim G \\\\ H_{0} : F = G \\quad \\text{ vs.} \\quad H_{1} : F \\neq G \\] Permutation tests can be performed to check for independence, homogeneity, tests for more than 2 groups, etc. Let \\(Z\\) be the ordered set \\(\\{X_{1}, ..., X_{n}, Y_{1}, ..., Y_{m}\\}\\) indexed by the indices \\(\\nu = \\{1, ..., n, n + 1, ..., n + m\\} = \\{1, ..., N\\}\\). Under \\(H_{0}\\), the chance to select \\(n\\) elements of \\(Z\\) is \\[ \\frac{1}{\\binom{N}{n}} = \\frac{n! m!}{N!}. \\] If \\(\\hat{\\theta}(X, Y) = \\hat{\\theta}(Z, \\nu)\\) is a statistic, then the permutation distribution of \\(\\hat{\\theta}^{*}\\) is the distribution of the replicates \\[\\begin{align} \\left\\{ \\hat{\\theta}^{*} \\right\\} &amp;= \\left\\{ \\hat{\\theta}(Z, \\pi_{j}(\\nu)), j = 1, ..., \\binom{N}{n} \\right\\} \\\\ &amp; = \\left\\{ \\hat{\\theta}^{(j)} | \\pi_{j}(\\nu) \\text{ is a permutation of } \\nu \\right\\}. \\end{align}\\] The cdf of \\(\\hat{\\theta}^{*}\\) is given by \\[ F_{\\theta^{*}}(t) = P(\\hat{\\theta}^{*} \\leq t) = \\binom{N}{n}^{-1} \\sum_{j = 1}^{N} I\\left( \\hat{\\theta}^{(j)} \\leq t \\right). \\] Thus, if \\(\\hat{\\theta}\\) is applied to test a hypothesis and large values of \\(\\hat{\\theta}\\) are significant, then the permutation test rejects the null hypothesis when \\(\\hat{\\theta}\\) is large relative to the distribution of the permutation replicates. The achieved significance level (ASL) of the observed statistic \\(\\hat{\\theta}\\) is the probability \\[ P(\\hat{\\theta}^{*} \\geq \\hat{\\theta}) = \\binom{N}{n}^{-1} \\sum_{j = 1}^{N} I\\left( \\hat{\\theta}^{(j)} \\geq t \\right), \\] where \\(\\hat{\\theta} = \\hat{\\theta}(Z, \\nu)\\) is the statistic computed on the observed sample. The ASL for a lower-tail or two-tail test based on \\(\\hat{\\theta}\\) is computed in a similar way. In practice, unless the sample size is very small, evaluating the test statistic for all of the \\(\\binom{N}{n}\\) permutations is computationally excessive. An approximate permutation test is implemented by randomly drawing a large number of samples without replacement. Approximate permutation test procedure Compute the observed test statistic \\(\\hat{\\theta}(X, Y) = \\hat{\\theta}(Z, \\nu)\\)ν). For each replicate, indexed \\(b = 1, ..., B\\): Generate a random permutation \\(\\pi_{b} = \\pi(\\nu)\\). Compute the statistic \\(\\hat{\\theta}^{(b)} = \\hat{\\theta}^{*}(Z, \\pi_{b})\\). If large values of \\(\\hat{\\theta}\\) support the alternative, compute the ASL (the em- pirical p-value) by \\[ \\hat{p} = \\frac{1 + \\#\\left\\{ \\hat{\\theta}^{(b)} \\geq \\hat{\\theta} \\right\\}}{B + 1} = \\frac{ \\left\\{ 1 + \\sum_{b = 1}^{B} I\\left( \\hat{\\theta}^{(b)} \\geq \\hat{\\theta} \\right) \\right\\}}{B + 1}. \\] Reject \\(H_{0}\\) at significance level \\(\\alpha\\) if \\(\\hat{p} \\leq \\alpha\\). knitr::include_graphics(&quot;iBagens/permutation.png&quot;) Figure 2: A representation of a permutation test. Example 4 (Birth weight of chickens according 2 diets) data(&quot;chickwts&quot;) (x &lt;- with(chickwts, sort(as.vector(weight[feed == &quot;soybean&quot;])))) [1] 158 171 193 199 230 243 248 248 250 267 271 316 327 329 (y &lt;- with(chickwts, sort(as.vector(weight[feed == &quot;linseed&quot;])))) [1] 141 148 169 181 203 213 229 244 257 260 271 309 r = 100 z = c(x, y) k = length(z) t.s = vector(&quot;numeric&quot;, r) t = t.test(x, y)$statistic for (i in 1:r){ ks = sample(k, size = length(x), replace = FALSE) t.s[i] = t.test(z[ks], z[-ks])$statistic } (p &lt;- mean(c(t, t.s) &gt;= t)) [1] 0.1188119 "],
["exercises-chapter-8-permutation-tests.html", "Exercises - Chapter 8: Permutation Tests Exercise 8.1 Exercise 8.2 Exercise 8.3 Exercise 8.4", " Exercises - Chapter 8: Permutation Tests Students: Henrique Aparecido Laureano Elainy Marciano Batista Heidi Mara do Rosário Souza Joubert Miranda Guedes Nicolas Romano Pedro Luciano de Oliveira Gomes Ricardo de Faria Souza Telma Tompson Thais Castelo Branco Monho Exercises from chapter 8 of the book: Maria L. Rizzo, 2008. Statistical Computing with R, Chapman &amp; Hall/CRC, ISBN 1-58488-545-9, 393 pages. Exercise 8.1 Implement the two-sample Cramér-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2. Solution: data(&quot;chickwts&quot;) ## Packages library(latticeExtra) ## function: two-sample Cramér-von Mises test for equal distributions cvm &lt;- function(x, y, data){ r &lt;- 10000 # Permutation samples reps &lt;- vector(&quot;numeric&quot;, r) n &lt;- length(x) m &lt;- length(y) v.n &lt;- vector(&quot;numeric&quot;, n) # Replication vectors v1.n &lt;- vector(&quot;numeric&quot;, n) v.m &lt;- vector(&quot;numeric&quot;, m) v1.m &lt;- vector(&quot;numeric&quot;, m) z &lt;- c(x, y) N &lt;- length(z) for (i in 1:n) v.n[i] &lt;- ( x[i] - i )**2 for (j in 1:m) v.m[j] &lt;- ( y[j] - j )**2 # Test statistic reps_0 &lt;- ( (n * sum(v.n) + m * sum(v.m)) / (m * n * N) ) - (4 * m * n - 1) / (6 * N) for (k in 1:r) { # Permautation samples w &lt;- sample(N, size = n, replace = FALSE) x1 &lt;- sort( z[ w] ) y1 &lt;- sort( z[-w] ) for (i in 1:n) { v1.n[i] &lt;- ( x1[i] - i )**2 } for (j in 1:m) { v1.m[j] &lt;- ( y1[j] - j )**2 } reps[k] &lt;- ( (n * sum(v1.n) + m * sum(v1.m)) / (m * n * N) ) - (4 * m * n - 1) / (6 * N) } p &lt;- mean( c(reps_0, reps) &gt;= reps_0 ) return( histogram(c(reps_0, reps) # Histogram , type = &quot;density&quot; , col = &quot;#0080ff&quot; , xlab = &quot;Replicates of Cramér-Von Mises (CVM) statistic&quot; , ylab = list(rot = 0) , main = paste0( &quot;Data: &quot;, data, &quot;\\n&quot; , &quot;Permutation distribution of CVM statistic&quot;) , sub = list(substitute(paste(hat(p), &quot; = &quot;,pvalue) , list(pvalue = p)) , col = 2) , panel = function(...){ panel.histogram(...) panel.abline(v = reps_0, col = 2, lwd = 2) }) ) } ## Data: Example 8.1 x &lt;- with(chickwts, sort(as.vector(weight[feed == &quot;soybean&quot;]))) y &lt;- with(chickwts, sort(as.vector(weight[feed == &quot;linseed&quot;]))) cvm_8.1 &lt;- cvm(x, y, &quot;Example 8.1&quot;) ## Data: Example 8.2 x &lt;- with(chickwts, sort(as.vector(weight[feed == &quot;sunflower&quot;]))) y &lt;- with(chickwts, sort(as.vector(weight[feed == &quot;linseed&quot;]))) cvm_8.2 &lt;- cvm(x, y, &quot;Example 8.2&quot;) ## Results print(cvm_8.1, position = c(0, 0, .5, 1), more = TRUE) print(cvm_8.2, position = c(.5, 0, 1, 1)) Exercise 8.2 Implement the bivariate Spearman rank correlation test for independence [255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = &quot;spearman&quot;. Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples. Solution: data(&quot;iris&quot;) ## Dataset z &lt;- as.matrix(iris[1:50, 1:4]) x &lt;- (z[ , 1:2]) y &lt;- (z[ , 3:4]) cor(x,y, method = &quot;spearman&quot;) Petal.Length Petal.Width Sepal.Length 0.2788849 0.2994989 Sepal.Width 0.1799110 0.2865359 cor.test(x, y, method = &quot;spearman&quot;, exact = FALSE)$estimate rho 0.8292811 ## Packages library(boot) ## function: bivariate Spearman rank correlation test for independence spear.cor &lt;- function(z) { rho.est &lt;- function(z, i) { # Test statistic x &lt;- z[ , 1:2] y &lt;- z[i, 3:4] return(cor.test(x, y, method = &quot;spearman&quot;, exact = FALSE)$estimate) } perm &lt;- boot(data = z # 10000 permutation samples , statistic = rho.est , sim = &quot;permutation&quot; , R = 10000) p &lt;- with(perm, mean( c(t, t0) &gt;= t0 )) # p-value return( histogram(with(perm, c(t, t0)) # Histogram , type = &quot;density&quot; , col = &quot;#0080ff&quot; , xlab = &quot;Replicates correlation&quot; , ylab = list(rot = 0) , main = paste( &quot;Permutation distribution of the bivariate Spearman&quot;, &quot;rank correlation test&quot;) , sub = list(substitute(paste(hat(p), &quot; = &quot;,pvalue) , list(pvalue = p)) , col = 2) , panel = function(...){ panel.histogram(...) panel.abline(v = perm$t0, col = 2, lwd = 2) }) ) } spear.cor(z) Exercise 8.3 The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal. Solution: ## Dataset n_1 &lt;- 20 # Defining the sample sizes n_2 &lt;- 30 x &lt;- rnorm(n_1, 0, 5) y &lt;- rnorm(n_2, 0, 5) ## function: permutation test for equal variance based ## on the maximum number of extreme points count5 &lt;- function(x, y) { count5test &lt;- function(x, y) { # Test statistic X &lt;- x - mean(x) Y &lt;- y - mean(y) outx &lt;- sum(X &gt; max(Y)) + sum(X &lt; min(Y)) outy &lt;- sum(Y &gt; max(X)) + sum(Y &lt; min(X)) # Return 1 (reject) or 0 (do not reject H_{0}) return( as.integer( max(c(outx, outy) ) &gt; 5) ) } r &lt;- 10000 # Permutation samples z &lt;- c(x, y) n &lt;- length(z) reps &lt;- vector(&quot;numeric&quot;, r) t0 &lt;- count5test(x, y) for (i in 1:r){ # Permutation test k = sample(n, size = length(x), replace = FALSE) reps[i] = count5test(z[k], z[-k]) } p &lt;- ifelse(t0 == 0, mean( c(t0, reps) &gt; t0 ), mean( c(t0, reps) &gt;= t0 )) return( histogram(c(t0, reps) # Histogram , type = &quot;density&quot; , col = &quot;#0080ff&quot; , xlab = &quot;Replicates of the Count 5 test&quot; , ylab = list(rot = 0) , main = &quot;Permutation distribution of the Count 5 test&quot; , sub = list(substitute(paste(hat(p), &quot; = &quot;,pvalue) , list(pvalue = p)) , col = 2) , panel = function(...){ panel.histogram(...) panel.abline(v = t0, col = 2, lwd = 2) }) ) } count5(x, y) Exercise 8.4 Complete the steps to implement a \\(r^{th}\\) -nearest neighbors test for equal distributions. Write a function to compute the test statistic. The function should take the data matrix as its first argument, and an index vector as the second argument. The number of nearest neighbors r should follow the index argument. Solution: data(&quot;chickwts&quot;) ## Dataset x &lt;- with(chickwts, as.vector(weight[feed == &quot;sunflower&quot;])) y &lt;- with(chickwts, as.vector(weight[feed == &quot;linseed&quot;])) z &lt;- cbind( c(x, y), rep(0, length(c(x, y))) ) ## Packages library(boot) library(FNN) library(latticeExtra) ## function: r^{th} -nearest neighbors test for equal distributions my.knn &lt;- function(z, nn) { t_n.i &lt;- function(z, nn, i) { n_g &lt;- nrow(z) / 2 # length x and length y n &lt;- nrow(z) z &lt;- z[i, ] z &lt;- cbind(z, rep(0, n)) knn &lt;- get.knn(z, k = nn) # Obtaining the k nearest neighbors n_1 &lt;- knn$nn.index[1:n_g, ] # Dividing x n_2 &lt;- knn$nn.index[(n_g + 1):n, ] # and y i_1 &lt;- sum(n_1 &lt; n_g + .5) # Obtaining the sum of i_2 &lt;- sum(n_2 &gt; n_g + .5) # the indicator functions return( (i_1 + i_2) / (nn * n) ) # Test statistic } perm &lt;- boot(data = z # 10000 permutation samples , statistic = t_n.i , sim = &quot;permutation&quot; , R = 10000 , nn = 3) p &lt;- with(perm, mean( c(t, t0) &gt;= t0 )) # p-value return( histogram(with(perm, c(t, t0)) # Histogram , type = &quot;density&quot; , col = &quot;#0080ff&quot; , xlim = c(-.1, 1.1) , xlab = paste0(&quot;Replicates of T(n, &quot;, nn, &quot;) statistic&quot;) , ylab = list(rot = 0) , main = paste0(&quot;Permutation distribution of T(n, &quot; , nn, &quot;) statistic&quot;) , sub = list(substitute(paste(hat(p), &quot; = &quot;, pvalue) , list(pvalue = p)) , col = 2) , panel = function(...){ panel.histogram(...) panel.abline(v = p, col = 2, lwd = 2) }) ) } my.knn(z, 3) # k (or r) nearest neighbors = 3 "]
]
