---
title: "QUASI-LIKELIHOOD FUNCTIONS"
subtitle: By Peter McCullagh, 1983
author: Henrique Laureano ([.github.io](https://henriquelaureano.github.io))
date: July 23, 2021
institute: LEG @ UFPR
classoption: [aspectratio=169]
output:
  beamer_presentation:
    includes:
      in_header: beamerheader.txt
---

###

\begin{columns}
 \column{3cm}
  \includegraphics[height=3cm]{logo/mccullagh_old.jpg}
 \column{12cm}
  \includegraphics[height=4.5cm]{
   logo/quasi_likelihood_functions-cover.png}
\end{columns}

\bigskip
\begin{columns}
 \column{3cm}
  \includegraphics[height=3cm]{logo/mccullagh_now.jpg}
 \column{12cm}
  \begin{enumerate}
   \item Distinguished Professor\\
         in the Department of Statistics @ University of Chicago;
   \vspace{0.15cm}
   \item Completed his PhD at Imperial College London,\\
         supervised by David Cox and Anthony Atkinson;
   \vspace{0.15cm}
   \item Also at Imperial College London,\\
         was the PhD supervisor of Gauss Cordeiro.
  \end{enumerate}
\end{columns}

# Introduction

### Introduction

\vspace{-0.1cm}
1. Likelihood function with \textbf{exponential family form}\newline
   \hspace*{0.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
   MLE through \textbf{weighted least squares}

   - \textbf{variance (assumed) constant}:
     we minimize a sum of squared residuals;
   
   - \textbf{variance not constant}:\newline
     estimating equations can be thought as a generalization of the
     scoring method.

2. Likelihood function \textcolor{AlertOrange}{without} exponential
   family form\newline
   \hspace*{0.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
   In some cases: weighted least squares\newline
   \hspace*{1cm}\rotatebox[origin=c]{180}{$\Lsh$}
   Jorgensen, B. (1983). Maximum likelihood estimation and large sample
                         \newline
                         \hspace*{5.05cm}
                         inference for generalized linear and non-linear
                         \newline
                         \hspace*{5.05cm}
                         regression models. \textit{Biometrika} 70

\vspace{-0.1cm}
\begin{minipage}{14cm}
\begin{block}{Paper purposes}
 \begin{enumerate}
  \item Maximize the likelihood function through weighted least squares
        \newline
        \hspace*{.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
        In which class of problems;
  \item Weighted least squares under 2nd moment assumptions
        (\textbf{quasi-likelihood}).
 \end{enumerate}
\end{block}
\end{minipage}

# A class of likelihood functions

### A class of likelihood functions

\[
 \text{Log likelihood written in the form}: \qquad
 \sigma^{-2}\{\mathbf{y}^{\top}\bm{\theta} -
              \mathbf{b}(\bm{\theta}) - \mathbf{c}(\mathbf{y}, \sigma)\}
\]

\begin{center}
 \begin{minipage}{12.25cm}
  \begin{block}{The first two cumulants}
   By differentiating it and assuming that the support does not depend
   on \(\bm{\theta}\)\newline
   \hspace*{.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
   \(E(\mathbf{Y}) = \bm{\mu} = {\mathbf{b}}'(\bm{\theta})\) and
   \(\text{Cov}(\mathbf{Y}) = \sigma^{2}{\mathbf{b}}''(\bm{\theta})
                            = \sigma^{2}\mathbf{V}(\bm{\mu})\).
  \end{block}
 \end{minipage}
\end{center}

\bigskip
\begin{description}
 \item[In fact,] the \(r\)th order cumulants of \(\mathbf{Y}\) are given
                by \(\kappa_{r} = \sigma^{2r-2}
                                  \mathbf{b}^{(r)}(\bm{\theta})\).
\end{description}

\bigskip
\begin{enumerate}
 \item The first two cumulants describe the random component of the
       model;
 \item However, in applications it is usually the systematic or
       nonrandom variation that is of primary importance\newline
       \hspace*{0.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
       \(E(\mathbf{Y}) = \bm{\mu} = \bm{\mu}(\bm{\beta})\) or
       \(E\{(\mathbf{h}(\mathbf{Y}))\} = \bm{\psi}(\bm{\beta})\)
       (implicitly involving \(\sigma^{2}\)).
\end{enumerate}

### A class of likelihood functions

\begin{description}
 \item[If \(\sigma^{2}\) is known,] log-likelihood is an exponential
                                    family
  \vspace{0.15cm}\newline
  \hspace*{0.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
  variance and all higher order cumulants of \(\mathbf{Y}\) are\newline
  \hspace*{0.82cm}functions of the mean vector alone
  \vspace{0.15cm}\newline
  \hspace*{1cm}\rotatebox[origin=c]{180}{$\Lsh$}
  exponential, Poisson, multinomial, noncentral hypergeometric\newline
  \hspace*{1.31cm}and partial likelihoods (survival analysis)
  \vspace{0.15cm}\newline
  \hspace*{1.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
  MLE of \(\bm{\beta}\) through weighted least squares\newline
  \hspace*{2cm}\rotatebox[origin=c]{180}{$\Lsh$}
  \(\bm{\mu}\) and \(\sigma^{2}\) are orthogonal\newline
  \hspace*{2.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
  \(\bm{\beta}\) and \(\sigma^{2}\) also orthogonal;
 \bigskip
 \item[If \(\sigma^{2}\) is unknown,] log-likelihood is
          \textcolor{AlertOrange}{not generally} an exponential
          family\newline
          \hspace*{0.5cm}\rotatebox[origin=c]{180}{$\Lsh$}
          However, MLE of \(\bm{\beta}\) still through weighted least
          squares\newline
          \hspace*{1cm}\rotatebox[origin=c]{180}{$\Lsh$}
          \textcolor{UniBlue}{If}
          \(E(\mathbf{Y})\) does not involve \(\sigma^{2}\).
\end{description}

### A class of likelihood functions

\vspace{-0.25cm}
\begin{minipage}{13cm}
 \begin{block}{Least square equations}
  \[
   \def\hat{\mathaccent "705E\relax}
   \mathbf{D}^{\top}\mathbf{V}^{-}
   \{\mathbf{y} - \bm{\mu}(\hat{\bm{\beta}})\} = \mathbf{0}, \qquad
   \text{for the parameters in} \quad
   E(\mathbf{Y}) = \bm{\mu}(\bm{\beta})
  \]
  \begin{multicols}{2}
   \begin{itemize}
    \item \(\mathbf{D} = d\bm{\mu}/d\bm{\beta}, \quad N \times p\);
    \item \(\mathbf{V}^{-}\) is a generalized inverse of \(\mathbf{V}\).
   \end{itemize}
  \end{multicols}
  \vspace{-0.15cm}
  Why its name?
  \vspace{0.15cm}
  \begin{enumerate}
   \item \textbf{Geometrical interpretation}:
         projections of the residual vector
         \(\def\hat{\mathaccent "705E\relax}
           \mathbf{y} - \bm{\mu}(\hat{\bm{\beta}}_{0})\) on to the
         tangent space of the solution locus \(\bm{\mu}(\bm{\beta})\);
   \vspace{0.15cm}
   \item These equations \textbf{do not} depend on \(\sigma^{2}\).
  \end{enumerate}
 \end{block}
\end{minipage}

\vspace{-0.1cm}
\begin{center}
 \begin{minipage}{12cm}
  \begin{block}{Newton-Raphson method}
   We replace the second derivative matrix by its expected value,
   \(\mathbf{D}^{\top}\mathbf{V}^{-}\mathbf{D}\)
   \[
    \def\hat{\mathaccent "705E\relax}
    \hat{\bm{\beta}}_{1} - \hat{\bm{\beta}}_{0} =
    (\mathbf{D}^{\top}\mathbf{V}^{-}\mathbf{D})^{-1}
    \mathbf{D}^{\top}\mathbf{V}^{-}(\mathbf{y} - \hat{\bm{\mu}}_{0}).
   \]
  \end{block}
 \end{minipage}
\end{center}

# Quasi-likelihood functions

### Quasi-likelihood functions

\begin{minipage}{14.5cm}
 \begin{block}{Reversing the natural order of assumptions}
  \begin{enumerate}
   \item Instead of taking the log-likelihood to be of the exponential
         family form and \textcolor{AlertOrange}{then} deriving its
         moments;
          
   \vspace{0.15cm}
   \item We begin with the moments and \textcolor{AlertOrange}{then}
         attempt to reconstruct the log-likelihood.
  \end{enumerate}
  \vspace{0.3cm}
  \[
   \text{The reconstituted function is called a
   \textbf{quasi-likelihood}}.
  \]
 \end{block}
\end{minipage}

\vspace{0.15cm}
\begin{columns}
\column{9cm}
The log-quasi-likelihood, function of \(\bm{\mu}\),\newline
is given by the system of partial differential equations
\column{6cm}
\begin{block}{}
 \[
  \frac{\partial \ell(\bm{\mu}; \mathbf{y})}{\partial \bm{\mu}} =
  \mathbf{V}^{-}(\bm{\mu})(\mathbf{y} - \bm{\mu}).
 \]
\end{block}
\end{columns}

Which extends \textbf{Wedderburn's (1974)} definition.

\begin{enumerate}
 \item We get \(\def\hat{\mathaccent "705E\relax} \hat{\bm{\beta}}\)
       from \(\def\hat{\mathaccent "705E\relax}
              \mathbf{D}^{\top}\mathbf{V}^{-}
              \{\mathbf{y} - \bm{\mu}(\hat{\bm{\beta}})\} = \mathbf{0}\)
       (\textbf{generalized least squares equations});
 \item There is no guarantee that
       \(\def\hat{\mathaccent "705E\relax} \hat{\bm{\beta}}\)
       is the MLE.
\end{enumerate}

# Properties of quasi-likelihood functions

### Properties of quasi-likelihood functions

\vspace{-0.15cm}
\begin{minipage}{13.5cm}
 \textit{Very similar to those of ordinary likelihoods
  \textcolor{AlertOrange}{except} that the nuisance parameter,
  \(\sigma^{2}\), when unknown, is treated separately from
  \(\bm{\beta}\) and is not estimated by weighted least squares.
}
\end{minipage}

\vspace{0.15cm}
The principal results fall into three classes:

\begin{enumerate}
 \item Those concerning the score function
       \(\mathbf{U}_{\beta} = \partial\ell / \partial\bm{\beta}\);
  \begin{block}{}
   \vspace{-0.5cm}
   \[
    \mathbf{U}_{\beta} =
    \mathbf{D}^{\top}\mathbf{V}^{-}(\mathbf{Y} - \bm{\mu}(\bm{\beta}))
    \quad \text{has zero mean and covariance matrix} \quad
    \sigma^{2}\mathbf{i}_{\beta} =
    \sigma^{2}\mathbf{D}^{\top}\mathbf{V}^{-}\mathbf{D}
   \]
  \end{block}
  \vspace{-0.15cm}
  where \(\mathbf{i}_{\beta}\) is the expected second derivative matrix
  of \(\ell(\bm{\mu}(\bm{\beta});\mathbf{Y})\).
  
 \vspace{0.15cm}
 \item Those concerning the estimator
       \(\def\hat{\mathaccent "705E\relax} \hat{\bm{\beta}}\);
  \begin{block}{}
   \vspace{-0.15cm}
   \[
    \def\hat{\mathaccent "705E\relax}
    \text{There exists a}~\hat{\bm{\beta}}~\text{satisfying}~
    \hat{\bm{\beta}} - \bm{\beta} =
    \mathbf{I}_{\beta^{\ast}}^{-1}\mathbf{U}_{\beta} \quad
    \text{(minimum asymptotic variance)}
   \]
  \end{block}
  \vspace{-0.15cm}
  where \(\mathbf{I}_{\beta}\) is the observed matrix of second
  derivatives\newline
  and \(\bm{\beta}^{\ast}\) is a point lying on the line segment joining
  \(\def\hat{\mathaccent "705E\relax}
    \hat{\bm{\beta}}~\text{and}~\bm{\beta}\).
\end{enumerate}

### Properties of quasi-likelihood functions

3. Those concerning the distribution of the quasi-likelihood-ratio
   statistic.
\begin{block}{}
 \vspace{-0.15cm}
 \[
  \def\hat{\mathaccent "705E\relax}
  2\ell(\hat{\bm{\beta}}; \mathbf{Y}) - 2\ell(\bm{\beta}; \mathbf{Y}) =
  \mathbf{U}_{\beta}^{\top}\mathbf{i}_{\beta}^{-1}\mathbf{U}_{\beta} +
  O_{p}(N^{-1/2}) \quad
  \text{is asymptotically} \quad \sigma^{2}\chi_{p}^{2}
 \]
\end{block}

The \textbf{asymptotic optimality} follows the same line\newline
as the optimal property of Gauss-Markov estimators.

The conclusions (asymptotic unbiasedness),

+ while they apply \textcolor{AlertOrange}{more widely} than the
  Gauss-Markov theorem,\newline
  are inevitably a \textcolor{AlertOrange}{little weaker} being
  asymptotic rather than exact.

# Estimation of \(\sigma^{2}\)

### Estimation of \(\sigma^{2}\)

In the absense of information beyond the second moments:
\begin{block}{}
 \vspace{-0.1cm}
 \[
  \def\tilde{\mathaccent "707E\relax}
  \def\hat{\mathaccent "705E\relax}
  \tilde{\sigma}^{2} =
  \frac{(\mathbf{y} - \hat{\bm{\mu}})^{\top}\mathbf{V}^{-}
        (\mathbf{y} - \hat{\bm{\mu}})}{N - p} =
  \frac{X^{2}}{N - p}
 \]
\end{block}
\vspace{-0.15cm}
where \(X^{2}\) is a generalized form of Pearson's statistic.

\vspace{0.15cm}
\textcolor{UniBlue}{If} the log-likelihood is in the exponential
family,\newline
an estimate can be obtained by equating the observed deviance
\[
 \def\hat{\mathaccent "705E\relax}
 d(\mathbf{y}; \hat{\bm{\mu}}) = 2\ell(\mathbf{y}; \mathbf{y}) -
                                 2\ell(\hat{\bm{\mu}}; \mathbf{y}),
 ~\text{to its approximate expected value}.
\]

\begin{description}
 \item[Advantage:] asymptotically independent of
                   \(\def\hat{\mathaccent "705E\relax}
                     \hat{\bm{\beta}}\);
 
 \item[Disadvantage:] the expectation of
                      \(\def\hat{\mathaccent "705E\relax}
                        d(\mathbf{Y}; \hat{\bm{\mu}})\) is often
                      difficult to compute.
\end{description}

# Examples of quasi-likelihood functions

### Examples of quasi-likelihood functions

\begin{description}
 \item[Least squares.] \(\mathbf{V}(\bm{\mu}) = \mathbf{V}\);

 \item[Uncorrelated observations.]
  \(\mathbf{V}(\bm{\mu}) = \text{diag}\{\mathbf{v}(\bm{\mu}_{1}),
                                        \mathbf{v}(\bm{\mu}_{2}),
                                        \dots,
                                        \mathbf{v}(\bm{\mu}_{N})\}\);
 
 \item[Invariance under \textcolor{AlertOrange}{linear}
       transformations.]
  \[
   \mathbf{Y}_{L} = \mathbf{LY}, \quad
   \bm{\mu}_{L} = \mathbf{L}\bm{\mu}, \quad
   \mathbf{V}_{L} = \mathbf{LVL}^{\top},
  \]
  
  \vspace{-0.3cm}
  with \(\mathbf{L}\) being a nonsingular matrix of order \(N\).
 
  \begin{itemize}
   \item Weaker than the corresponding result for log-likelihoods which
         are invariant under all \textcolor{AlertOrange}{invertible}
         transformations;
  \end{itemize}
 
 \item[Multinomial response models.] Logistic regression, e.g.;
 
 \item[Models with constant coefficient of variation.]
 
 \item[Over-dispersion.]
  Quasi-likelihood for over-dispersed discrete observations.
\end{description}

# A higher order theory

### A higher order theory

\vspace{0.5cm}

+ Basically, there are problems if the exponential family is curved,
  \newline
  conditioning on ancillary statistics.

\bigskip
\begin{description}
 \item[Acknowledgments.]
  McCullagh says thanks to Professor D. R. Cox and\newline
            \textbf{Dr. B. Jorgensen} (patr√£o's PhD advisor, nice!).
\end{description}

\vspace{1cm}
\Large
\textcolor{UniBlue}{\textbf{THANKS FOR WATCHING AND HAVE A GREAT DAY}}

\normalsize
\vspace{1cm}
\begin{columns}
 \column{10cm}
 \column{5cm}
  \includegraphics[height=0.3cm,angle=90]{../../laurence.jpg}
  \hspace{0.05cm}
  \href{https://henriquelaureano.github.io}{henriquelaureano.github.io}
\end{columns}
