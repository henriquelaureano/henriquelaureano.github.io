\documentclass[12pt, oldfontcommands]{article}
\usepackage[english]{babel}
%\usepackage[brazilian, brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage[top = 2cm, left = 2cm, right = 2cm, bottom = 2cm]{geometry}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{multicol}
\usepackage{vwcol}
\usepackage[normalem]{ulem}
\usepackage{breqn}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{xcolor}
\usepackage{cancel}
\setlength\parindent{0pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\newcommand{\Perp}{\perp \! \! \! \perp}
\newcommand{\nPerp}{\cancel{\perp \! \! \! \perp}}

\title{  
 \normalfont \normalsize 
 \textsc{CS 229 - Machine Learning} \\
 Xiangliang Zhang \\
 Computer Science (CS)/Statistics (STAT) Program \\
 Computer, Electrical and Mathematical Sciences \& Engineering (CEMSE) Division \\
 King Abdullah University of Science and Technology (KAUST) \\[25pt]
 \horrule{.5pt} \\ [.4cm]
 \LARGE HOMEWORK \\
  IV
 \horrule{2pt} \\[ .5cm]}
 
\author{Henrique Aparecido Laureano}
\date{\normalsize Spring Semester 2018}

\begin{document}

\maketitle

%\newpage

\vspace{\fill}

\tableofcontents

\horrule{1pt} \\

\newpage

<<setup, include=FALSE>>=
# <r code> ===================================================================== #
library(knitr)

tema <- knit_theme$get("acid")

knit_theme$set(tema)

opts_chunk$set(size='small'
               , cache=TRUE
               , cache.path='cache/'
               , comment=NA
               , warning=FALSE
               , message=FALSE
               , fig.align='center'
               , dpi=100
               , fig.path='iBagens/'
               , fig.pos='H'
               , background='#ffffff'
               , results='hold'
               , fig.show='hold')
# </r code> ==================================================================== #
@

\section*{Question 1: Fisher's criterion (Exercise 4.5 of Bishop's book)}
\addcontentsline{toc}{section}{Question 1}

\horrule{1pt} \\

\textbf{By making use of (4.20), (4.23), and (4.24), show that the Fisher
        criterion (4.25) can be written in the form (4.26).} \\

\underline{Solution:}

\begin{align*}
 \text{Fisher criterion}: \quad
 J(\mathbf{w}) & = \frac{(m_{2} - m_{1})^{2}}{s_{1}^{2} + s_{2}^{2}} \tag{4.25} \\
 & = \frac{(\mathbf{w}^{\top} \mathbf{m}_{2} -
            \mathbf{w}^{\top} \mathbf{m}_{1})^{2}}{
      \sum_{n \in C_{1}} (\mathbf{w}^{\top} \mathbf{x}_{n} -
                          \mathbf{w}^{\top} \mathbf{m}_{1})^{2} +
      \sum_{n \in C_{2}} (\mathbf{w}^{\top} \mathbf{x}_{n} -
                          \mathbf{w}^{\top} \mathbf{m}_{2})^{2}},
\end{align*}

applying

\begin{align*}
 m_{k} & = \mathbf{w}^{\top} \mathbf{m}_{k}, \tag{4.23} \\
 s_{k}^{2} & = \sum_{n \in C_{k}} (y_{n} - m_{k})^{2} \quad \text{ and} \tag{4.24}
 \\
 y & = \mathbf{w}^{\top} \mathbf{x}. \tag{4.20}
\end{align*}

Returning to \(J\),

\begin{align*}
 J(\mathbf{w}) & =\frac{(\mathbf{w}^{\top} \mathbf{m}_{2} -
                         \mathbf{w}^{\top} \mathbf{m}_{1})^{2}}{
                   \sum_{n \in C_{1}} (\mathbf{w}^{\top} \mathbf{x}_{n} -
                                       \mathbf{w}^{\top} \mathbf{m}_{1})^{2} +
                   \sum_{n \in C_{2}} (\mathbf{w}^{\top} \mathbf{x}_{n} -
                                       \mathbf{w}^{\top} \mathbf{m}_{2})^{2}} \\
               & = \frac{\mathbf{w}^{\top}
                         (\mathbf{m}_{2} - \mathbf{m}_{1})
                         (\mathbf{m}_{2} - \mathbf{m}_{1})^{\top}
                         \mathbf{w}}{
                    \mathbf{w}^{\top} \Big[
                    \sum_{n \in C_{1}}
                    (\mathbf{x}_{n} - \mathbf{m}_{1})
                    (\mathbf{x}_{n} - \mathbf{m}_{1})^{\top} +
                    \sum_{n \in C_{2}}
                    (\mathbf{x}_{n} - \mathbf{m}_{2})
                    (\mathbf{x}_{n} - \mathbf{m}_{2})^{\top}
                    \Big] \mathbf{w}} \\
               & = \frac{\mathbf{w}^{\top} \mathbf{S}_{\rm B} \mathbf{w}}{
                    \mathbf{w}^{\top} \mathbf{S}_{\rm W} \mathbf{w}}, \tag{4.26}
\end{align*}

with

\begin{align*}
 \mathbf{S}_{\rm B} & =
 (\mathbf{m}_{2} - \mathbf{m}_{1}) (\mathbf{m}_{2} - \mathbf{m}_{1})^{\top}
 \quad \text{ and} \\
 \mathbf{S}_{\rm W} & =
 \sum_{n \in C_{1}}
 (\mathbf{x}_{n} - \mathbf{m}_{1}) (\mathbf{x}_{n} - \mathbf{m}_{1})^{\top} +
 \sum_{n \in C_{2}}
 (\mathbf{x}_{n} - \mathbf{m}_{2}) (\mathbf{x}_{n} - \mathbf{m}_{2})^{\top}.
\end{align*}

\hfill \(\square\)

\section*{Question 2: Bayes Net - Proofs}
\addcontentsline{toc}{section}{Question 2}

\horrule{1pt} \\

\textbf{Given this Bayes net}

\begin{figure}[H]
 \centering \includegraphics[width=.5\textwidth]{q2.png}
\end{figure}

\textbf{Prove that C is conditionally independent of A, given B. You should use
        the definition of joint probabilities for a Bayes net.} \\

\underline{Solution:}

\begin{align*}
 p(C, A \mid B) = \frac{p(C, A, B)}{p(B)}
                = \frac{p(A) p(B \mid A) p(C \mid B)}{p(B)}
                = \frac{p(A, B) p(C \mid B)}{p(B)}
              & = p(A \mid B) p(C \mid B) \\
              & \Rightarrow C \Perp A \mid B.
\end{align*}

\hfill \(\square\)

\section*{Question 3: Bayesian Network} \addcontentsline{toc}{section}{Question 3}

\horrule{1pt} \\

\textbf{We are going to take the perspective of an instructor who wants to
        determine whether a student has understood the material, based on the exam
        score. Figure \ref{fig:2} gives a Bayes net for this. As you can see, whether the
        student scores high on the exam is influenced both by whether she is a
        good test taker, and whether she understood the material. Both of those,
        in turn, are influenced by whether she is intelligent; whether she
        understood the material is also influenced by whether she is a hard
        worker.}
        
\setcounter{figure}{1}
\begin{figure}[H]
 \centering \includegraphics[]{q3.png}
 \caption{A Bayesian network representing what influences an exam score.}
  \label{fig:2}
\end{figure}

\subsection*{(1)} \addcontentsline{toc}{subsection}{(1)}

\textbf{Using variable elimination (by hand!), compute the probability that a
        student who did well on the test actually understood the material, that
        is, compute \(\text{P}(\text{+ u} \mid \text{+ e})\).} \\

\underline{Solution:} \\

Renaming the states for convenience:

\begin{multicols}{3}
 \begin{itemize}
  \item Intelligent: I,
  \item Hardworking : H,
  \item good Test taker: T,
  \item Understands material: U,
  \item high Exam score: E.
 \end{itemize}
\end{multicols}

We are interested in

\[
 \mathbb{P}(\text{U} \mid \text{E}) =
 \frac{\mathbb{P}(\text{U}, \text{E})}{\mathbb{P}(\text{E})}
\]

and we have 6 initial factors (giving by the Bayesian network representation in
Figure \ref{fig:2}):

\[
 {\color{red}\mathbb{P}(\text{I})}, \quad
 {\color{blue}\mathbb{P}(\text{H})}, \quad
 {\color{red}\mathbb{P}(\text{T} \mid \text{I})}, \quad
 {\color{red}\mathbb{P}(\text{U} \mid \text{I}, \text{H})}, \quad
 {\color{magenta}\mathbb{P}(\text{E} \mid \text{T}, \text{U})}.
\]

To compute first \(\mathbb{P}(\text{U}, \text{E})\), following the elimination
order \({\color{red}\text{I}}, {\color{blue}\text{H}},
{\color{magenta}\text{T}}\), we have

\begin{align*}
 {\color{red}f_{1}(\text{H}, \text{T}, \text{U})} & =
 \sum_{\rm i} \mathbb{P}(\text{i})
          \mathbb{P}(\text{T} \mid \text{i})
          \mathbb{P}(\text{U} \mid \text{i}, \text{H}) =  0.672 \\
 {\color{blue}f_{2}(\text{T}, \text{U})} & =
 \sum_{\rm h}
 \mathbb{P}(\text{h}) {\color{red}f_{1}(\text{h}, \text{T}, \text{U})} = 0.4032 \\
 {\color{magenta}f_{3}(\text{E}, \text{U})} & =
 \sum_{\rm t}
 \mathbb{P}(\text{E} \mid \text{t}, \text{U})
 {\color{blue}f_{2}(\text{t}, \text{U})} = 0.56448 \quad
 \Rightarrow \mathbb{P}(\text{U}, \text{E}).
\end{align*}

Now, \(\mathbb{P}(\text{E})\):

\begin{align*}
 \mathbb{P}(\text{I}, \text{H}, \text{T}, \text{U}, \text{E}) & =
 \mathbb{P}(\text{I})
 \mathbb{P}(\text{T} \mid \text{I})
 \mathbb{P}(\text{H})
 \mathbb{P}(\text{U} \mid \text{I}, \text{H})
 \mathbb{P}(\text{E} \mid \text{T}, \text{U}) \\
 \mathbb{P}(\text{E}) & =
 \sum_{\rm i} \sum_{\rm h} \sum_{\rm t} \sum_{\rm u}
 \mathbb{P}(\text{I})
 \mathbb{P}(\text{T} \mid \text{I})
 \mathbb{P}(\text{H})
 \mathbb{P}(\text{U} \mid \text{I}, \text{H})
 \mathbb{P}(\text{E} \mid \text{T}, \text{U}) \\
 {\color{cyan}f_{\rm h}(\text{U} \mid \text{I})} & =
 \sum_{\rm h} \mathbb{P}(\text{h}) \mathbb{P}(\text{U} \mid \text{I}, \text{h}) \\
 \mathbb{P}(\text{E}) & =
 \sum_{\rm i} \sum_{\rm t} \sum_{\rm u}
 {\color{cyan}f_{\rm h}(\text{U} \mid \text{I})}
 \mathbb{P}(\text{I})
 \mathbb{P}(\text{T} \mid \text{I})
 \mathbb{P}(\text{E} \mid \text{T}, \text{U}) \\
 {\color{cyan}f_{\rm i}(\text{U}, \text{T})} & =
 \sum_{\rm i}
 \mathbb{P}(\text{i})
 {\color{cyan}f_{\rm h}(\text{U} \mid \text{i})}
 \mathbb{P}(\text{T} \mid \text{i}) \\
 \mathbb{P}(\text{E}) & =
 \sum_{\rm t} \sum_{\rm u}
 {\color{cyan}f_{\rm i}(\text{u}, \text{t})}
 \mathbb{P}(\text{E} \mid \text{t}, \text{u}) \\
 & = {\color{cyan}f_{\text{t, u}}(\text{E})} \\
 & = 0.9.
\end{align*}

Therefore,

\[
 \mathbb{P}(\text{U} \mid \text{E}) =
 \frac{\mathbb{P}(\text{U}, \text{E})}{\mathbb{P}(\text{E})} =
 \frac{0.56448}{0.9} = 0.6272.
\]

The probability that a student who did well on the test actually understood the
material is 0.6272. \\

<<include=FALSE>>=
# <r code> ===================================================================== #
f1 <- sum( c(.7 * .8 * c(.9, .3)) )
f2 <- .6 * f1
f3 <- sum(c(.9, .5) * f2)
# </r code> ==================================================================== #
@

\hfill \(\square\)

\subsection*{(2)} \addcontentsline{toc}{subsection}{(2)}

\textbf{For the above Bayesian network, label the following statements about
        conditional independence as true or false. For this question, you should
        consider only the structure of the Bayesian network, not the specific
        probabilities. Explain each of your answers.}

\begin{description}
 \item[1) T and U are independent.] \hfill \\
 
  False.
  \begin{align*}
   p(t, u, i, h) & = p(i) p(h) p(t \mid i) p(u \mid i, h) \\
   p(t, u) & = \sum_{i} \sum_{h} p(t \mid i) p(i) p(u \mid i, h) p(h) \\
   p(t, u) & = \sum_{i} p(t \mid i) p(i) \sum_{h} p(u \mid i, h) p(h) \\
   p(t, u) & = \sum_{i} p(t \mid i) p(i) p(u \mid i) \\
           & \quad t \nPerp u \mid \varnothing
  \end{align*}
 \item[2)	T and U are conditionally independent given I, E, and H.] \hfill \\
 
  False.
  \begin{align*}
   p(t, u \mid i, e, h) & = \frac{p(t, u, i, e, h)}{p(i, e, h)} \\
   & = \frac{p(i) p(h) p(t \mid i) p(u \mid i, h) p(e \mid t, u)}{
        p(i) p(h) p(e \mid t, u)} \\
   & = p(t \mid i) p(u \mid i, h) \\
   & \quad t \Perp u \mid i, h
  \end{align*}
  T and U are conditionally independent given I and H, not given I, E, and H.
 \item[3)	T and U are conditionally independent given I and H.] \hfill \\
 
  True.
  \begin{align*}
   p(t, u \mid i, h) & = \frac{p(t, u, i, h)}{p(i, h)} \\
   & = \frac{p(i) p(h) p(t \mid i) p(u \mid i, h)}{p(i) p(h)} \\
   & = p(t \mid i) p(u \mid i, h) \\
   & \quad t \Perp u \mid i, h
  \end{align*}
 \item[4)	E and H are conditionally independent given U.] \hfill \\
 
  True.
  \begin{align*}
   p(e, h \mid u) & = \frac{p(e, h, u)}{p(u)} \\
   & = \frac{p(h) p(u \mid i, h) p(e \mid t, u)}{p(u)}
  \end{align*}
  
  Sorry, no enough time to complete the homework, I have weekly homeworks in the
  four courses that I'm doing and this week I was not able to finish this.
  
 \item[5)	E and H are conditionally independent given U, I, and T.]
 \item[6)	I and H are conditionally independent given E.]
 \item[7)	I and H are conditionally independent given T.]
 \item[8)	T and H are independent.]
 \item[9)	T and H are conditionally independent given E.]
 \item[10) T and H are conditionally independent given E and U.]
\end{description}

\hfill \(\square\)

\section*{Question 4: k-nearest neighbor}
\addcontentsline{toc}{section}{Question 4}

\horrule{1pt} \\

\textbf{Write your k-nn code to classify}

\subsection*{(1)} \addcontentsline{toc}{subsection}{(1)}

\textbf{Students in data set in \texttt{test_data_binary.txt} by training data in
\texttt{logreg_data_binary.txt}.}

\subsection*{(2)} \addcontentsline{toc}{subsection}{(2)}

\textbf{Students in data set in \texttt{test_data_3class.txt} by training data in
\texttt{logreg_data_3class.txt}.} \\

\textbf{Report how many student labels in test data are correctly predicted.} \\

\textbf{NOTE: choose an appropriate \(k\) to reach the best prediction.}

\hfill \(\blacksquare\)

\horrule{.5pt}

\vspace{\fill}

\horrule{1pt} \\

\end{document}