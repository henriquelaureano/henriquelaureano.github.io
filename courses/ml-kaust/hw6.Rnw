\documentclass[12pt, oldfontcommands]{article}
\usepackage[english]{babel}
%\usepackage[brazilian, brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage[top = 2cm, left = 2cm, right = 2cm, bottom = 2cm]{geometry}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{multicol}
\usepackage{vwcol}
\usepackage[normalem]{ulem}
\usepackage{breqn}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{xcolor}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage{hyperref}
\setlength\parindent{0pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\newcommand{\Perp}{\perp \! \! \! \perp}
\newcommand{\nPerp}{\cancel{\perp \! \! \! \perp}}

\title{  
 \normalfont \normalsize 
 \textsc{CS 229 - Machine Learning} \\
 Xiangliang Zhang \\
 Computer Science (CS)/Statistics (STAT) Program \\
 Computer, Electrical and Mathematical Sciences \& Engineering (CEMSE) Division \\
 King Abdullah University of Science and Technology (KAUST) \\[25pt]
 \horrule{.5pt} \\ [.4cm]
 \LARGE HOMEWORK \\
  VI
 \horrule{2pt} \\[ .5cm]}
 
\author{Henrique Aparecido Laureano}
\date{\normalsize Spring Semester 2018}

\begin{document}

\maketitle

%\newpage

\vspace{\fill}

\tableofcontents

\horrule{1pt} \\

\newpage

<<setup, include=FALSE>>=
# <r code> ===================================================================== #
library(knitr)

tema <- knit_theme$get("acid")

knit_theme$set(tema)

opts_chunk$set(size='small'
               , cache=TRUE
               , cache.path='cache/'
               , comment=NA
               , warning=FALSE
               , message=FALSE
               , fig.align='center'
               , dpi=100
               , fig.path='iBagens/'
               , fig.pos='H'
               , background='#ffffff'
               , results='hold'
               , fig.show='hold')
# </r code> ==================================================================== #
@

\section*{Question 1: Property of derivatives of Error function \\
          \vspace{0cm} \hfill (Exercise 5.6 and 5.7 of Bishop’s book)}
\addcontentsline{toc}{section}{Question 1}

\horrule{1pt}

\subsection*{(1)} \addcontentsline{toc}{subsection}{(1)}

\horrule{.5pt} \\

\textbf{Show the derivative of the error function}

\[
 E(\mathbf{w}) = -\sum_{n = 1}^{N} \{t_{n} \ln y_{n} +
                                     (1 - t_{n}) \ln (1 - y_{n})
                                   \} \tag{5.21}
\]

\textbf{with respect to the activation \(a_{k}\) for an output unit having a
        logistic sigmoid activation function}

\[ y_{k} = \frac{1}{1 + \exp(-a_{k})} \]

\textbf{satisfies}

\[ \frac{\partial E}{\partial a_{k}} = y_{k} - t_{k}. \tag{5.18} \]

\underline{Solution}:

\begin{align*}
 \frac{\partial E}{\partial a_{k}} & =
 -\bigg[\frac{t_{k}}{y_{k}} \frac{\partial y_{k}}{\partial a_{k}} +
        \frac{1 - t_{k}}{1 - y_{k}} \frac{\partial (-y_{k})}{\partial a_{k}}
  \bigg] \\
 & = -\bigg[\frac{t_{k}}{y_{k}} \frac{\exp(-a_{k})}{(1 + \exp(-a_{k}))^{2}} -
           \frac{1 - t_{k}}{1 - y_{k}} \frac{\exp(-a_{k})}{(1 + \exp(-a_{k}))^{2}}
      \bigg] \\
 & = -\bigg[t_{k} (1 + \exp(-a_{k})) \frac{\exp(-a_{k})}{(1 + \exp(-a_{k}))^{2}} -
           \frac{1 - t_{k}}{\frac{\exp(-a_{k})}{1 + \exp(-a_{k})}}
           \frac{\exp(-a_{k})}{(1 + \exp(-a_{k}))^{2}}
      \bigg] \\
 & = -\bigg[t_{k} \frac{\exp(-a_{k})}{1 + \exp(-a_{k})} -
            (1 - t_{k}) \frac{1 + \exp(-a_{k})}{\exp(-a_{k})}
            \frac{\exp(-a_{k})}{(1 + \exp(-a_{k}))^{2}}
      \bigg] \\
 & = -\bigg[t_{k} \frac{\exp(-a_{k})}{1 + \exp(-a_{k})} -
            \frac{1 - t_{k}}{1 + \exp(-a_{k})}
      \bigg] \\
 & = -\bigg[t_{k} (1 - y_{k}) - (1 - t_{k}) y_{k}\bigg] \\
 & = -\Big[t_{k} - y_{k}\Big] \\
 & = \boxed{y_{k} - t_{k}.}
\end{align*}

\hfill \(\square\)

\subsection*{(2)} \addcontentsline{toc}{subsection}{(2)}

\horrule{.5pt} \\

\textbf{Show the derivative of the error function}

\[
 E(\mathbf{w}) = -\sum_{n = 1}^{N} \sum_{k = 1}^{K}
                 t_{nk} \ln y_{nk} (\mathbf{x}_{n}, \mathbf{w}) \tag{5.24}
\]

\textbf{with respect to the activation \(a_{k}\) for output units having a
        softmax activation function}

\[
 y_{k} (\mathbf{x}, \mathbf{w}) =
 \frac{\exp (a_{k} (\mathbf{x}, \mathbf{w}))}{
       \sum_{j} \exp (a_{j} (\mathbf{x}, \mathbf{w}))}
\]

\textbf{satisfies}

\[ \frac{\partial E}{\partial a_{k}} = y_{k} - t_{k}. \tag{5.18} \]

\begin{description}
 \item[Hint]:
  \textbf{for each \(\mathbf{x}_{n}\), its true label has \(t_{nk} = 0\) or \(1\),
          and \(\sum_{k = 1}^{K} t_{nk} = 1\). That is to say, for the activation
          \(a_{k}\) activated by an \(\mathbf{x}\), the corresponding \(t_{k}\)
          could be either \(1\) or \(0\).}
\end{description}

\underline{Solution}:

\[
 \frac{\partial E}{\partial a_{k}} = -\sum_{n = 1}^{N} t_{n}
           \frac{\partial \ln y_{n} (\mathbf{x}_{n}, \mathbf{w})}{\partial a_{k}}.
\]

%Expressing \(y_{n} (\mathbf{x}_{n}, \mathbf{w})\) as

\[
 y_{n} (\mathbf{x}, \mathbf{w}) =
 \frac{\exp (a_{n} (\mathbf{x}, \mathbf{w}))}{
       \sum_{j} \exp (a_{j} (\mathbf{x}, \mathbf{w}))} \quad \Rightarrow \quad
 \ln y_{n} (\mathbf{x}_{n}, \mathbf{w}) = a_{n} (\mathbf{x}, \mathbf{w}) -
 \ln \sum_{j} \exp (a_{j} (\mathbf{x}, \mathbf{w}))
\]

we have

\[
 \frac{\partial \ln y_{n} (\mathbf{x}_{n}, \mathbf{w})}{\partial a_{k}} =
 \delta_{nk} - \frac{1}{\sum_{j} \exp (a_{j} (\mathbf{x}, \mathbf{w}))}
   \frac{\partial \sum_{j} \exp (a_{j} (\mathbf{x}, \mathbf{w}))}{\partial a_{k}},
\]

with \(\delta_{nk}\) being the Kronecker delta (1 or 0). \\

Then the gradient of the softmax-denominator is

\[
 \frac{\partial \sum_{j} \exp (a_{j} (\mathbf{x}, \mathbf{w}))}{\partial a_{k}} =
 \sum_{j} \exp (a_{j} (\mathbf{x}, \mathbf{w})) \delta_{jk} =
 \exp (a_{k} (\mathbf{x}, \mathbf{w}))
\]

which gives

\[
 \frac{\partial \ln y_{n} (\mathbf{x}_{n}, \mathbf{w})}{\partial a_{k}} =
 \delta_{nk} - \frac{\exp (a_{k} (\mathbf{x}, \mathbf{w}))}{
                \sum_{j} \exp (a_{j} (\mathbf{x}, \mathbf{w}))} =
 \delta_{nk} - y_{k}.
\]

So the gradient of \(E\) with respect to \(a_{k}\) is then

\[
 \frac{\partial E}{\partial a_{k}} =
 \sum_{n = 1}^{N} t_{n} (y_{k} - \delta_{nk}) =
 y_{k} \Big(\sum_{n = 1}^{N} t_{n}\Big) - t_{k} = y_{k} \cdot 1 - t_{k} =
 \boxed{y_{k} - t_{k}}.
\]

\hfill \(\square\)

\section*{Question 2: A different error function \\
          \vspace{0cm} \hfill (Exercise 5.9 of Bishop’s book)}
\addcontentsline{toc}{section}{Question 2}

\horrule{1pt} \\

\textbf{The error function}

\[
 E(\mathbf{w}) = -\sum_{n = 1}^{N} \{t_{n} \ln y_{n} +
                                     (1 - t_{n}) \ln (1 - y_{n})
                                   \} \tag{5.21}
\]

\textbf{for binary classification problems was derived for a network having a
        logistic-sigmoid output activation function, so that
        \(0 \leq y(x,w) \leq 1\), and data having target values
        \(t \in \{0, 1\}\). \textit{Derive the corresponding error function} if
        we consider a network having an output \(-1 \leq y(x, w) \leq 1\) and
        target values \(t = 1\) for class C1 and \(t = -1\) for class C2. What
        would be the appropriate
        \textit{choice of output unit activation function}?} \\

\underline{Solution}: \\

Scaling and shifting the binary outputs directly gives the activation function
(using the motation from (5.19))

\[ y = 2\sigma(a) - 1. \]

The error function is constructed from (5.21) by applying the inverse
transformation to \(y_{n}\) and \(t_{n}\)

\begin{align*}
 E(\mathbf{w}) & =
 -\sum_{n = 1}^{N} \frac{1 + t_{n}}{2} \ln \frac{1 + y_{n}}{2} +
                   \bigg(1 - \frac{1 + t_{n}}{2}\bigg)
                   \ln \bigg(1 - \frac{1 + y_{n}}{2}\bigg) \\
               & = \boxed{-\frac{1}{2}
                          \sum_{n = 1}^{N} \{(1 + t_{n}) \ln (1 + y_{n}) +
                                             (1 - t_{n}) \ln (1 - y_{n})
                                           \}} + N \ln 2
\end{align*}

(the last term can be dropped, since it is independent of \(\mathbf{w}\)). \\

To find the activation function we apply the linear transformation to the logistic
sigmoid given by (5.19), which gives

\begin{align*}
 y = 2\sigma(a) - 1 & = \frac{2}{1 + \exp(-a)} - 1 \\
                    & = \frac{1 - \exp(-a)}{1 + \exp(-a)} \\
                    & = \frac{\exp(a/2) - \exp(-a/2)}{\exp(a/2) + \exp(-a/2)} \\
                    & = \boxed{\tanh(a/2).} \qquad (\text{hyperbolic tangent})
\end{align*}

\hfill \(\square\)

\section*{Question 3: Implementation of NN (using Back-Propagation)}
\addcontentsline{toc}{section}{Question 3}

\horrule{1pt}

\begin{description}
 \item[Data]:
  \textbf{Generate a set of data \textit{points} \((\mathbf{x}, \mathbf{y})\), by
          choosing a \textit{nonlinear function} \(f(\mathbf{x})\) and evaluating
          \(\mathbf{y} = f(\mathbf{x}) + \textbf{noise}\) for \textit{random}
          \(\mathbf{x}\) values, where each \(\mathbf{x}\) is a
          \textit{real vector of at least 2 elements} and each \(\mathbf{y}\)
          \textit{is a real scalar or vector}. As an alternative, you can use a
          data set you find online or are using in other research. You should
          clearly state what your data set is, or how you generated it.} \\
          
  Here I'm using a subset of cereal dataset shared by Carnegie Mellon University
  (CMU). The details of the dataset are on the following link: \\
  http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html. The objective is to predict
  rating of the cereals variables such as calories, proteins, fat etc. See the
  range of values and behavior of the variables in Figure \ref{fig:da}). The data
  is in .csv format and can be downloaded by clicking:
  \href{https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/09/07122416/cereals.csv}{cereals}.
<<da, fig.height=4.25, fig.cap="relationships between the rating, y, and the x's.">>=
# <r code> ===================================================================== #
path <- "~/Dropbox/KAUST/machine_learning/hw6/"                  # files directory
df <- read.csv(paste0(path, "cereals.csv"), header = TRUE)       # loading dataset
par(mfrow = c(2, 3), mar = c(4, 4, 0, 1) + .1)             # graphical definitions
                                                              # plotting variables
for (i in 1:5) plot(rating ~ df[ , i], xlab = names(df)[i], df)
# </r code> ==================================================================== #
@
<<>>=
# <r code> ===================================================================== #
                    # sampling random index to separate the data in train and test
random <- sample(75, 50)                         # 50 values for the train dataset
train <- df[random, ] ; test <- df[-random, ]
                                                  # applying z-score normalization
train.std <- apply(train, 2, function(x) (x - mean(x)) / sd(x))
test.std <- test
for (i in 1:6)
  test.std[ , i] <- (test.std[ , i] - mean(train[ , i])) / sd(train[ , i]) 
# </r code> ==================================================================== #
@
 \item[Task]:
  \textbf{Implement a two-layer neural network with back-propagation. \\
          The network should have \textit{2 or more inputs}. The inputs connect to
          \textit{M} neurons in the \textit{hidden layer}, each of which takes a
          weighted sum of its inputs plus a bias and then applies the
          \textit{hyperbolic tangent function (as the activation function)}. \\
          The network should have 1 or more outputs. Each output of the network is
          a weighted sum plus bias of the outputs of the hidden layer (need no
          activation function or use \textit{identity function}
          \(f(\mathbf{x}) = \mathbf{x}\) for the \textit{output} because this is a
          regression problem). \\
          During learning, both weights and biases change to decrease the mean
          squared error.}
\end{description}

\subsection*{1)} \addcontentsline{toc}{subsection}{1)}

\horrule{.5pt} \\

\textbf{\textit{Describe all the parameters} you chose, including the number of
        inputs, outputs, and hidden neurons, the sizes of the initial random
        weights, learning rate etc.}

\begin{description}
 \item[Solution/Implementation]:
<<>>=
# <r code> ===================================================================== #
                                                     # hyperbolic tangent function
htan <- function(x) (exp(x) - exp(-x)) / (exp(x) + exp(-x))

prop.for <- function(x, w1, w2) {                            # forward propagation
  z1 = cbind(1, x) %*% w1
  h = htan(z1)
  z2 = h %*% w2
  list(output = z2, h = h)
}                                                               
                                                                # back-propagation
backpropagate <- function(x, target, y, h, w1, w2, learn.rate) {
  delta = y - target
  dw1 = t(t(delta %*% t(w2) %*% (1 - crossprod(h))) %*% cbind(1, x))
  dw2 = t(delta) %*% h

  w1 = w1 - learn.rate * dw1
  w2 = w2 - learn.rate * t(dw2)
  
  list(w1 = w1, w2 = w2)
}
      # neural network: 10 hidden neurons, learn rate of 0.001 and 2000 iterations
neuralnet <- function(x, target, hidden = 10, learn.rate = 1e-3, iter = 2e3) {
  d = ncol(x) + 1
  w1 = matrix(rnorm(d * hidden), d, hidden)
  w2 = as.matrix(rnorm(hidden))
  error = matrix(NA, 2, iter)
  for (i in 1:iter) {
    pf = prop.for(x, w1, w2)
    bp = backpropagate(x, target, y = pf$output, h = pf$h, w1, w2
                       , learn.rate = learn.rate)
    w1 <- bp$w1 ; w2 <- bp$w2
    error[1, i] = (1/length(target)) * sum((pf$output - target)**2)
    error[2, i] = sd(abs(pf$output - target))
  }
  list(output = pf$output, w1 = w1, w2 = w2, error = error)
}

x <- data.matrix(train.std[ , -6]) ; target <- train.std[ , 6]
                                        # five inputs of size 50 plus an intercept
                                                        # vector output of size 50
                 # initial random weights generated by a normal of mean 0 and sd 1
                        # w_{1}: matrix of dim 6 x 10, w_{2}: matrix of dim 1 x 10
nn.train <- neuralnet(x, target)
# </r code> ==================================================================== #
@
\end{description}

\hfill \(\square\)

\subsection*{2)} \addcontentsline{toc}{subsection}{2)}

\horrule{.5pt} \\

\textbf{Find a learning rate that allows it to learn to a small mean squared
        error. \textit{Plot a figure of how the error decreases during learning}.}

\begin{description}
 \item[Solution]: \\
 
 The biggest learning rate that allows learning consecutive times is with 0.001.
<<nn-train, fig.height=4.5, fig.cap="Neural Network MSE during learning.">>=
# <r code> ===================================================================== #
plot(nn.train$error[1, ], type = "l", col = 2                   # plotting the MSE
     , main = paste("MSE:", round(nn.train$error[1, 2e3], 3))
     , xlab = "Iteration", ylab = "MSE")
# </r code> ==================================================================== #
@
\end{description}

\hfill \(\square\)

\subsection*{3)} \addcontentsline{toc}{subsection}{3)}

\horrule{.5pt} \\

\textbf{\textit{Test} the NN you learned by a \textit{different} set of data
        \textit{points} \((\mathbf{x}, \mathbf{y})\) (different from training set,
        but \(\mathbf{y}\) is still generated by
        \(f(\mathbf{x}) + \textbf{noise}\)), what’s the error when comparing the
        \textit{predicted} \(y_{n}\) with the \textit{true target} \(y\)? Give the
        mean and standard deviation of errors.}

\begin{description}
 \item[Solution]:
<<results='markup'>>=
# <r code> ===================================================================== #
x <- data.matrix(test.std[ , -6]) ; target <- test.std[ , 6]        # test dataset
                                        # five inputs of size 25 plus an intercept
                                                        # vector output of size 25
                                                          # testing neural network
target.test <- prop.for(x, w1 = nn.train$w1, w2 = nn.train$w2)$output

(1/length(target)) * sum( (target.test - target)**2 )                  # MSE error
mean( abs(target.test - target) )                                     # error mean
sd( abs(target.test - target) )                         # error standard deviation
# </r code> ==================================================================== #
@
\end{description}

\hfill \(\square\)

\subsection*{4)} \addcontentsline{toc}{subsection}{4)}

\horrule{.5pt} \\

\textbf{How will the training error and testing error be different if you re-train
        the NN by different initializations of weights? And how will they change
        if you set \textit{M} (the number of hidden units) to be different values?
        \textit{Plot the error bars}
        (as examples of http://www.mathworks.com/help/techdoc/ref/errorbar.html)
        \textit{for both training and testing}.}

\begin{description}
 \item[Solution]: \\
 
 How will the training error and testing error be different if you re-train the
 NN by different initializations of weights? \\
 Maybe a little different, but in average very similar. Doing this we only put
 different start points. The ideia of the algorithm (gradient descent) is to find
 the same/desired region, even with different start points (the method is robust),
 if the same data is used. \\
 
 And how will they change if you set \textit{M} (the number of hidden units) to be
 different values? \\
 Also maybe a little different, but in average very similar. Generally, a
 \textit{M} bigger than the number of input nodes is already good enough to reach
 similar results.
 
<<errorbar, fig.height=3.5, fig.cap="Left: Error bars (plus/minus standard deviation of errors at different iterations) for the MSE in the training. Right: Error mean with standard deviation error bar for the test.">>=
# <r code> ===================================================================== #
                                                         # plotting the error bars
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1) + .1)             # graphical definitions

plot(nn.train$error[1, ], type = "l", col =  2         # plotting the training MSE
     , main = "Training", xlab = "Iteration", ylab = "MSE", ylim = c(.25, 6.65))

xs <- seq(250, 2000, 250) ; ys <- nn.train$error[1, xs]
arrows(xs, ys - nn.train$error[2, xs], xs, ys + nn.train$error[2, xs] # error bars
       , length = .05, angle = 90, code = 3, col = 2)

                                            # plotting the testing error/error bar
error.mean <- mean(abs(target.test - target))
error.sd <- sd(abs(target.test - target))

plot(error.mean, pch = 19, col = 2, ylim = c(.1, .85), axes = FALSE
     , xlab = NA, ylab = "Error", main = "Test")

Axis(side = 2, at = seq(.1, .85, length.out = 4))
arrows(1, error.mean - error.sd, 1, error.mean + error.sd
       , length = .05, angle = 90, code = 3, col = 2)
# </r code> ==================================================================== #
@
\end{description}

\hfill \(\square\)

\section*{An Additional Exercise \\
          \vspace{0cm} \hfill (\textit{a bonus} for those who still have time, \\
          \vspace{0cm} \hfill energy and interest to work)}
\addcontentsline{toc}{section}{An Additional Exercise}

\horrule{1pt} \\

\textbf{Implement the \textit{momentum} discussed in class. \\
        Run it and see if it learns faster, or learns a better solution (in terms
        of testing error). Describe whether it improved speed, accuracy, or both,
        and why you think that occurred. (It’s OK if you discover your
        “improvement” had no effect or even made it worse. The important thing is
        to test it and explain the results).} \\

\underline{Solution}:

\hfill \(\blacksquare\)

\horrule{.5pt}

\vspace{\fill}

\horrule{1pt} \\

\end{document}