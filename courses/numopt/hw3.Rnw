\documentclass[12pt]{article}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{float}
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}

\title{Comparing Unconstrained Optimization Methods}
\date{Spring Semester\\ 2018}
\author{Henrique Ap. Laureano\\ ID 158811\\ \texttt{KAUST}}

\begin{document}

\maketitle

<<setup, include=FALSE>>=
# <r code> ===================================================================== #
library(knitr)

tema <- knit_theme$get("acid")

knit_theme$set(tema)

opts_chunk$set(size='small'
               , cache=TRUE
               , cache.path='cache/'
               , comment=NA
               , warning=FALSE
               , message=FALSE
               , fig.align='center'
               , dpi=100
               , fig.path='iBagens/'
               , fig.pos='H'
               , background='#ffffff'
               , results='hold'
               , fig.show='hold')
# </r code> ==================================================================== #
@

In this problem, we will solve the linear least squares (LLS) problem below:

\[
 \mathbf{x}^{\ast} = \underset{\mathbf{x}}{\arg \min}
                     \parallel \mathbf{Ax} - \mathbf{b} \parallel_{2}^{2}
\]

The matrix \(\mathbf{A}\) and vector \(\mathbf{b}\) are saved in \textit{Ab.mat}
provided with the homework. In what follows, you will implement your own version
of the different unconstrained optimization methods we talked about in class.
Submit all your code. \\

\textbf{Here I'm doing everything in R}.

<<>>=
# <r code> ===================================================================== #
path <- "~/Dropbox/KAUST/numerical_optimization/hw3/"                 # files path

library(R.matlab)                           # loading library to read the datasets
                                  # reading A, the function import as a list class
data_a <- readMat(paste0(path, "data_a.mat"))

a <- matrix(unlist(data_a), 1e4, 1e3)                     # converting to a matrix

data_b <- readMat(paste0(path, "data_b.mat"))                          # reading b

b <- matrix(unlist(data_b), 1e4, 1)                       # converting to a matrix
# </r code> ==================================================================== #
@

\section*{(a)}

Use the stationarity equation to compute \(\mathbf{x}^{\ast}\). What is
\(\parallel \mathbf{Ax}^{\ast} - \mathbf{b} \parallel_{2}\)? Based on this result,
is \(\mathbf{b} \in \mathcal{R}(A)\)? \\

\noindent \underline{Solution}:

\begin{align*}
 f(\mathbf{x}) = \parallel \mathbf{Ax} - \mathbf{b} \parallel_{2}^{2}
             & = (\mathbf{Ax} - \mathbf{b})^{\top} (\mathbf{Ax} - \mathbf{b}) \\
             & = \mathbf{x}^{\top} \mathbf{A}^{\top} \mathbf{Ax}
                 - 2 \mathbf{x}^{\top} \mathbf{A}^{\top} \mathbf{b}
                 + \mathbf{b}^{\top} \mathbf{b}
 \\ \boldsymbol{\bigtriangledown} f(\mathbf{x}) & =
    2 \mathbf{A}^{\top} \mathbf{Ax} - 2 \mathbf{A}^{\top} \mathbf{b}
\end{align*}

\begin{align*}
 \boldsymbol{\bigtriangledown} f(\mathbf{x}) = \mathbf{0} \quad \Rightarrow \quad
 2 \mathbf{A}^{\top} \mathbf{Ax} & = 2 \mathbf{A}^{\top} \mathbf{b} \\
 \mathbf{x}^{\ast} & =
 (\mathbf{A}^{\top} \mathbf{Ax})^{-1} \mathbf{A}^{\top} \mathbf{b}
\end{align*}

<<>>=
# <r code> ===================================================================== #
x.ast <- solve(t(a) %*% a) %*% t(a) %*% b            # computing \mathbf{x}^{\ast}

norm(a %*% x.ast - b, type = "2")                          # computing the L2 norm
# </r code> ==================================================================== #
@

Is \(\mathbf{b} \in \mathcal{R}(A)\)? No, because
\(\parallel \mathbf{Ax}^{\ast} - \mathbf{b} \parallel_{2} = 31.66 > 0\).

\hfill \(\square\)

\section*{(b)}

Implement steepest descent with exact line search and apply it to the LLS problem
above. Initialize at \(\mathbf{x}_{0} = \mathbf{0}\) and stop your descent loop at
iteration \(k\) when
\(\parallel \mathbf{x}^{\ast} - \mathbf{x}_{k} \parallel_{2} \leq
  \parallel \mathbf{x}^{\ast} \parallel_{2} \ast 10^{-6}\). Plot the evolution of
the objective value (in log scale) and plot
\(\parallel \mathbf{x}^{\ast} - \mathbf{x}_{k} \parallel_{2}\) with increasing
\(k\). Discuss your findings. \\

\noindent \underline{Solution}: \\

Steepest descent iteration \(k + 1\) with exact line search:

\[
 \mathbf{x}_{k+1} = \mathbf{x}_{k} -
 \frac{\boldsymbol{\bigtriangledown} f_{k}^{\top}
       \boldsymbol{\bigtriangledown} f_{k}
      }{\boldsymbol{\bigtriangledown} f_{k}^{\top} 2 \mathbf{A}^{\top} \mathbf{A}
        \boldsymbol{\bigtriangledown} f_{k}
       } \boldsymbol{\bigtriangledown} f_{k},
\]

with \(2 \mathbf{A}^{\top} \mathbf{A}\) being the Hessian of \(f(\mathbf{x})\) and
with the step length
\(\alpha_{k} = \frac{\boldsymbol{\bigtriangledown} f_{k}^{\top}
                     \boldsymbol{\bigtriangledown} f_{k}
                    }{\boldsymbol{\bigtriangledown} f_{k}^{\top}
                      2 \mathbf{A}^{\top} \mathbf{A}
                      \boldsymbol{\bigtriangledown} f_{k}
                     }\).

<<fig.height=3.5, fig.cap="Steepest descent with exact line search. (a): The objective value (in log scale) with increasing $k$; (b): $\\parallel \\mathbf{x}^{\\ast} - \\mathbf{x}_{k} \\parallel_{2}$ with increasing $k$.">>=
# <r code> ===================================================================== #
steep.desc_els <- function(x, a, b) {    # steepest descent with exact line search
  x_k = matrix(rep(0, length(x)))    # initializing at \mathbf{x}_{0} = \mathbf{0}
  i = 1                                               # setting iterations counter
                                                         # defining stop criterium
  while (norm(x - x_k[ , i], type = "2") > 1e-6 * norm(x, type = "2")) {
    grad = 2 * t(a) %*% (a %*% x_k[ , i] - b)                 # computing gradient
    alpha = (t(grad) %*% grad) / (2 * t(grad) %*% t(a) %*% a %*% grad)    # \alpha
    x_k = cbind(x_k, x_k[ , i] - as.numeric(alpha) * grad)      # putting together
    i = i + 1                                              # addying new iteration
  }
  return(xks = x_k[ , -1])      # returning the \mathbf{x}_{k}'s of each iteration
}
xks <- steep.desc_els(x.ast, a, b)                          # running the function

iter <- ncol(xks)                                           # number of iterations
                      # empty object to keep the objective value of each iteration
obj.value <- numeric(iter)
                                 # computing the objective value of each iteration
for (i in 1:iter) obj.value[i] = norm(a %*% xks[ , i] - b, type = "2")**2

norms <- numeric(iter)  # empty object to keep the L2 norm value of each iteration
                                         # computing the L2 norm of each iteration
for(i in 1:iter) norms[i] = norm(x.ast - xks[ , i], type = "2")

par(mfrow = c(1, 2), mar = c(4, 4, 3, 2) + .1)             # graphical definitions
plot(obj.value, log = "y", type = "b"
     , xlab = "Iteration", ylab = "Objective value (in log scale)", main = "(a)")
plot(norms, type = "b", xlab = "Iteration", ylab = "L2 norm", main = "(b)")
# </r code> ==================================================================== #
@

We see that the objective value and
\(\parallel \mathbf{x}^{\ast} - \mathbf{x}_{k} \parallel_{2}\) decrease quickly
as \(k\) increase.

<<>>=
# <r code> ===================================================================== #
obj.value[iter]                                            # final objective value
norms[iter]                                                           # final norm
# </r code> ==================================================================== #
@

\hfill \(\square\)

\section*{(c)}

Implement steepest descent with backtracking. Use the same initialization and
stopping criterion as in (b). Plot the evolution of the objective value (in log
scale) and plot \(\parallel \mathbf{x}^{\ast} - \mathbf{x}_{k} \parallel_{2}\)
with increasing \(k\). Compare these results to those of \textbf{(b)} and discuss.
\\

\noindent \underline{Solution}: \\

Steepest descent iteration \(k + 1\):

\[
 \mathbf{x}_{k+1} = \mathbf{x}_{k}
                  - \alpha_{k}
                    \frac{\boldsymbol{\bigtriangledown} f_{k}
                         }{
                       \parallel \boldsymbol{\bigtriangledown} f_{k} \parallel_{2}
                          }, \quad \text{ with the step length }
                             \alpha_{k} \text{ gave by the backtracking line
                                               search}.
\]
\includegraphics[width=.5\textwidth]{algorithm_3-1.png}

<<>>=
# <r code> ===================================================================== #
steep.desc_back <- function (x, a, b) {       # steepest descent with backtracking
  x_k = matrix(rep(0, length(x)))    # initializing at \mathbf{x}_{0} = \mathbf{0}
  i = 1                                               # setting iterations counter
  fn <- function(x, a, b) {                                 # function to compute:
    obj = norm(a %*% x - b, type = "2")**2                          # object value
    grad = 2 * t(a) %*% (a %*% x - b)                                   # gradient
    return(list(obj = obj, grad = grad))
  }                  # function to compute the search direction (steepest descent)
  steep <- function(fn) with(fn, - grad / norm(grad, type = "2"))
  alpha = 1                                           # initializing at \alpha = 1
           # setting \rho and the constant as 0.7 (to be used in the backtracking)
  rho = const = .7
                       # empthy object to keep the object values at each iteration
  obj.value = numeric(1)
                             # empthy object to count how many times the algorithm
  count = numeric(1)         #        enters in the backtracking at each iteration
  norms = numeric(1)           # empthy object to keep the norms at each iteration
                  # computing the object value and the gradient for the first time
  fn_k = fn(x_k[ , i], a, b)
  direc = steep(fn_k)                             # computing the search direction
                             # initializing while loop with the stopping criterion
  while (norm(x - x_k[ , i], type = "2") > 1e-6 * norm(x, type = "2")) {
    while (fn(x_k[ , i] + alpha * direc, a, b)$obj >   # initializing backtracking
           fn_k$obj + const * alpha * t(fn_k$grad) %*% direc) {
                # counting how many times we do the backtracking at each iteration
      count[i] = count[i] + 1
      alpha = rho * alpha               # updating \alpha (doing the backtracking)
    }
    x_k = cbind(x_k, x_k[ , i] + alpha * direc) # computing the new \mathbf{x}_{k}
    i = i + 1                                         # updating iteration counter
    fn_k = fn(x_k[ , i], a, b)           # computing new object value and gradient
    obj.value[i] = fn_k$obj                         # keeping the new object value
    norms[i] = norm(x - x_k[ , i], type = "2")    # computing and keeping the norm
    direc = steep(fn_k)                       # computing the new search direction
    count[i] = 0                 # setting the count to zero for the new iteration
  }
    # returning the \mathbf{x}_{k}'s, object values, norms and backtracking counts
  return(list(x = x_k[ , -1], obj.value = obj.value[-1], norms = norms[-1]
              , count = count))
}                                 # running the steepest descent with backtracking
xks <- steep.desc_back(x.ast, a, b)

tail(xks$obj.value, 1)                                     # final objective value
tail(xks$norms, 1)                                                    # final norm
xks$count              # number of times that we do backtracking at each iteration
# </r code> ==================================================================== #
@

Comparing with \textbf{(b)} - steepest descent with exact line search, we have the
same final results (values). However, with steepest descent with backtracking we
see more iterations with the values decreasing more slowly.

<<fig.height=3.5, fig.cap="Steepest descent with backtracking. (a): The objective value (in log scale) with increasing $k$; (b): $\\parallel \\mathbf{x}^{\\ast} - \\mathbf{x}_{k} \\parallel_{2}$ with increasing $k$.">>=
# <r code> ===================================================================== #
par(mfrow = c(1, 2), mar = c(4, 4, 3, 2) + .1)             # graphical definitions
plot(xks$obj.value, log = "y", type = "b" # plotting the object values (log scale)
     , xlab = "Iteration", ylab = "Objective value (in log scale)", main = "(a)")
                                     # plotting norms with increasing iterations k
plot(xks$norms, type = "b", xlab = "Iteration", ylab = "L2 norm", main = "(b)")
# </r code> ==================================================================== #
@

\hfill \(\square\)

\section*{(d)}

Implement BFGS using backtracking. Use the same initialization and stopping
criterion as in (b). Plot the evolution of the objective value (in log scale) and
plot \(\parallel \mathbf{x}^{\ast} - \mathbf{x}_{k} \parallel_{2}\) with
increasing \(k\). Compare these results to \textbf{(c)} and discuss. \\

\noindent \underline{Solution}: \\

\includegraphics[width=.835\textwidth]{algorithm_6-1.png}

\[
 \text{with } \quad
 \mathbf{H}_{k+1} = (\mathbf{I} - \boldsymbol{\rho}_{k}
                                  \mathbf{s}_{k}
                                  \mathbf{y}_{k}^{\top}) \mathbf{H}_{k}
                    (\mathbf{I} - \boldsymbol{\rho}_{k}
                                  \mathbf{y}_{k}
                                  \mathbf{s}_{k}^{\top})
                  + \boldsymbol{\rho}_{k}
                    \mathbf{s}_{k}
                    \mathbf{s}_{k}^{\top}, \quad \text{ and }
                                                 \boldsymbol{\rho}_{k}
                                                 \text{ defined by }
                    \boldsymbol{\rho}_{k} = \frac{1}{\mathbf{y}_{k}^{\top}
                                                     \mathbf{s}_{k}
                                                    }.
\]

Here the stopping criterion is different (see problem statement) and the step
length \(\alpha_{k}\) is gave by the backtracking line search. \\

\includegraphics[width=.5\textwidth]{algorithm_3-1.png}

<<>>=
# <r code> ===================================================================== #
bfgs_back <- function (x, a, b) {                        # BFGS using backtracking
  h = solve(2 * t(a) %*% a)                               # inverse of the hessian
  obj <- function(x, a, b) norm(a %*% x - b, type = "2")**2         # object value
  grad <- function(x, a, b) 2 * t(a) %*% (a %*% x - b)                  # gradient
  id = diag(1, nrow = nrow(x))                                   # identity matrix
  x_k = matrix(rep(0, length(x)))    # initializing at \mathbf{x}_{0} = \mathbf{0}
  i = 1                                               # setting iterations counter
  alpha = 1                                           # initializing at \alpha = 1
           # setting \rho and the constant as 0.7 (to be used in the backtracking)
  rho = const = .7
  obj.value = obj(x_k, a, b)       # computing the object value for the first time
                             # empthy object to count how many times the algorithm
  count = numeric(1)         #        enters in the backtracking at each iteration
  p_k = -h %*% grad(x_k, a, b)                        # computing search direction
  norms = numeric(1)           # empthy object to keep the norms at each iteration
                             # initializing while loop with the stopping criterion
  while (norm(x - x_k[ , i], type = "2") > 1e-6 * norm(x, type = "2")) {
    while (obj(x_k[ , i] + alpha * p_k, a, b) >        # initializing backtracking
           obj.value[i] + const * alpha * t(grad(x_k[ , i], a, b)) %*% p_k) {
                # counting how many times we do the backtracking at each iteration
      count[i] = count[i] + 1
      alpha = rho * alpha               # updating \alpha (doing the backtracking)
    }
    x_k = cbind(x_k, x_k[ , i] + alpha * p_k)   # computing the new \mathbf{x}_{k}
    i = i + 1                                         # updating iteration counter
            # computing the difference between \mathbf{x}_{k+1} and \mathbf{x}_{k}
    s = x_k[ , i] - x_k[ , i-1]
                                      # computing the difference between gradients
    y = grad(x_k[ , i], a, b) - grad(x_k[ , i-1], a, b)
    rhok = as.numeric(1/(t(y) %*% s))                         # computing \rho_{k}
    h = (id - rhok * s %*% t(y)) %*% h %*% (id - rhok * y %*% t(s)) +
      rhok * s %*% t(s)                               # computing \mathbf{H}_{k+1}
    p_k = -h %*% grad(x_k[ , i], a, b)            # computing new search direction
    obj.value[i] = obj(x_k[ , i], a, b)   # computing and keeping new object value
    norms[i] = norm(x - x_k[ , i], type = "2")    # computing and keeping the norm
    count[i] = 0                 # setting the count to zero for the new iteration
  }
    # returning the \mathbf{x}_{k}'s, object values, norms and backtracking counts
  return(list(x = x_k[ , -1], obj.value = obj.value[-1], norms = norms[-1]
              , count = count))
} ; xks <- bfgs_back(x.ast, a, b)             # running the BFGS with backtracking
tail(xks$obj.value, 1)                                     # final objective value
tail(xks$norms, 1)                                                    # final norm
xks$count              # number of times that we do backtracking at each iteration
# </r code> ==================================================================== #
@

<<fig.height=3.5, fig.cap="BFGS with backtracking. (a): The objective value (in log scale) with increasing $k$; (b): $\\parallel \\mathbf{x}^{\\ast} - \\mathbf{x}_{k} \\parallel_{2}$ with increasing $k$.">>=
# <r code> ===================================================================== #
par(mfrow = c(1, 2), mar = c(4, 4, 3, 2) + .1)             # graphical definitions
plot(xks$obj.value, log = "y", type = "b" # plotting the object values (log scale)
     , xlab = "Iteration", ylab = "Objective value (in log scale)", main = "(a)")
                                     # plotting norms with increasing iterations k
plot(xks$norms, type = "b", xlab = "Iteration", ylab = "L2 norm", main = "(b)")
# </r code> ==================================================================== #
@

Comparing with \textbf{(c)} - steepest descent with backtracking, we have the
same final results (values). However, with BFGS with backtraking we see less
iterations, 1/3 less, with the values decreasing more faster.

\hfill \(\square\)

\section*{(e)}

Implement Newton's method with exact line search. Use the same initialization and stopping criterion as in \textbf{(b)}. What do you notice? \\

\noindent \underline{Solution}: \\

Newton's method iteration \(k + 1\) with exact line search:

\[
 \mathbf{x}_{k+1} = \mathbf{x}_{k} -
 \frac{\mathbf{p}_{k}^{\top}
       (\mathbf{A}^{\top}
        (\mathbf{A} \mathbf{x}_{k} - \mathbf{b})
       )}{\mathbf{p}_{k}^{\top} \mathbf{A}^{\top} \mathbf{A} \mathbf{p}_{k}}
 \mathbf{p}_{k}, \quad \text{ with } \quad
 \mathbf{p}_{k} = -(\mathbf{A}^{\top} \mathbf{A})^{-1}
                   (2 \mathbf{A}^{\top} (\mathbf{A}\mathbf{x}_{k} - \mathbf{b})).
\]

<<>>=
# <r code> ===================================================================== #
newton_els <- function(x, a, b) {         # newton's method with exact line search
  x_k = matrix(rep(0, length(x)))    # initializing at \mathbf{x}_{0} = \mathbf{0}
  i = 1                                               # setting iterations counter
  h_inv = solve(t(a) %*% a)                               # inverse of the hessian
                             # initializing while loop with the stopping criterion
  while (norm(x - x_k[ , i], type = "2") > 1e-6 * norm(x, type = "2")) {
                                                      # computing search direction
    p_k =  - h_inv %*% (2 * t(a) %*% (a %*% x_k[ , i] - b))
    alpha = - (t(p_k) %*% (t(a) %*% (a %*% x_k[ , i] - b))) /
      (t(p_k) %*% t(a) %*% a %*% p_k)           # computing step length \alpha_{k}
                                                # computing the new \mathbf{x}_{k}
    x_k = cbind(x_k, x_k[ , i] + as.numeric(alpha) * p_k)
    i = i + 1                                         # updating iteration counter
  }
  return(xks = x_k[ , -1])                        # returning the \mathbf{x}_{k}'s
}
xks <- newton_els(x.ast, a, b) # running the newtons method with exact line search
norm(a %*% xks - b, type = "2")**2                     # computing objective value
norm(x.ast - xks, type = "2")                                     # computing norm
# </r code> ==================================================================== #
@

Newton's method for optimization converges in one step if the function is
quadratic, as here. So here we have convergence in one interation. In \textbf{(b)}
- steepest descent we have convergence after more than 20 iterations.

\hfill \(\square\)

\section*{(f)}

Implement the original and economic linear CG methods. Use the same initialization
and stopping criterion as in \textbf{(b)}. Compare the performance of both methods
w.r.t. the number of iterations needed to converge and the total time needed to
converge. You can use the MATLAB commands \textit{tic} and \textit{toc} to measure
the overall runtime. \\

\noindent \underline{Solution}: \\

Original CG (Conjugate Gradient) method: \\

\includegraphics[width=.925\textwidth]{algorithm_5-1.png}

<<>>=
# <r code> ===================================================================== #
cg_orig <- function (x, a, b) {          # original cg - conjugate gradient method
  x_k = matrix(rep(0, length(x)))    # initializing at \mathbf{x}_{0} = \mathbf{0}
  r_k = 2 * t(a) %*% (a %*% x_k - b)    # gradient = residual of the linear system
  p_k = -r_k                                            # initial search direction
  i = 1                                               # setting iterations counter
  obj.value = norm(a %*% x_k - b, type = "2")**2                    # object value
  norms = numeric(1)           # empthy object to keep the norms at each iteration
  t1 = Sys.time()                                                   # initial time
                             # initializing while loop with the stopping criterion
  while (norm(x - x_k[ , i], type = "2") > 1e-6 * norm(x, type = "2")) {
                                             # step length using exact line search
    alpha = - (t(r_k) %*% p_k) / (2 * t(p_k) %*% t(a) %*% a %*% p_k)
                                                # computing the new \mathbf{x}_{k}
    x_k = cbind(x_k, x_k[ , i] + as.numeric(alpha) * p_k)
    i = i + 1                                         # updating iteration counter
    r_k = 2 * t(a) %*% (a %*% x_k[ , i] - b)                        # new residual
                                                # computing the constant \beta_{k}
    beta_k = (t(r_k) %*% t(a) %*% a %*% p_k) / (t(p_k) %*% t(a) %*% a %*% p_k)
    p_k = -r_k + as.numeric(beta_k) * p_k                   # new search direction
                                          # computing and keeping new object value
    obj.value[i] = norm(a %*% x_k[ , i] - b, type = "2")**2
    norms[i] = norm(x - x_k[ , i], type = "2")    # computing and keeping the norm
  }
 # returning the \mathbf{x}_{k}'s, object values and total time needed to converge
  return(list(x = x_k[ , -1], obj.value = obj.value[-1], norms = norms[-1]
              , time = Sys.time() - t1))
}                            # running the original cg - conjugate gradient method
xks_cg.orig <- cg_orig(x.ast, a, b)
# </r code> ==================================================================== #
@

<<fig.height=3.5, fig.cap="Original CG - conjugate gradient method. (a): The objective value (in log scale) with increasing $k$; (b): $\\parallel \\mathbf{x}^{\\ast} - \\mathbf{x}_{k} \\parallel_{2}$ with increasing $k$.">>=
# <r code> ===================================================================== #
par(mfrow = c(1, 2), mar = c(4, 4, 3, 2) + .1)             # graphical definitions
                                          # plotting the object values (log scale)
plot(xks_cg.orig$obj.value, log = "y", type = "b"
     , xlab = "Iteration", ylab = "Objective value (in log scale)", main = "(a)")
                                    # plotting norms with increasing iterations k
plot(xks_cg.orig$norms
     , type = "b", xlab = "Iteration", ylab = "L2 norm", main = "(b)")
# </r code> ==================================================================== #
@

Economic Linear CG (Conjugate Gradient) method: \\

\includegraphics[width=.925\textwidth]{algorithm_5-2.png}

<<>>=
# <r code> ===================================================================== #
cg_eco <- function (x, a, b) {    # economic linear cg - conjugate gradient method
  x_k = matrix(rep(0, length(x)))    # initializing at \mathbf{x}_{0} = \mathbf{0}
                        # matrix to keep the residuals of two different iterations
  r_k = matrix(NA, ncol = 2, nrow = nrow(x))
                                        # gradient = residual of the linear system
  r_k[ , 1] = 2 * t(a) %*% (a %*% x_k - b)
  p_k = -r_k[ , 1]                                      # initial search direction
  i = 1                                               # setting iterations counter
  obj.value = norm(a %*% x_k - b, type = "2")**2                    # object value
  norms = numeric(1)           # empthy object to keep the norms at each iteration
  t1 = Sys.time()                                                   # initial time
                             # initializing while loop with the stopping criterion
  while (norm(x - x_k[ , i], type = "2") > 1e-6 * norm(x, type = "2")) {
                                             # step length using exact line search
    heavy = t(a) %*% a %*% p_k
    alpha = (t(r_k[ , 1]) %*% r_k[ , 1]) / (2 * t(p_k) %*% heavy)
                                                # computing the new \mathbf{x}_{k}
    x_k = cbind(x_k, x_k[ , i] + as.numeric(alpha) * p_k)
    i = i + 1                                         # updating iteration counter
                                                                    # new residual
    r_k[ , 2] = r_k[ , 1] + as.numeric(alpha) * 2 * heavy
                                                # computing the constant \beta_{k}
    beta_k = (t(r_k[ , 2]) %*% r_k[ , 2]) / (t(r_k[ , 1]) %*% r_k[ , 1])
    p_k = -r_k[ , 2] + as.numeric(beta_k) * p_k             # new search direction
                                          # computing and keeping new object value
    obj.value[i] = norm(a %*% x_k[ , i] - b, type = "2")**2
    norms[i] = norm(x - x_k[ , i], type = "2")    # computing and keeping the norm
    r_k[ , 1] = r_k[ , 2]           # setting the new residual as the old residual
  }
 # returning the \mathbf{x}_{k}'s, object values and total time needed to converge
  return(list(x = x_k[ , -1], obj.value = obj.value[-1], norms = norms[-1]
              , time = Sys.time() - t1))
}                     # running the economic linear cg - conjugate gradient method
xks_cg.eco <- cg_eco(x.ast, a, b)
# </r code> ==================================================================== #
@

<<fig.height=3.5, fig.cap="Economic Linear CG - conjugate gradient method. (a): The objective value (in log scale) with increasing $k$; (b): $\\parallel \\mathbf{x}^{\\ast} - \\mathbf{x}_{k} \\parallel_{2}$ with increasing $k$.">>=
# <r code> ===================================================================== #
par(mfrow = c(1, 2), mar = c(4, 4, 3, 2) + .1)             # graphical definitions
                                          # plotting the object values (log scale)
plot(xks_cg.eco$obj.value, log = "y", type = "b"
     , xlab = "Iteration", ylab = "Objective value (in log scale)", main = "(a)")
                                    # plotting norms with increasing iterations k
plot(xks_cg.eco$norms
     , type = "b", xlab = "Iteration", ylab = "L2 norm", main = "(b)")
# </r code> ==================================================================== #
@

\newpage

\noindent Comparing:

<<>>=
# <r code> ===================================================================== #
                                         # original cg - conjugate gradient method
tail(xks_cg.orig$obj.value, 1)                             # final objective value
tail(xks_cg.orig$norms, 1)                                            # final norm
xks_cg.orig$time                                   # total time needed to converge
# </r code> ==================================================================== #
@

<<>>=
# <r code> ===================================================================== #
                                  # economic linear cg - conjugate gradient method
tail(xks_cg.eco$obj.value, 1)                              # final objective value
tail(xks_cg.eco$norms, 1)                                             # final norm
xks_cg.eco$time                                    # total time needed to converge
# </r code> ==================================================================== #
@

Both methods reach the same values with the same number of iterations, 12.
However, the original CG method need 4 seconds to converge, while the economic
linear CG method need 1.49 minutes.

\hfill \(\blacksquare\)

\end{document}